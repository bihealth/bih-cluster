{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home \u00b6 This is the documentation of the BIH high-performance compute (HPC) cluster. This documentation is maintained bih BIH HPC IT, BIH CUBI (Core Unit Bioinformatics), and the user community. The global table of contents is on the left, it is on the right for the current page . Getting Started \u00b6 To get started, the following is a good reading order after your first successful connection to the cluster. Getting Access . Getting Help (and Writing Good Tickets ). For the Impatient . The Cluster Tutorial: First Steps . Then, continue reading through the manual. This is a living document that you can update and add to. See How-To: Contribute to this Document for details. Acknowledging BIH HPC Usage Acknowledge usage of the cluster in your manuscript as \"Computation has been performed on the HPC for Research/Clinic cluster of the Berlin Institute of Health\" . Please add your publications using the cluster to this list . Maintenance Announcements \u00b6 Current and Future Maintenances Head node migration -- July/August 2020 /fast => /data -- autumn 2020 See Maintenance for a detailed list of current, planned, and previous maintenance and update work. Connecting to the Cluster \u00b6 You will need to perform some configuration steps after you have been registered with the cluster (via email to hpc-gatekeeper@bihealth.de by a group leader/PI). Here are the most important points: Generating SSH Keys in Linux or Windows . Submitting the key to Charite or to MDC . Configuring your SSH client on Linux and Mac or Windows . Bonus: Connecting from external networks . There are various other topics covered in the \"Connecting\" section that might be of interest to you. Documentation Structure \u00b6 The documentation is structured as follows: Administrative provides information about administrative processes such as how to get access, register users, work groups, and projects. Getting Help explains how you can obtain for using the BIH HPC. Overview provides detailed information about the cluster setup. This includes the description of the hardware, network, software, and policies. Connecting provides technical help on performing connections to the cluster. First Steps provides information for getting you started quickly. Slurm Scheduler provides technical help on using the Slurm scheduler. Best Practice provides guidelines on recommended usage of certain aspects of the system. Static Data provides documentation about the static data (files) collection on the cluster. How-To provides short(ish) solutions for specific technical problems. Miscellaneous contains a growing list of pages not fitting anywhere else.","title":"Home"},{"location":"#home","text":"This is the documentation of the BIH high-performance compute (HPC) cluster. This documentation is maintained bih BIH HPC IT, BIH CUBI (Core Unit Bioinformatics), and the user community. The global table of contents is on the left, it is on the right for the current page .","title":"Home"},{"location":"#getting-started","text":"To get started, the following is a good reading order after your first successful connection to the cluster. Getting Access . Getting Help (and Writing Good Tickets ). For the Impatient . The Cluster Tutorial: First Steps . Then, continue reading through the manual. This is a living document that you can update and add to. See How-To: Contribute to this Document for details. Acknowledging BIH HPC Usage Acknowledge usage of the cluster in your manuscript as \"Computation has been performed on the HPC for Research/Clinic cluster of the Berlin Institute of Health\" . Please add your publications using the cluster to this list .","title":"Getting Started"},{"location":"#maintenance-announcements","text":"Current and Future Maintenances Head node migration -- July/August 2020 /fast => /data -- autumn 2020 See Maintenance for a detailed list of current, planned, and previous maintenance and update work.","title":"Maintenance Announcements"},{"location":"#connecting-to-the-cluster","text":"You will need to perform some configuration steps after you have been registered with the cluster (via email to hpc-gatekeeper@bihealth.de by a group leader/PI). Here are the most important points: Generating SSH Keys in Linux or Windows . Submitting the key to Charite or to MDC . Configuring your SSH client on Linux and Mac or Windows . Bonus: Connecting from external networks . There are various other topics covered in the \"Connecting\" section that might be of interest to you.","title":"Connecting to the Cluster"},{"location":"#documentation-structure","text":"The documentation is structured as follows: Administrative provides information about administrative processes such as how to get access, register users, work groups, and projects. Getting Help explains how you can obtain for using the BIH HPC. Overview provides detailed information about the cluster setup. This includes the description of the hardware, network, software, and policies. Connecting provides technical help on performing connections to the cluster. First Steps provides information for getting you started quickly. Slurm Scheduler provides technical help on using the Slurm scheduler. Best Practice provides guidelines on recommended usage of certain aspects of the system. Static Data provides documentation about the static data (files) collection on the cluster. How-To provides short(ish) solutions for specific technical problems. Miscellaneous contains a growing list of pages not fitting anywhere else.","title":"Documentation Structure"},{"location":"admin/getting-access/","text":"Getting Access \u00b6 Get Access Register Users. Group leaders register their members via hpc-gatekeeper@bihealth.de . Upload Key. Upload your SSH key through the Charite and MDC Infrastructure. Connect. ssh -l <user>_<c or m>@med-login1.bihealth.org Access to the BIH HPC is based on work groups (also known as labs, units). Each group is headed by a leader (also known as principle investigator/PI). Data can also be managed in project which allow cross-group collaboration but also providing a limited access space, e.g., for controlled data access data where only a few group members may access the data. # Charite Users host:~$ ssh -l user_c login.research.hpc.bihealth.org # MDC Users host:~$ ssh -l user_m login.research.hpc.bihealth.org Accounts and Email Adresses All users on the cluster must already have an account with either Charite/BIH or MDC. Please only use email addresses from the institutions Charite, BIH, MDC. Work Groups \u00b6 The process to create a new group is as follows. The group leader sends an email to hpc-gatekeeper@bihealth.de and fills out the form below. Please consider the notes below on this page. hpc-gatekeeper decides on the request and the corresponding objects are created on the cluster (users, groups, directories). All new users are notified and further instructions are sent to them via email. Subsequently, both owner and delegate can initiate changes (new users, resource changes etc.) to the group. Form \u00b6 Example values are given in curly braces. # Group \"ag-doe\" Group leader/PI: {John Doe} Delegate [optional]: {Max Mustermann} Purpose of cluster usage [short]: {RNA-seq analysis in colorectal cancer} Required resources: - storage in TB: {1 TB} - CPU hours per year: {approx. 1000} - GPU hours per year: {none} - Number of files [if >> 1M]: {less than 1M} Users for each member: # User 1 - first name: John - last name: Doe - affiliation: Charite, Department of Oncology - institute email: john.doe@charite.de - institute phone: 030-8445-0 - user has account with - [ ] BIH - [ ] Charite - [ ] MDC - BIH/Charite/MDC user name: doej - duration of cluster access (max 1 year): 2020-03-30 to 2021-03-30 [etc.] Notes \u00b6 Work groups on the cluster must have an owner (its leader, principal investigator, etc.) Group ownership implies control but also accountability for their work group and members. Through the delegate mechanism, control can be delegated to up to one person (e.g., post-doc in the lab). Users can be members of one work group only. For multi-group collaborations, please use the project mechanism described below. Projects \u00b6 Projects are very similar to work groups with the main distinction that users can be a member of more than one project, but project membership does not grant cluster access (group membership is still required). Project creation can be initiated by group leaders and group delegates with the following process. The initiator sends an email to hpc-gatekeeper@bihealth.de and fills out the following form. Please consider the notes below on this page. hpc-gatekeeper decides on the request and the corresponding objects are created on the cluster (users, groups, directories). All new users are notified and further instructions are sent to them via email. Subsequently, both owner and delegate can initiate changes (new users, resource changes etc.) to the project. Form \u00b6 Example values are given in curly braces. # Project \"doe-dbgap-rna\" Project owner: {John Doe} Delegate [optional]: {Max Mustermann} Purpose of cluster usage [short]: {RNA-seq data from dbGAP} Required resources: - storage in TB: {1 TB} - CPU hours per year: {approx. 1000} - GPU hours per year: {none} - Number of files [if >> 1M]: {less than 1M} Additional members: - Susi Sorglos <Susi.Sorglos@charite.de>","title":"Getting Access"},{"location":"admin/getting-access/#getting-access","text":"Get Access Register Users. Group leaders register their members via hpc-gatekeeper@bihealth.de . Upload Key. Upload your SSH key through the Charite and MDC Infrastructure. Connect. ssh -l <user>_<c or m>@med-login1.bihealth.org Access to the BIH HPC is based on work groups (also known as labs, units). Each group is headed by a leader (also known as principle investigator/PI). Data can also be managed in project which allow cross-group collaboration but also providing a limited access space, e.g., for controlled data access data where only a few group members may access the data. # Charite Users host:~$ ssh -l user_c login.research.hpc.bihealth.org # MDC Users host:~$ ssh -l user_m login.research.hpc.bihealth.org Accounts and Email Adresses All users on the cluster must already have an account with either Charite/BIH or MDC. Please only use email addresses from the institutions Charite, BIH, MDC.","title":"Getting Access"},{"location":"admin/getting-access/#work-groups","text":"The process to create a new group is as follows. The group leader sends an email to hpc-gatekeeper@bihealth.de and fills out the form below. Please consider the notes below on this page. hpc-gatekeeper decides on the request and the corresponding objects are created on the cluster (users, groups, directories). All new users are notified and further instructions are sent to them via email. Subsequently, both owner and delegate can initiate changes (new users, resource changes etc.) to the group.","title":"Work Groups"},{"location":"admin/getting-access/#form","text":"Example values are given in curly braces. # Group \"ag-doe\" Group leader/PI: {John Doe} Delegate [optional]: {Max Mustermann} Purpose of cluster usage [short]: {RNA-seq analysis in colorectal cancer} Required resources: - storage in TB: {1 TB} - CPU hours per year: {approx. 1000} - GPU hours per year: {none} - Number of files [if >> 1M]: {less than 1M} Users for each member: # User 1 - first name: John - last name: Doe - affiliation: Charite, Department of Oncology - institute email: john.doe@charite.de - institute phone: 030-8445-0 - user has account with - [ ] BIH - [ ] Charite - [ ] MDC - BIH/Charite/MDC user name: doej - duration of cluster access (max 1 year): 2020-03-30 to 2021-03-30 [etc.]","title":"Form"},{"location":"admin/getting-access/#notes","text":"Work groups on the cluster must have an owner (its leader, principal investigator, etc.) Group ownership implies control but also accountability for their work group and members. Through the delegate mechanism, control can be delegated to up to one person (e.g., post-doc in the lab). Users can be members of one work group only. For multi-group collaborations, please use the project mechanism described below.","title":"Notes"},{"location":"admin/getting-access/#projects","text":"Projects are very similar to work groups with the main distinction that users can be a member of more than one project, but project membership does not grant cluster access (group membership is still required). Project creation can be initiated by group leaders and group delegates with the following process. The initiator sends an email to hpc-gatekeeper@bihealth.de and fills out the following form. Please consider the notes below on this page. hpc-gatekeeper decides on the request and the corresponding objects are created on the cluster (users, groups, directories). All new users are notified and further instructions are sent to them via email. Subsequently, both owner and delegate can initiate changes (new users, resource changes etc.) to the project.","title":"Projects"},{"location":"admin/getting-access/#form_1","text":"Example values are given in curly braces. # Project \"doe-dbgap-rna\" Project owner: {John Doe} Delegate [optional]: {Max Mustermann} Purpose of cluster usage [short]: {RNA-seq data from dbGAP} Required resources: - storage in TB: {1 TB} - CPU hours per year: {approx. 1000} - GPU hours per year: {none} - Number of files [if >> 1M]: {less than 1M} Additional members: - Susi Sorglos <Susi.Sorglos@charite.de>","title":"Form"},{"location":"admin/maintenance/","text":"This page documents the current and known upcoming maintenance windows. Next Maintenance Window \u00b6 There currently is no maintenance planned. Pending Changes \u00b6 Network Maintenance: June 3, 2020 \u00b6 On June 3, we need to perform a network maintenance at 8 am. If everything goes well, there might be a short delay in network packages and connections will survive. In this case, the maintenance will end 8:30 am. Otherwise, the maintenance will finish by noon. Cluster Maintenance with Downtime: June 16 \u00b6 We need to schedule a full cluster downtime on June 16. Slurm Migration \u00b6 We will switch to the Slurm workload scheduler (from the legacy SGE). The main reason is that Slurm allows for better scheduling of GPUs (and has loads of improvements over SGE), but the syntax is a bit different. Currently, our documentation is in an transient state. We are currently extending our Slurm-specific documentation . March 7, 2020 (test stage) : Slurm will provide 16 CPU and 3 GPU nodes (with 4 Tesla V100 each), and two high memory nodes, the remaining nodes are available in SGE. We ask users to look into scheduling with Slurm. March 31, 2020 (intermediate stage) : Half of the nodes will be migrated to the Slurm cluster (~100), all high memory and GPU nodes will be moved to Slurm. New users are advised to use not learn SGE any more but directly use Slurm. Support for SGE is limited to bug fixing only (documentation and tips are phased out). May 31, 2020 (sunsetting SGE) : All but 16 nodes will remain in the SGE cluster. June 31, 2020 (the end) : SGE has reached its end of life on hpc4research. SSH Key Management \u00b6 SSH Key Management has switched to using Charite and MDC ActiveDirectory servers. You need to upload all keys by the end of April 2020. MDC Key Upload Charite Key Upload Schedule Feb 4, 2020: Keys are now also taken from central MDC/Charite servers. You do not need to contact us any more to update your keys (we cannot accelerate the process at MDC). May 1, 2020: Keys are now only taken from central MDC/Charite servers. You must upload your keys to central servers by then. Login Node Migration \u00b6 The login nodes will be moved from physical machines to virtual machines in high-availability mode. Further, they will be available as login-1.research.hpc.bihealth.org and login-2... instead of med-login{1,2} . The aim is to improve stability and make everything easier to manage by administration. Schedule May 15, 2020: Availability of the head nodes for testing. May 31, 2020: (planned) New login nodes are the preferred connection method. June 15, 2020: Old login nodes are switched off. Transfer Node Migration (in planning) \u00b6 Note This task is currently being planned. No schedule has been fixed yet. CentOS 8 Migration (in planning) \u00b6 Note This task is currently being planned. No schedule has been fixed yet. All nodes will be upgraded to CentOS 8. This will be done in a rolling fashion over the course of 1 month. The login nodes must be rebooted which we will do with a break of 2 days (one node will remain running). Unification of Mass Data Mounts \u00b6 Note This task is currently being planned. No schedule has been fixed yet. To harmonize the mount options with HPC 4 Clinic, the mount location of /fast must be changed to /data/gpfs-1 . We will start by creating a symlink from /data/gpfs-1 to /fast . To do this properly, a full reboot will be required. We will keep around a symlink from /fast to /data/gpfs-1 for some time during a migration phase. Previous Maintenance Windows \u00b6 Switch update, Location Flip of med-login2 and med-transfer1 \u00b6 Monday, February 23, 9am-15am. Affected systems: med-transfer1 med-transfer2 med-login2 a few compute nodes The compute nodes are non-critical as we are taking them out of the queues now. CentOS 7.6 Upgrade, January 29, February 5 \u00b6 Wednesday, January 29, 2018: Reboot med-login1, med-transfer1 Wednesday, February 5, 2018: Reboot med-login2, med-transfer2 September 03-30, 2018 \u00b6 Starting monday 03.09.2018 we will be performing rolling update of the cluster from CentOS 7.4 to CentOS 7.5. Since update will be performed in small bunches of nodes, the only impact you should notice is smaller number of nodes available for computation. Also, for around two weeks, you can expect that your jobs can hit both CentOS 7.4 & CentOS 7.5 nodes. This should not impact you in any way, but if you encounter any unexpected behavior of the cluster during this time, please let us know. At some point we will have to update the transfer, and login nodes. We will do this also in parts, so the you can switch to the other machine. Key dates are: 18.09.2018 - med-login1 & med-transfer1 will not be available, and you should switch to med-login2 & med-transfer2 respectively. 25.09.2018 - med-login2 & med-transfer2 will not be available, and you should switch to med-login1 & med-transfer1 respectively. Please also be informed that non-invasive maintenance this weekend which we announced has been canceled, so cluster will operate normally. In case of any concerns, issues, do not hesitate to contact us via hpc-admin@bihealth.de , or hpc-helpdesk@bihealth.de . June 18, 2018, 0600-1500 \u00b6 Due to tasks we need to perform on BIH cluster, we have planned maintenance: Maintenance start: 18.06.2018 06:00 AM Maintenance end: 18.06.2018 3:00 PM During maintenance we will perform several actions: GPFS drives re-balancing to improve performance OS update on cluster, transfer, and login nodes During maintenance whole cluster will not be usable, this includes: you will not be able to run jobs on cluster (SGE queuing system will be shutdown) med-login{1,2} nodes will not work reliably during this time med-transfer{1-2} nodes, and resources shared by them will be not available Maintenance window is quite long, since we are dependent on external vendor. However, we will recover services as soon as possible. We will keep you posted during maintenance with services status. March 16-18, 2018 (MDC IT) \u00b6 MDC IT has a network maintenance from Friday, March 16 18:00 hours until Sunday March 18 18:00 hours. This will affect connections to the cluster but no connections within the cluster. January 17, 2018 (Complete) \u00b6 STATUS: complete The first aim of this window is to upgrade the cluster to CentOS 7.4 to patch against the Meltdown/Spectre vulnerabilities. For this, the login and transfer nodes have to be rebooted. The second aim of this window is to reboot the file server to mitigate some NFS errors. For this, the SGE master has to be stopped for some time. Plan/Progress \u00b6 reboot med-file1 update to CentOS 7.4 front nodes med-login1 med-login2 med-login3 (admin use only) med-transfer1 med-transfer2 infrastructure nodes qmaster* install-srv compute nodes med0100 to med0246 med0247 to med0764 special purpose compute nodes med0401 (high-memory) med0402 (high-memory) med0403 (high-memory) med0404 (high-memory) med0405 (GPU) Previous Maintenance \u00b6 (since January 2010) none","title":"Maintenance etc."},{"location":"admin/maintenance/#next-maintenance-window","text":"There currently is no maintenance planned.","title":"Next Maintenance Window"},{"location":"admin/maintenance/#pending-changes","text":"","title":"Pending Changes"},{"location":"admin/maintenance/#network-maintenance-june-3-2020","text":"On June 3, we need to perform a network maintenance at 8 am. If everything goes well, there might be a short delay in network packages and connections will survive. In this case, the maintenance will end 8:30 am. Otherwise, the maintenance will finish by noon.","title":"Network Maintenance: June 3, 2020"},{"location":"admin/maintenance/#cluster-maintenance-with-downtime-june-16","text":"We need to schedule a full cluster downtime on June 16.","title":"Cluster Maintenance with Downtime: June 16"},{"location":"admin/maintenance/#slurm-migration","text":"We will switch to the Slurm workload scheduler (from the legacy SGE). The main reason is that Slurm allows for better scheduling of GPUs (and has loads of improvements over SGE), but the syntax is a bit different. Currently, our documentation is in an transient state. We are currently extending our Slurm-specific documentation . March 7, 2020 (test stage) : Slurm will provide 16 CPU and 3 GPU nodes (with 4 Tesla V100 each), and two high memory nodes, the remaining nodes are available in SGE. We ask users to look into scheduling with Slurm. March 31, 2020 (intermediate stage) : Half of the nodes will be migrated to the Slurm cluster (~100), all high memory and GPU nodes will be moved to Slurm. New users are advised to use not learn SGE any more but directly use Slurm. Support for SGE is limited to bug fixing only (documentation and tips are phased out). May 31, 2020 (sunsetting SGE) : All but 16 nodes will remain in the SGE cluster. June 31, 2020 (the end) : SGE has reached its end of life on hpc4research.","title":"Slurm Migration"},{"location":"admin/maintenance/#ssh-key-management","text":"SSH Key Management has switched to using Charite and MDC ActiveDirectory servers. You need to upload all keys by the end of April 2020. MDC Key Upload Charite Key Upload Schedule Feb 4, 2020: Keys are now also taken from central MDC/Charite servers. You do not need to contact us any more to update your keys (we cannot accelerate the process at MDC). May 1, 2020: Keys are now only taken from central MDC/Charite servers. You must upload your keys to central servers by then.","title":"SSH Key Management"},{"location":"admin/maintenance/#login-node-migration","text":"The login nodes will be moved from physical machines to virtual machines in high-availability mode. Further, they will be available as login-1.research.hpc.bihealth.org and login-2... instead of med-login{1,2} . The aim is to improve stability and make everything easier to manage by administration. Schedule May 15, 2020: Availability of the head nodes for testing. May 31, 2020: (planned) New login nodes are the preferred connection method. June 15, 2020: Old login nodes are switched off.","title":"Login Node Migration"},{"location":"admin/maintenance/#transfer-node-migration-in-planning","text":"Note This task is currently being planned. No schedule has been fixed yet.","title":"Transfer Node Migration (in planning)"},{"location":"admin/maintenance/#centos-8-migration-in-planning","text":"Note This task is currently being planned. No schedule has been fixed yet. All nodes will be upgraded to CentOS 8. This will be done in a rolling fashion over the course of 1 month. The login nodes must be rebooted which we will do with a break of 2 days (one node will remain running).","title":"CentOS 8 Migration (in planning)"},{"location":"admin/maintenance/#unification-of-mass-data-mounts","text":"Note This task is currently being planned. No schedule has been fixed yet. To harmonize the mount options with HPC 4 Clinic, the mount location of /fast must be changed to /data/gpfs-1 . We will start by creating a symlink from /data/gpfs-1 to /fast . To do this properly, a full reboot will be required. We will keep around a symlink from /fast to /data/gpfs-1 for some time during a migration phase.","title":"Unification of Mass Data Mounts"},{"location":"admin/maintenance/#previous-maintenance-windows","text":"","title":"Previous Maintenance Windows"},{"location":"admin/maintenance/#switch-update-location-flip-of-med-login2-and-med-transfer1","text":"Monday, February 23, 9am-15am. Affected systems: med-transfer1 med-transfer2 med-login2 a few compute nodes The compute nodes are non-critical as we are taking them out of the queues now.","title":"Switch update, Location Flip of med-login2 and med-transfer1"},{"location":"admin/maintenance/#centos-76-upgrade-january-29-february-5","text":"Wednesday, January 29, 2018: Reboot med-login1, med-transfer1 Wednesday, February 5, 2018: Reboot med-login2, med-transfer2","title":"CentOS 7.6 Upgrade, January 29, February 5"},{"location":"admin/maintenance/#september-03-30-2018","text":"Starting monday 03.09.2018 we will be performing rolling update of the cluster from CentOS 7.4 to CentOS 7.5. Since update will be performed in small bunches of nodes, the only impact you should notice is smaller number of nodes available for computation. Also, for around two weeks, you can expect that your jobs can hit both CentOS 7.4 & CentOS 7.5 nodes. This should not impact you in any way, but if you encounter any unexpected behavior of the cluster during this time, please let us know. At some point we will have to update the transfer, and login nodes. We will do this also in parts, so the you can switch to the other machine. Key dates are: 18.09.2018 - med-login1 & med-transfer1 will not be available, and you should switch to med-login2 & med-transfer2 respectively. 25.09.2018 - med-login2 & med-transfer2 will not be available, and you should switch to med-login1 & med-transfer1 respectively. Please also be informed that non-invasive maintenance this weekend which we announced has been canceled, so cluster will operate normally. In case of any concerns, issues, do not hesitate to contact us via hpc-admin@bihealth.de , or hpc-helpdesk@bihealth.de .","title":"September 03-30, 2018"},{"location":"admin/maintenance/#june-18-2018-0600-1500","text":"Due to tasks we need to perform on BIH cluster, we have planned maintenance: Maintenance start: 18.06.2018 06:00 AM Maintenance end: 18.06.2018 3:00 PM During maintenance we will perform several actions: GPFS drives re-balancing to improve performance OS update on cluster, transfer, and login nodes During maintenance whole cluster will not be usable, this includes: you will not be able to run jobs on cluster (SGE queuing system will be shutdown) med-login{1,2} nodes will not work reliably during this time med-transfer{1-2} nodes, and resources shared by them will be not available Maintenance window is quite long, since we are dependent on external vendor. However, we will recover services as soon as possible. We will keep you posted during maintenance with services status.","title":"June 18, 2018, 0600-1500"},{"location":"admin/maintenance/#march-16-18-2018-mdc-it","text":"MDC IT has a network maintenance from Friday, March 16 18:00 hours until Sunday March 18 18:00 hours. This will affect connections to the cluster but no connections within the cluster.","title":"March 16-18, 2018 (MDC IT)"},{"location":"admin/maintenance/#january-17-2018-complete","text":"STATUS: complete The first aim of this window is to upgrade the cluster to CentOS 7.4 to patch against the Meltdown/Spectre vulnerabilities. For this, the login and transfer nodes have to be rebooted. The second aim of this window is to reboot the file server to mitigate some NFS errors. For this, the SGE master has to be stopped for some time.","title":"January 17, 2018 (Complete)"},{"location":"admin/maintenance/#planprogress","text":"reboot med-file1 update to CentOS 7.4 front nodes med-login1 med-login2 med-login3 (admin use only) med-transfer1 med-transfer2 infrastructure nodes qmaster* install-srv compute nodes med0100 to med0246 med0247 to med0764 special purpose compute nodes med0401 (high-memory) med0402 (high-memory) med0403 (high-memory) med0404 (high-memory) med0405 (GPU)","title":"Plan/Progress"},{"location":"admin/maintenance/#previous-maintenance","text":"(since January 2010) none","title":"Previous Maintenance"},{"location":"admin/provided-software/","text":"Administration-Provided Software \u00b6 Some software is provided by HPC Administration based on the criteria that it is: system-near or system-level, very commonly used. Currently, this includes: GCC v7.2.0 CMake v3.11.0 LLVM v6.0.0 OpenMPI v4.0.3 On the GPU node, this also includes a recent NVIDIA CUDA versions. To see the available software, use module avail on the compute nodes (this will not work on the login nodes): $ module avail --------------------- /opt/local/modules --------------------- cmake/3.11.0-0 llvm/6.0.0-0 gcc/7.2.0-0 openmpi/4.0.3-0 To load software, use module load . This will adjust the environment variables accordingly, in particular update PATH such that the executable are available. $ which gcc /bin/gcc $ module load gcc/7.2.0-0 $ which gcc /opt/local/gcc-7.2.0-0/bin/gcc","title":"Provided Software"},{"location":"admin/provided-software/#administration-provided-software","text":"Some software is provided by HPC Administration based on the criteria that it is: system-near or system-level, very commonly used. Currently, this includes: GCC v7.2.0 CMake v3.11.0 LLVM v6.0.0 OpenMPI v4.0.3 On the GPU node, this also includes a recent NVIDIA CUDA versions. To see the available software, use module avail on the compute nodes (this will not work on the login nodes): $ module avail --------------------- /opt/local/modules --------------------- cmake/3.11.0-0 llvm/6.0.0-0 gcc/7.2.0-0 openmpi/4.0.3-0 To load software, use module load . This will adjust the environment variables accordingly, in particular update PATH such that the executable are available. $ which gcc /bin/gcc $ module load gcc/7.2.0-0 $ which gcc /opt/local/gcc-7.2.0-0/bin/gcc","title":"Administration-Provided Software"},{"location":"admin/resource-registration/","text":"Resource Registration \u00b6 This page describes the necessary registration steps for accessing special resources on the cluster, such as: GPU nodes, high-memory nodes, Matlab licenses, access to the critical partition. Overall, there are no technical restrictions to accessing these resources. Rather, we expect users to informally register with hpc-gatekeeper@bihealth.de and only then use the resources. Registrations have to be refreshed every 6 months . We trust that our users are grown-ups and are able to arrange for fair usage without administration intervention. House Rules Using the GPU node without prior arrangement with hpc-gatekeeper, not being reachable via email or phone within one working day while blocking the node, or other uncooperative behaviour can lead to HPC administration killing your jobs. GPU Nodes \u00b6 Hint Make sure to read the FAQ entry \" I have problems connecting to the GPU node! What's wrong? \".** If you want to use the GPU node, please send an email to hpc-gatekeeper@bihealth.de with the following information: For how long do you want to have access? Please limit yourself to up to 6 months per request, you can extend the request afterwards with the same process. How frequently do you need the GPU node, for how long at a time? Do you plan to do more interactive work (e.g., using ipython ) or using batch jobs? At the moment, all requests will be granted by hpc-gatekeeper. Use the GPU node by following the instructions How To: Connect to GPU Nodes Be nice and cooperative with other users, e.g., ask arrange sharing of the node via email and phone. Type getent passwd USER_NAME on the cluster to see user's contact details. High-Memory Nodes \u00b6 Note Note that the purpose of the high memory nodes is to run jobs that don't run on the remainder of the cluster. As the normal cluster nodes have 126-189GB of RAM each, we expect many jobs to fit on the (plenty) cluster nodes and these don't have to run on the (few and sparse) high memory nodes. If you want to use the high memory nodes, please send an email to hpc-gatekeeper@bihealth.de with the following information: For how long do you want to have access? Please limit yourself to up to 6 months per request, you can extend the request afterwards with the same process. How frequently do you need the nodes, for how long at a time? Do you plan to do more interactive work (e.g., using ipython ) or using batch jobs? What kind of work do you plan to do (e.g., assembly, running R scripts that don't run on the usual nodes) At the moment, all requests will be granted by hpc-gatekeeper. Use the high-memory node by following the instructions How-To: Connect to High-Memory Node Be nice and cooperative with other users, e.g., ask arrange sharing of the node via email and phone. Type getent passwd USER_NAME on the cluster to see user's contact details. Matlab Licenses \u00b6 GNU Octave as Matlab alternative Note that GNU Octave is an Open Source alternative to Matlab. While both packages are not 100% compatible, Octave is an alternative that does not require any license management. Further, you can easily install it yourself using Conda . Want to use the Matlab GUI? Make sure you understand X forwarding as outline in this FAQ entry . If you want to use the Matlab nodes, please send an email to hpc-gatekeeper@bihealth.de with the following information: For how long do you want to have access? Please limit yourself to up to 6 months per request, you can extend the request afterwards with the same process. How frequently do you need the nodes, for how long at a time? Do you plan to do more interactive work or using batch jobs? At the moment, all requests will be granted by hpc-gatekeeper. Use the high-memory node by following the instructions How-To: Use Matlab Be nice and cooperative with other users, e.g., ask arrange sharing of the node via email and phone. Type getent passwd USER_NAME on the cluster to see user's contact details. Requesting Licenses Before using matlab, you have to request a license by passing -L matlab_r2016b to your srun or sbatch job. Failure to do so can lead to other user's jobs crashing because license are not available. Violations of this rule can lead to HPC administration killing your jobs. Critical Partition \u00b6 The cluster provides a critical partition for jobs with deadlines (e.g., on paper submission or for tasks supporting clinical applications). Access to this partition has to be explicitely granted as with the other resources on this page. If you want to use the critical partition, please send an email to hpc-gatekeeper@bihealth.de with the following information: For how long do you want to have access? Please limit yourself to up to 6 months per request, you can extend the request afterwards with the same process. How frequently do you need the nodes, for how long at a time? Do you plan to do more interactive work or using batch jobs? At the moment, all requests will be granted by hpc-gatekeeper. Use the high-memory node by following the instructions How-To: Use the Critical Partition Be nice and cooperative with other users, e.g., ask arrange sharing of the node via email and phone. Type getent passwd USER_NAME on the cluster to see user's contact details.","title":"Resource Registration"},{"location":"admin/resource-registration/#resource-registration","text":"This page describes the necessary registration steps for accessing special resources on the cluster, such as: GPU nodes, high-memory nodes, Matlab licenses, access to the critical partition. Overall, there are no technical restrictions to accessing these resources. Rather, we expect users to informally register with hpc-gatekeeper@bihealth.de and only then use the resources. Registrations have to be refreshed every 6 months . We trust that our users are grown-ups and are able to arrange for fair usage without administration intervention. House Rules Using the GPU node without prior arrangement with hpc-gatekeeper, not being reachable via email or phone within one working day while blocking the node, or other uncooperative behaviour can lead to HPC administration killing your jobs.","title":"Resource Registration"},{"location":"admin/resource-registration/#gpu-nodes","text":"Hint Make sure to read the FAQ entry \" I have problems connecting to the GPU node! What's wrong? \".** If you want to use the GPU node, please send an email to hpc-gatekeeper@bihealth.de with the following information: For how long do you want to have access? Please limit yourself to up to 6 months per request, you can extend the request afterwards with the same process. How frequently do you need the GPU node, for how long at a time? Do you plan to do more interactive work (e.g., using ipython ) or using batch jobs? At the moment, all requests will be granted by hpc-gatekeeper. Use the GPU node by following the instructions How To: Connect to GPU Nodes Be nice and cooperative with other users, e.g., ask arrange sharing of the node via email and phone. Type getent passwd USER_NAME on the cluster to see user's contact details.","title":"GPU Nodes"},{"location":"admin/resource-registration/#high-memory-nodes","text":"Note Note that the purpose of the high memory nodes is to run jobs that don't run on the remainder of the cluster. As the normal cluster nodes have 126-189GB of RAM each, we expect many jobs to fit on the (plenty) cluster nodes and these don't have to run on the (few and sparse) high memory nodes. If you want to use the high memory nodes, please send an email to hpc-gatekeeper@bihealth.de with the following information: For how long do you want to have access? Please limit yourself to up to 6 months per request, you can extend the request afterwards with the same process. How frequently do you need the nodes, for how long at a time? Do you plan to do more interactive work (e.g., using ipython ) or using batch jobs? What kind of work do you plan to do (e.g., assembly, running R scripts that don't run on the usual nodes) At the moment, all requests will be granted by hpc-gatekeeper. Use the high-memory node by following the instructions How-To: Connect to High-Memory Node Be nice and cooperative with other users, e.g., ask arrange sharing of the node via email and phone. Type getent passwd USER_NAME on the cluster to see user's contact details.","title":"High-Memory Nodes"},{"location":"admin/resource-registration/#matlab-licenses","text":"GNU Octave as Matlab alternative Note that GNU Octave is an Open Source alternative to Matlab. While both packages are not 100% compatible, Octave is an alternative that does not require any license management. Further, you can easily install it yourself using Conda . Want to use the Matlab GUI? Make sure you understand X forwarding as outline in this FAQ entry . If you want to use the Matlab nodes, please send an email to hpc-gatekeeper@bihealth.de with the following information: For how long do you want to have access? Please limit yourself to up to 6 months per request, you can extend the request afterwards with the same process. How frequently do you need the nodes, for how long at a time? Do you plan to do more interactive work or using batch jobs? At the moment, all requests will be granted by hpc-gatekeeper. Use the high-memory node by following the instructions How-To: Use Matlab Be nice and cooperative with other users, e.g., ask arrange sharing of the node via email and phone. Type getent passwd USER_NAME on the cluster to see user's contact details. Requesting Licenses Before using matlab, you have to request a license by passing -L matlab_r2016b to your srun or sbatch job. Failure to do so can lead to other user's jobs crashing because license are not available. Violations of this rule can lead to HPC administration killing your jobs.","title":"Matlab Licenses"},{"location":"admin/resource-registration/#critical-partition","text":"The cluster provides a critical partition for jobs with deadlines (e.g., on paper submission or for tasks supporting clinical applications). Access to this partition has to be explicitely granted as with the other resources on this page. If you want to use the critical partition, please send an email to hpc-gatekeeper@bihealth.de with the following information: For how long do you want to have access? Please limit yourself to up to 6 months per request, you can extend the request afterwards with the same process. How frequently do you need the nodes, for how long at a time? Do you plan to do more interactive work or using batch jobs? At the moment, all requests will be granted by hpc-gatekeeper. Use the high-memory node by following the instructions How-To: Use the Critical Partition Be nice and cooperative with other users, e.g., ask arrange sharing of the node via email and phone. Type getent passwd USER_NAME on the cluster to see user's contact details.","title":"Critical Partition"},{"location":"best-practice/bashrc-guide/","text":"~/.bashrc Guide \u00b6","title":"~/.bashrc Guide"},{"location":"best-practice/bashrc-guide/#bashrc-guide","text":"","title":"~/.bashrc Guide"},{"location":"best-practice/env-modules/","text":"Custom Environment Modules \u00b6 This document contains a few tips for helping you using environment modules more effectively. As the general online documentation is lacking a bit, we also give the most popular commands here. How does it Work? \u00b6 Environment modules are descriptions of software packages. The module command is provided which allows the manipulation of environment variables such as PATH , MANPATH , etc., such that programs are available without passing the full path. Environment modules also allow specifying dependencies between packages and conflicting packages (e.g., when the same binary is available in two packages). Further, environment variables allow the parallel installation of different software versions in parallel and then using software \"a la carte\" in your projects. Popular Commands \u00b6 Querying \u00b6 List currently loaded modules: $ module list Show all available modules $ module avail Loading/Unloading Modules \u00b6 Load one module, make sure to use a specific version to avoid ambiguities. $ module load Jannovar/0.16-Java-1.7.0_80 Unload one module $ module unload Jannovar Unload all modules $ module purge Getting Help \u00b6 Get help for environment modules $ module help Get help for a particular environment module $ module help Jannovar/0.16-Java-1.7.0_80 Using your own Module Files \u00b6 You can also create your own environment modules. Simply create a directory with module files and then use module use for using the modules from the directory tree. $ module use path/to/modules FAQ: Why -bash: module: command not found ? \u00b6 On the login nodes, the module command is not installed. You should not run any computations there, so why would you need environment modules there? ;) meg-login2$ module -bash: module: command not found Use srun --pty bash -i to get to one of the compute nodes. Auto-loading a set of Modules \u00b6 You will certainly finding yourself using a set of programs regularly without it being part of the core cluster installation, e.g., SAMtools, or Python 3. Just putting the appropriate module load lines in your ~/.bashrc will generate warnings when logging into the login node. It is thus recommended to use the following snippet for loading modules automatically on logging into a compute node: case \" ${ HOSTNAME } \" in med-login* ) ;; * ) # load Python3 environment module module load Python/3.4.3-foss-2015a # Define path for temporary directories, don't forget to cleanup! # Also, this will only work after /fast is available. export TMPDIR = /fast/users/ $USER /scratch/tmp ;; esac","title":"Custom Environment Modules"},{"location":"best-practice/env-modules/#custom-environment-modules","text":"This document contains a few tips for helping you using environment modules more effectively. As the general online documentation is lacking a bit, we also give the most popular commands here.","title":"Custom Environment Modules"},{"location":"best-practice/env-modules/#how-does-it-work","text":"Environment modules are descriptions of software packages. The module command is provided which allows the manipulation of environment variables such as PATH , MANPATH , etc., such that programs are available without passing the full path. Environment modules also allow specifying dependencies between packages and conflicting packages (e.g., when the same binary is available in two packages). Further, environment variables allow the parallel installation of different software versions in parallel and then using software \"a la carte\" in your projects.","title":"How does it Work?"},{"location":"best-practice/env-modules/#popular-commands","text":"","title":"Popular Commands"},{"location":"best-practice/env-modules/#querying","text":"List currently loaded modules: $ module list Show all available modules $ module avail","title":"Querying"},{"location":"best-practice/env-modules/#loadingunloading-modules","text":"Load one module, make sure to use a specific version to avoid ambiguities. $ module load Jannovar/0.16-Java-1.7.0_80 Unload one module $ module unload Jannovar Unload all modules $ module purge","title":"Loading/Unloading Modules"},{"location":"best-practice/env-modules/#getting-help","text":"Get help for environment modules $ module help Get help for a particular environment module $ module help Jannovar/0.16-Java-1.7.0_80","title":"Getting Help"},{"location":"best-practice/env-modules/#using-your-own-module-files","text":"You can also create your own environment modules. Simply create a directory with module files and then use module use for using the modules from the directory tree. $ module use path/to/modules","title":"Using your own Module Files"},{"location":"best-practice/env-modules/#faq-why-bash-module-command-not-found","text":"On the login nodes, the module command is not installed. You should not run any computations there, so why would you need environment modules there? ;) meg-login2$ module -bash: module: command not found Use srun --pty bash -i to get to one of the compute nodes.","title":"FAQ: Why -bash: module: command not found?"},{"location":"best-practice/env-modules/#auto-loading-a-set-of-modules","text":"You will certainly finding yourself using a set of programs regularly without it being part of the core cluster installation, e.g., SAMtools, or Python 3. Just putting the appropriate module load lines in your ~/.bashrc will generate warnings when logging into the login node. It is thus recommended to use the following snippet for loading modules automatically on logging into a compute node: case \" ${ HOSTNAME } \" in med-login* ) ;; * ) # load Python3 environment module module load Python/3.4.3-foss-2015a # Define path for temporary directories, don't forget to cleanup! # Also, this will only work after /fast is available. export TMPDIR = /fast/users/ $USER /scratch/tmp ;; esac","title":"Auto-loading a set of Modules"},{"location":"best-practice/project-structure/","text":"Project File System Structure \u00b6 This Wiki page dscribes best pratices for managing your Bioinformatics projects on the file system. General Aims \u00b6 Mostly, you can separate the files in your projects/pipelines into one of the following categories: scripts (and their documentation) configuration data Ideally, scripts and documentation are independent of a given project and can be separated from the rest. Configuration is project-dependent and small and mostly does not contain any sensitive information (such as genotypes that allows for reidentification of donors). In most cases, data might be large and is either also stored elsewhere or together with scripts and configuration can be regenerated easily. There is no backup of work and scratch The cluster GPFS file system /fast is not appropriate for keeping around single \"master\" copies of data. You should have a backup and archival strategy for your valuable \"master\" copy data. Best Practices \u00b6 Scripts \u00b6 Your scripts should go into version control, e.g., a Git repository. Your scripts should be driven by command line parameters and/or configuration such that no paths etc. are hard-coded. If for a second data set, you need to make a copy of your scripts and adjust some variables, e.g., at the top, you're doing something in a suboptimal fashion. Rather, get these values from the command line or a configuration file and only store (sensible) defaults in your script where appropriate. Thus, ideally your scripts are not project-specific. Configuration \u00b6 Your configuration usually is project-specific. Your configuration should also go into version contro, e.g., a Git repository. In addition, you might need project-specific \"wrapper\" scripts that just call your project-independent script with the correct paths for your project. These scripts rather fall into the \"configuration\" category and should then live together with your configuration. Data \u00b6 Your data should go into a location separate from your scripts and configuration. Ideally, the raw input data is separated from the work and output files such that you can make these files and directories read-only and don't accidentally damage these files. Temporary files You really should keep temporary files in a temporary directory, set the environment variable TMPDIR appropriately and automatically clean them up (see Useful Tips: Temporary Files )** Best Practices in Practice \u00b6 But how can we put this into practice? Below, we give some examples of how to do this. Note that for simplicity's sake we put all scripts and configuration into one directory/repository contrary to the best practices above. This is for educational purposes only and you should strive for reuseable scripts where it makes sense and separate scripts and configuration. We will limit this to simple Bash scripts for education's purposes. You should be able to easily adapt this to your use cases. Thus, the aim is to separate the data from the non-data part of the project such that we can put the non-data part of the project into a separate location and under version control. We call the location for non-data part of the project the home location of your project and the location for the data part of the project the work location of your project. Overall, we have three options: Your processes are run in the home location and the sub directories used for execution are links into the work location using symlinks. Your processes are run in the work location and the scripts are linked into the work location using symlinks, OR the scripts are called from the home location, maybe through project-specific wrapper scripts. Example: Link config/scripts into work location (Option 1) \u00b6 Creating the work directory and copy the input files into work/input . $ mkdir -p project/work/input $ cp /fast/projects/cubit/tutorial/input/* project/work/input Creating the home space. We initialize a Git repository, properly configure the .gitignore file and add a README.md file. $ mkdir -p project/home $ cd project/home $ cat <<EOF >.gitignore *~ .*.sw? EOF $ cat <<EOF >README.md # Example Project This is an example project with config/scripts linked into work location. EOF $ git init $ git add .gitignore README.md $ git commit -m ' Initial project# We then create the a simple script for executing the mapping step and a configuration file that gives the path to the index and list of samples to process. $ mkdir scripts $ cat << \"EOF\" >scripts/run-mapping.sh #!/bin/bash # Unofficial Bash script mode, see: # http://redsymbol.net/articles/unofficial-bash-strict-mode/ set -euo pipefail # Get directory to bash file, see # https://stackoverflow.com/a/4774063/84349 SCRIPTPATH = \" $( cd \" $( dirname \" $0 \" ) \" ; pwd -P ) \" # Helper function to print help to stderr. help () { > & 2 echo \"Run Mapping Step\" > & 2 echo \"\" > & 2 echo \"run-mapping.sh [-c config.sh] [-h]\" } # Parse command line arguments into bash variables. CONFIG = while getopts \"hs:\" arg ; do case $arg in h ) help () exit ;; s ) CONFIG = $OPTARG ;; esac done # Print the executed commands. set -x # Load default configuration, then load configuration file if any was given. source $SCRIPTPATH /../config/default-config.sh if [[ -z \" $CONFIG \" ]] ; then source $CONFIG fi # Create output directory. mkdir -p output # Actually perform the mapping. This assumes that you have # made the bwa and samtools commands available, e.g., using conda. for sample in $SAMPLES ; do bwa mem \\ $BWA_INDEX \\ input/ ${ sample } _R1.fq.gz \\ input/ ${ sample } _R2.fq.gz \\ | samtools sort \\ -o output/ ${ sample } .bam \\ /dev/stdin done EOF $ chmod +x scripts/run-mapping.sh $ mkdir -p config $ cat << \"EOF\" >config/default-config.sh BWA_INDEX = /fast/projects/cubit/current/static_data/reference/GRCh37/hs37d5/hs37d5.fa SAMPLES = EOF $ cat << \"EOF\" >config/project-config.sh $ BWA_INDEX comes from default configuration already SAMPLES = test EOF This concludes the basic project setup. Now, to the symlinks: $ cd ../work $ ln -s ../home/scripts ../home/config . And, to the execution... $ ./scripts/run-mapping -c config/project-config.sh [ ... ] Example: Link Data Into Home (Option 2.1). \u00b6 We can reuse the project up to the statement \"This concludes the basic project setup\" in the example for option 1. Then, we can do the following: $ cd ../work $ mkdir -p output $ cd ../home $ cat <<\"EOF\" >>.gitignore # Ignore all data input/ work/ output/ EOF $ git add .gitignore $ git commit -m 'Ignoring data file in .gitignore' $ ln -s ../work ../output . And we can execute everything in the home directory. $ ./scripts/run-mapping -c config/project-config.sh [ ... ] Example: Wrapper Scripts in Home (Option 2.2) \u00b6 Again, we can reuse the project up to the statement \"This concludes the basic project setup\" in the example for option 1. Then, we do the following: $ cd ../work $ cat <<\"EOF\" >do-run-mapping.sh #!/bin/bash ../home/scripts/run-mapping.sh \\ -c ../home/config/project-config.sh EOF $ chmod +x do-run-mapping.sh Note that the the do-run.sh script could also go into the project-specific Git repository and be linked into the work directory. Finally, we can run our pipeline: $ cd ../work $ ./do-run-mapping.sh [ ... ]","title":"Project Structure"},{"location":"best-practice/project-structure/#project-file-system-structure","text":"This Wiki page dscribes best pratices for managing your Bioinformatics projects on the file system.","title":"Project File System Structure"},{"location":"best-practice/project-structure/#general-aims","text":"Mostly, you can separate the files in your projects/pipelines into one of the following categories: scripts (and their documentation) configuration data Ideally, scripts and documentation are independent of a given project and can be separated from the rest. Configuration is project-dependent and small and mostly does not contain any sensitive information (such as genotypes that allows for reidentification of donors). In most cases, data might be large and is either also stored elsewhere or together with scripts and configuration can be regenerated easily. There is no backup of work and scratch The cluster GPFS file system /fast is not appropriate for keeping around single \"master\" copies of data. You should have a backup and archival strategy for your valuable \"master\" copy data.","title":"General Aims"},{"location":"best-practice/project-structure/#best-practices","text":"","title":"Best Practices"},{"location":"best-practice/project-structure/#scripts","text":"Your scripts should go into version control, e.g., a Git repository. Your scripts should be driven by command line parameters and/or configuration such that no paths etc. are hard-coded. If for a second data set, you need to make a copy of your scripts and adjust some variables, e.g., at the top, you're doing something in a suboptimal fashion. Rather, get these values from the command line or a configuration file and only store (sensible) defaults in your script where appropriate. Thus, ideally your scripts are not project-specific.","title":"Scripts"},{"location":"best-practice/project-structure/#configuration","text":"Your configuration usually is project-specific. Your configuration should also go into version contro, e.g., a Git repository. In addition, you might need project-specific \"wrapper\" scripts that just call your project-independent script with the correct paths for your project. These scripts rather fall into the \"configuration\" category and should then live together with your configuration.","title":"Configuration"},{"location":"best-practice/project-structure/#data","text":"Your data should go into a location separate from your scripts and configuration. Ideally, the raw input data is separated from the work and output files such that you can make these files and directories read-only and don't accidentally damage these files. Temporary files You really should keep temporary files in a temporary directory, set the environment variable TMPDIR appropriately and automatically clean them up (see Useful Tips: Temporary Files )**","title":"Data"},{"location":"best-practice/project-structure/#best-practices-in-practice","text":"But how can we put this into practice? Below, we give some examples of how to do this. Note that for simplicity's sake we put all scripts and configuration into one directory/repository contrary to the best practices above. This is for educational purposes only and you should strive for reuseable scripts where it makes sense and separate scripts and configuration. We will limit this to simple Bash scripts for education's purposes. You should be able to easily adapt this to your use cases. Thus, the aim is to separate the data from the non-data part of the project such that we can put the non-data part of the project into a separate location and under version control. We call the location for non-data part of the project the home location of your project and the location for the data part of the project the work location of your project. Overall, we have three options: Your processes are run in the home location and the sub directories used for execution are links into the work location using symlinks. Your processes are run in the work location and the scripts are linked into the work location using symlinks, OR the scripts are called from the home location, maybe through project-specific wrapper scripts.","title":"Best Practices in Practice"},{"location":"best-practice/project-structure/#example-link-configscripts-into-work-location-option-1","text":"Creating the work directory and copy the input files into work/input . $ mkdir -p project/work/input $ cp /fast/projects/cubit/tutorial/input/* project/work/input Creating the home space. We initialize a Git repository, properly configure the .gitignore file and add a README.md file. $ mkdir -p project/home $ cd project/home $ cat <<EOF >.gitignore *~ .*.sw? EOF $ cat <<EOF >README.md # Example Project This is an example project with config/scripts linked into work location. EOF $ git init $ git add .gitignore README.md $ git commit -m ' Initial project# We then create the a simple script for executing the mapping step and a configuration file that gives the path to the index and list of samples to process. $ mkdir scripts $ cat << \"EOF\" >scripts/run-mapping.sh #!/bin/bash # Unofficial Bash script mode, see: # http://redsymbol.net/articles/unofficial-bash-strict-mode/ set -euo pipefail # Get directory to bash file, see # https://stackoverflow.com/a/4774063/84349 SCRIPTPATH = \" $( cd \" $( dirname \" $0 \" ) \" ; pwd -P ) \" # Helper function to print help to stderr. help () { > & 2 echo \"Run Mapping Step\" > & 2 echo \"\" > & 2 echo \"run-mapping.sh [-c config.sh] [-h]\" } # Parse command line arguments into bash variables. CONFIG = while getopts \"hs:\" arg ; do case $arg in h ) help () exit ;; s ) CONFIG = $OPTARG ;; esac done # Print the executed commands. set -x # Load default configuration, then load configuration file if any was given. source $SCRIPTPATH /../config/default-config.sh if [[ -z \" $CONFIG \" ]] ; then source $CONFIG fi # Create output directory. mkdir -p output # Actually perform the mapping. This assumes that you have # made the bwa and samtools commands available, e.g., using conda. for sample in $SAMPLES ; do bwa mem \\ $BWA_INDEX \\ input/ ${ sample } _R1.fq.gz \\ input/ ${ sample } _R2.fq.gz \\ | samtools sort \\ -o output/ ${ sample } .bam \\ /dev/stdin done EOF $ chmod +x scripts/run-mapping.sh $ mkdir -p config $ cat << \"EOF\" >config/default-config.sh BWA_INDEX = /fast/projects/cubit/current/static_data/reference/GRCh37/hs37d5/hs37d5.fa SAMPLES = EOF $ cat << \"EOF\" >config/project-config.sh $ BWA_INDEX comes from default configuration already SAMPLES = test EOF This concludes the basic project setup. Now, to the symlinks: $ cd ../work $ ln -s ../home/scripts ../home/config . And, to the execution... $ ./scripts/run-mapping -c config/project-config.sh [ ... ]","title":"Example: Link config/scripts into work location (Option 1)"},{"location":"best-practice/project-structure/#example-link-data-into-home-option-21","text":"We can reuse the project up to the statement \"This concludes the basic project setup\" in the example for option 1. Then, we can do the following: $ cd ../work $ mkdir -p output $ cd ../home $ cat <<\"EOF\" >>.gitignore # Ignore all data input/ work/ output/ EOF $ git add .gitignore $ git commit -m 'Ignoring data file in .gitignore' $ ln -s ../work ../output . And we can execute everything in the home directory. $ ./scripts/run-mapping -c config/project-config.sh [ ... ]","title":"Example: Link Data Into Home (Option 2.1)."},{"location":"best-practice/project-structure/#example-wrapper-scripts-in-home-option-22","text":"Again, we can reuse the project up to the statement \"This concludes the basic project setup\" in the example for option 1. Then, we do the following: $ cd ../work $ cat <<\"EOF\" >do-run-mapping.sh #!/bin/bash ../home/scripts/run-mapping.sh \\ -c ../home/config/project-config.sh EOF $ chmod +x do-run-mapping.sh Note that the the do-run.sh script could also go into the project-specific Git repository and be linked into the work directory. Finally, we can run our pipeline: $ cd ../work $ ./do-run-mapping.sh [ ... ]","title":"Example: Wrapper Scripts in Home (Option 2.2)"},{"location":"best-practice/screen-tmux/","text":"Screen and Tmux Best Pratice \u00b6 The program screen allows you to detach your session from your current login session. So in case you get disconnected your screen session will stay alive. Hint You have to reconnect to screen on the machine that you started it. We thus recommend starting it only on the login nodes and not on a compute node. Start and terminat a screen session \u00b6 You start a new screen session by $ screen When you are in a screen session you can terminate it with $ exit so its gone then. Detach a screen session \u00b6 If you want to detach your screen session press Ctrl+a d List screen sessions \u00b6 To list all your screen sessions run $ screen -ls There is a screen on: 2441.pts-1.med0236 (Detached) 1 Socket in /var/run/screen/S-kbentel. Reattach screen session \u00b6 To reattach a screen session run $ screen -r screen_session_id If you do not know the screen_session_id you can get it with screen -ls , e.g. 2441.pts-1.med0236 in the example above. You do not have to type the whole screen_session_id only as much as is necessary to identify it uniquely. In case there is only one screen session detached it is enough to run screen -r Kill a detached screen session \u00b6 Sometimes it is necessary to kill a detached screen session. This is done with the command $ screen -X -S screen_session_id quit Multiple windows in a screen session \u00b6 It is possible to have multiple windows in a screen session. So suppose you are logged into a screen session, these are the relevant shortcuts new win: Ctrl+a c next/previous win: Ctrl+a n/p To terminate a window just enter $ exit Configuration file \u00b6 Here is a sensible screen configuration. Save it as ~/.screenrc . screenrc Fix a broken screen session \u00b6 In case your screen session doesn't write to the terminal correctly, i.e. the formatting of the output is broken, you can fix it by typing to the terminal: $ tput smam","title":"Using Screen/Tmux"},{"location":"best-practice/screen-tmux/#screen-and-tmux-best-pratice","text":"The program screen allows you to detach your session from your current login session. So in case you get disconnected your screen session will stay alive. Hint You have to reconnect to screen on the machine that you started it. We thus recommend starting it only on the login nodes and not on a compute node.","title":"Screen and Tmux Best Pratice"},{"location":"best-practice/screen-tmux/#start-and-terminat-a-screen-session","text":"You start a new screen session by $ screen When you are in a screen session you can terminate it with $ exit so its gone then.","title":"Start and terminat a screen session"},{"location":"best-practice/screen-tmux/#detach-a-screen-session","text":"If you want to detach your screen session press Ctrl+a d","title":"Detach a screen session"},{"location":"best-practice/screen-tmux/#list-screen-sessions","text":"To list all your screen sessions run $ screen -ls There is a screen on: 2441.pts-1.med0236 (Detached) 1 Socket in /var/run/screen/S-kbentel.","title":"List screen sessions"},{"location":"best-practice/screen-tmux/#reattach-screen-session","text":"To reattach a screen session run $ screen -r screen_session_id If you do not know the screen_session_id you can get it with screen -ls , e.g. 2441.pts-1.med0236 in the example above. You do not have to type the whole screen_session_id only as much as is necessary to identify it uniquely. In case there is only one screen session detached it is enough to run screen -r","title":"Reattach screen session"},{"location":"best-practice/screen-tmux/#kill-a-detached-screen-session","text":"Sometimes it is necessary to kill a detached screen session. This is done with the command $ screen -X -S screen_session_id quit","title":"Kill a detached screen session"},{"location":"best-practice/screen-tmux/#multiple-windows-in-a-screen-session","text":"It is possible to have multiple windows in a screen session. So suppose you are logged into a screen session, these are the relevant shortcuts new win: Ctrl+a c next/previous win: Ctrl+a n/p To terminate a window just enter $ exit","title":"Multiple windows in a screen session"},{"location":"best-practice/screen-tmux/#configuration-file","text":"Here is a sensible screen configuration. Save it as ~/.screenrc . screenrc","title":"Configuration file"},{"location":"best-practice/screen-tmux/#fix-a-broken-screen-session","text":"In case your screen session doesn't write to the terminal correctly, i.e. the formatting of the output is broken, you can fix it by typing to the terminal: $ tput smam","title":"Fix a broken screen session"},{"location":"best-practice/software-craftmanship/","text":"General Software Craftmanship \u00b6 Computer software, or simply software, is a generic term that refers to a collection of data or computer instructions that tell the computer how to work, in contrast to the physical hardware from which the system is built, that actually performs the work. -- Wikipedia: Software As you will most probably never have contact with the HPC system hardware, everything you interact with on the HPC is software . All of your scripts, your configuration files, programs installed by you or administration, and all of your data. This should also answer the question why you should care about software and why you should try to create and use software of a minimal quality. Software craftsmanship is an approach to software development that emphasizes the coding skills of the software developers themselves. -- Wikipedia: Software Craftmanship This Wiki page is not mean to give you an introduction of creating good software but rather collect a (growing) list of easy-to-use and high-impact points to improve software quality. Also, it provides pointers to resources elsewhere on the internet. Use Version Control \u00b6 Use a version control system for your configuration and your code. Full stop. Modern version control systems are Git and Subversion. Official Git Documentation Github Help Fix Common Git Problems Do not Share Git/SVN Checkouts for Multiple Users \u00b6 Every user should have their own Git/Subversion checkout. Otherwise you are inviting a large number of problems. Document Your Code \u00b6 This includes programmer-level documentation in your source code, both inline and per code unit (e.g., function/class) top-level documentation, e.g., in README files. Document Your Data \u00b6 Document where you got things from, how to re-download, etc. E.g., put a README file into each of your data top level directories. Use Checksums \u00b6 Use MD5 or other checksums for your data. For example, md5sum and hashdeep are useful utilities for computing and checking them: md5sum How-To (tools such as sha256sum work the same...) hashdeep How-To Use a Workflow Management System \u00b6 Use some system for managing your workflows. These systems support you by Detect failures and don't continue working with broken data, continue where you left off when someting breaks, make things more reproducible, allow distribution of jobs on the cluster. Snakemake is a popular workflow management system widely used in Bioinformatics. A minimal approach is using Makefiles . Understand Bash and Shell Exit Codes \u00b6 If you don't want to use a workflow management system, e.g., for one-step jobs, you should at least understand Bash job management and exit codes. For example, you can use if/then/fi in Bash together with exit codes to: Only call a command if the previous command succeded. Remove incomplete output files in case of errors. if [[ ! -e file.md5 ]] ; then md5sum file >file.md5 \\ || rm -f file.md5 fi Also, learn about the inofficial Bash strict mode .","title":"Software Craftmanship"},{"location":"best-practice/software-craftmanship/#general-software-craftmanship","text":"Computer software, or simply software, is a generic term that refers to a collection of data or computer instructions that tell the computer how to work, in contrast to the physical hardware from which the system is built, that actually performs the work. -- Wikipedia: Software As you will most probably never have contact with the HPC system hardware, everything you interact with on the HPC is software . All of your scripts, your configuration files, programs installed by you or administration, and all of your data. This should also answer the question why you should care about software and why you should try to create and use software of a minimal quality. Software craftsmanship is an approach to software development that emphasizes the coding skills of the software developers themselves. -- Wikipedia: Software Craftmanship This Wiki page is not mean to give you an introduction of creating good software but rather collect a (growing) list of easy-to-use and high-impact points to improve software quality. Also, it provides pointers to resources elsewhere on the internet.","title":"General Software Craftmanship"},{"location":"best-practice/software-craftmanship/#use-version-control","text":"Use a version control system for your configuration and your code. Full stop. Modern version control systems are Git and Subversion. Official Git Documentation Github Help Fix Common Git Problems","title":"Use Version Control"},{"location":"best-practice/software-craftmanship/#do-not-share-gitsvn-checkouts-for-multiple-users","text":"Every user should have their own Git/Subversion checkout. Otherwise you are inviting a large number of problems.","title":"Do not Share Git/SVN Checkouts for Multiple Users"},{"location":"best-practice/software-craftmanship/#document-your-code","text":"This includes programmer-level documentation in your source code, both inline and per code unit (e.g., function/class) top-level documentation, e.g., in README files.","title":"Document Your Code"},{"location":"best-practice/software-craftmanship/#document-your-data","text":"Document where you got things from, how to re-download, etc. E.g., put a README file into each of your data top level directories.","title":"Document Your Data"},{"location":"best-practice/software-craftmanship/#use-checksums","text":"Use MD5 or other checksums for your data. For example, md5sum and hashdeep are useful utilities for computing and checking them: md5sum How-To (tools such as sha256sum work the same...) hashdeep How-To","title":"Use Checksums"},{"location":"best-practice/software-craftmanship/#use-a-workflow-management-system","text":"Use some system for managing your workflows. These systems support you by Detect failures and don't continue working with broken data, continue where you left off when someting breaks, make things more reproducible, allow distribution of jobs on the cluster. Snakemake is a popular workflow management system widely used in Bioinformatics. A minimal approach is using Makefiles .","title":"Use a Workflow Management System"},{"location":"best-practice/software-craftmanship/#understand-bash-and-shell-exit-codes","text":"If you don't want to use a workflow management system, e.g., for one-step jobs, you should at least understand Bash job management and exit codes. For example, you can use if/then/fi in Bash together with exit codes to: Only call a command if the previous command succeded. Remove incomplete output files in case of errors. if [[ ! -e file.md5 ]] ; then md5sum file >file.md5 \\ || rm -f file.md5 fi Also, learn about the inofficial Bash strict mode .","title":"Understand Bash and Shell Exit Codes"},{"location":"best-practice/software-installation-with-conda/","text":"Software Installation with Conda \u00b6 Conda \u00b6 For the management of the bioinformatics software on the BIH cluster we are using conda. Conda is a package management system that is based on channels, and one of those channels provides a huge selection of bioinformatics software. Conda is written in Python and is based on recipes, such that everybody can write recipes for missing software (if there is any). In general the packages are pre-compiled and conda just downloads the binaries from the conda servers. You are in charge of managing your own software stack, but conda makes it easy to do so. We will provide you with a description on how to install conda and how to use it. Of course there are many online resources that you can also use. Please find a list at the end of the document. Also note that some system-level software is managed through environment modules. See System-near Software Provided by HPC Administration below. Premise \u00b6 When you logged into the cluster, please make sure that you also executed srun to log into a computation node and perform the software installation there. Installing conda \u00b6 med-login1:~$ srun --pty bash -i med0127:~$ wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh med0127:~$ bash Miniconda3-latest-Linux-x86_64.sh -b -f -p $HOME /work/miniconda This will install conda to $HOME/work/miniconda . This path can be changed to your liking. Please note that the $HOME folder has limited space (an exception is the subfolder $HOME/work which has no space limit). NB: $HOME/scratch is not appropriate as files placed there will be removed automatically after 4 weeks. To make it available upon login, extend and export the $PATH variable with the installation path + /bin and add it to your $HOME/.bashrc : case \" ${ SLURMD_NODENAME - ${ HOSTNAME }} \" in med-login* ) ;; * ) export PATH = $HOME /work/miniconda/condabin: $PATH ;; esac The above code makes sure that you don't have conda available on the login nodes, where you are not allowed to start any computations. To make bioinformatics software available, we have to add the bioconda and some other channels to the conda configuration: med0127:~$ conda config --add channels bioconda med0127:~$ conda config --add channels default med0127:~$ conda config --add channels conda-forge You can also add channels to your liking. Installing software with conda \u00b6 Installing packages with conda is straight forward: med0127:~$ conda install <package> This will install a package into the conda root environment. We will explain environments in detail in the next section. To search for a package, e.g. to find the correct name in conda or if it exists at all, issue the command: med0127:~$ conda search <string> To choose a specific version (conda will install the latest version that is compatible with the current installed Python version), you can provide the version as follows: med0127:~$ conda install <package> = <version> Creating an environment \u00b6 Conda lets you create environments, such that you can test things in a different environment or group your software. Another common use case is to have different environments for the different Python versions. Since conda is Python-based, conflicting packages will mostly struggle with the Python version. By default, conda will install packages into its root environment. Please note that software that does not depend on Python and is installed in the root environment, is is available in all other environments. To create a Python 2.7 environment and activate it, issue the following commands: med0127:~$ conda create -n py27 python = 2 .7 med0127:~$ source activate py27 ( py27 ) med0127:~$ From now on, conda will install packages into the py27 environment when you issue the install command. To switch back to the root environment, simply deactivate the py27 environment: ( py27 ) med0127:~$ source deactivate py27 med0127:~$ Recommended packages \u00b6 Program Version package Python 3.5 python=3.5 snakemake latest snakemake drmaa samtools latest samtools To get you going, issue the following command. This will install Python 3.5 into your root environment alongside with Snakemake and samtools. By default, conda starts with Python 3.6, but most packages are not adapted to that Python version yet. med0127:~$ conda install python = 3 .5 snakemake drmaa samtools Please also read this document on how to use Snakemake with DRMAA.","title":"Install with Conda"},{"location":"best-practice/software-installation-with-conda/#software-installation-with-conda","text":"","title":"Software Installation with Conda"},{"location":"best-practice/software-installation-with-conda/#conda","text":"For the management of the bioinformatics software on the BIH cluster we are using conda. Conda is a package management system that is based on channels, and one of those channels provides a huge selection of bioinformatics software. Conda is written in Python and is based on recipes, such that everybody can write recipes for missing software (if there is any). In general the packages are pre-compiled and conda just downloads the binaries from the conda servers. You are in charge of managing your own software stack, but conda makes it easy to do so. We will provide you with a description on how to install conda and how to use it. Of course there are many online resources that you can also use. Please find a list at the end of the document. Also note that some system-level software is managed through environment modules. See System-near Software Provided by HPC Administration below.","title":"Conda"},{"location":"best-practice/software-installation-with-conda/#premise","text":"When you logged into the cluster, please make sure that you also executed srun to log into a computation node and perform the software installation there.","title":"Premise"},{"location":"best-practice/software-installation-with-conda/#installing-conda","text":"med-login1:~$ srun --pty bash -i med0127:~$ wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh med0127:~$ bash Miniconda3-latest-Linux-x86_64.sh -b -f -p $HOME /work/miniconda This will install conda to $HOME/work/miniconda . This path can be changed to your liking. Please note that the $HOME folder has limited space (an exception is the subfolder $HOME/work which has no space limit). NB: $HOME/scratch is not appropriate as files placed there will be removed automatically after 4 weeks. To make it available upon login, extend and export the $PATH variable with the installation path + /bin and add it to your $HOME/.bashrc : case \" ${ SLURMD_NODENAME - ${ HOSTNAME }} \" in med-login* ) ;; * ) export PATH = $HOME /work/miniconda/condabin: $PATH ;; esac The above code makes sure that you don't have conda available on the login nodes, where you are not allowed to start any computations. To make bioinformatics software available, we have to add the bioconda and some other channels to the conda configuration: med0127:~$ conda config --add channels bioconda med0127:~$ conda config --add channels default med0127:~$ conda config --add channels conda-forge You can also add channels to your liking.","title":"Installing conda"},{"location":"best-practice/software-installation-with-conda/#installing-software-with-conda","text":"Installing packages with conda is straight forward: med0127:~$ conda install <package> This will install a package into the conda root environment. We will explain environments in detail in the next section. To search for a package, e.g. to find the correct name in conda or if it exists at all, issue the command: med0127:~$ conda search <string> To choose a specific version (conda will install the latest version that is compatible with the current installed Python version), you can provide the version as follows: med0127:~$ conda install <package> = <version>","title":"Installing software with conda"},{"location":"best-practice/software-installation-with-conda/#creating-an-environment","text":"Conda lets you create environments, such that you can test things in a different environment or group your software. Another common use case is to have different environments for the different Python versions. Since conda is Python-based, conflicting packages will mostly struggle with the Python version. By default, conda will install packages into its root environment. Please note that software that does not depend on Python and is installed in the root environment, is is available in all other environments. To create a Python 2.7 environment and activate it, issue the following commands: med0127:~$ conda create -n py27 python = 2 .7 med0127:~$ source activate py27 ( py27 ) med0127:~$ From now on, conda will install packages into the py27 environment when you issue the install command. To switch back to the root environment, simply deactivate the py27 environment: ( py27 ) med0127:~$ source deactivate py27 med0127:~$","title":"Creating an environment"},{"location":"best-practice/software-installation-with-conda/#recommended-packages","text":"Program Version package Python 3.5 python=3.5 snakemake latest snakemake drmaa samtools latest samtools To get you going, issue the following command. This will install Python 3.5 into your root environment alongside with Snakemake and samtools. By default, conda starts with Python 3.6, but most packages are not adapted to that Python version yet. med0127:~$ conda install python = 3 .5 snakemake drmaa samtools Please also read this document on how to use Snakemake with DRMAA.","title":"Recommended packages"},{"location":"best-practice/temp-files/","text":"Temporary Files \u00b6 Often, it is necessary to use temporary files, i.e., write something out in the middle of your program, read it in again later, and then discard these files. For example, samtools sort has to write out chunks of sorted read alignments for allowing to sort files larger than main memory. Environment Variable TMPDIR \u00b6 Traditionally, in Unix, the environment variables TMPDIR is used for storing the location of the temporary directory. When undefined, usually /tmp is used. Temporary Directories on the BIH Cluster \u00b6 Generally, there are two locations where you could put temporary files: /fast/users/$USER/scratch/tmp -- inside your scratch folder on the fast GPFS file system; this location is available from all cluster nodes /tmp -- on the local node's temporary folder; this location is only available on the node itself Best Practice: Use /fast/users/$USER/scratch/tmp \u00b6 Use GPFS-based TMPDIR Generally setup your environment to use /fast/users/$USER/scratch/tmp as filling the local disk of a node with forgotten files can cause a lot of problems. Ideally, you append the following to your ~/.bashrc to use /fast/users/$USER/scratch/tmp as the temporary directory. This will also create the directory if it does not exist. export TMPDIR = /fast/users/ $USER /scratch/tmp mkdir -p $TMPDIR Prepending this to your job scripts is also recommended as it will ensure that the temporary directory exists. TMPDIR and the scheduler \u00b6 In the older nodes, the local disk is a relatively slow spinning disk, in the newer nodes, the local disk is a relatively fast SSD. Further, the local disk is independent from the GPFS file system, so I/O volume to it does not affect the network or any other job on other nodes. SGE and TMPDIR Note that the SGE scheduler will override any value given to TMPDIR from the outside. If you want to override TMPDIR you have to do this within your job script. For smaller I/O volumes, it is probably sensible to just use TMPDIR from the scheduler. For larger ones, you probably want to fall back to /fast/users/$USER/scratch/tmp . However, this is then not auto-cleaned any more. Here is how you get this behaviour. Use Bash Trapcs \u00b6 You can use the following code at the top of your job script to set TMPDIR to the location in your home directory and get the directory automatically cleaned when the job is done (regardless of successful or erroneous completion): # First, point TMPDIR to the scratch in your home as mktemp will use thi export TMPDIR = $HOME /scratch/tmp # Second, create another unique temporary directory within this directory export TMPDIR = $( mktemp -d ) # Finally, setup the cleanup trap trap \"rm -rf $TMPDIR \" EXIT","title":"Temporary Files"},{"location":"best-practice/temp-files/#temporary-files","text":"Often, it is necessary to use temporary files, i.e., write something out in the middle of your program, read it in again later, and then discard these files. For example, samtools sort has to write out chunks of sorted read alignments for allowing to sort files larger than main memory.","title":"Temporary Files"},{"location":"best-practice/temp-files/#environment-variable-tmpdir","text":"Traditionally, in Unix, the environment variables TMPDIR is used for storing the location of the temporary directory. When undefined, usually /tmp is used.","title":"Environment Variable TMPDIR"},{"location":"best-practice/temp-files/#temporary-directories-on-the-bih-cluster","text":"Generally, there are two locations where you could put temporary files: /fast/users/$USER/scratch/tmp -- inside your scratch folder on the fast GPFS file system; this location is available from all cluster nodes /tmp -- on the local node's temporary folder; this location is only available on the node itself","title":"Temporary Directories on the BIH Cluster"},{"location":"best-practice/temp-files/#best-practice-use-fastusersuserscratchtmp","text":"Use GPFS-based TMPDIR Generally setup your environment to use /fast/users/$USER/scratch/tmp as filling the local disk of a node with forgotten files can cause a lot of problems. Ideally, you append the following to your ~/.bashrc to use /fast/users/$USER/scratch/tmp as the temporary directory. This will also create the directory if it does not exist. export TMPDIR = /fast/users/ $USER /scratch/tmp mkdir -p $TMPDIR Prepending this to your job scripts is also recommended as it will ensure that the temporary directory exists.","title":"Best Practice:  Use /fast/users/$USER/scratch/tmp"},{"location":"best-practice/temp-files/#tmpdir-and-the-scheduler","text":"In the older nodes, the local disk is a relatively slow spinning disk, in the newer nodes, the local disk is a relatively fast SSD. Further, the local disk is independent from the GPFS file system, so I/O volume to it does not affect the network or any other job on other nodes. SGE and TMPDIR Note that the SGE scheduler will override any value given to TMPDIR from the outside. If you want to override TMPDIR you have to do this within your job script. For smaller I/O volumes, it is probably sensible to just use TMPDIR from the scheduler. For larger ones, you probably want to fall back to /fast/users/$USER/scratch/tmp . However, this is then not auto-cleaned any more. Here is how you get this behaviour.","title":"TMPDIR and the scheduler"},{"location":"best-practice/temp-files/#use-bash-trapcs","text":"You can use the following code at the top of your job script to set TMPDIR to the location in your home directory and get the directory automatically cleaned when the job is done (regardless of successful or erroneous completion): # First, point TMPDIR to the scratch in your home as mktemp will use thi export TMPDIR = $HOME /scratch/tmp # Second, create another unique temporary directory within this directory export TMPDIR = $( mktemp -d ) # Finally, setup the cleanup trap trap \"rm -rf $TMPDIR \" EXIT","title":"Use Bash Trapcs"},{"location":"connecting/from-external/","text":"Connecting from External Networks \u00b6 This page describes how to connect to the BIH HPC from external networks (e.g., another university or from your home). The options differ depending on your home organization and are described in detail below. MDC users can use the MDC SSH gateway/hop node, or MDC VPN. Charite users can use the Charite VPN with \"VPN Zusatzantrag B\". Getting Help with VPN and Gateway Nodes Please note that the VPNs and gateway nodes are maintained by the central IT departments of Charite/MDC. BIH HPC IT cannot assist you in problems with these serves. Authorative information and documentation is provided by the central IT departments as well. SSH Key Gotchas You should use separate SSH key pairs for your workstation, laptop, home computer etc. As a reminder, you will have to register the SSH keys with your home IT organization ( MDC or Charite ). When using gateway nodes, please make sure to use SSH key agents and agent forwarding ( ssh flag \" -A \"). For MDC Users \u00b6 The general prerequisite is to register the SSH keys with MDC IT via \"persdb\". MDC Gateway Node \u00b6 The host name of the MDC gateway node is ssh1.mdc-berlin.de . You will connect to this node with your plain MDC user name , e.g., doej without the \" _m \" suffix. Do not forget the \" -A \" flag for SSH agent key forwarding. Once you are on the gateway node, connect as if you were on your workstation: # SSH key agent must be active at this point! host:~$ ssh -A -l user ssh1.mdc-berlin.de # If the SSH key agent does not run on your client host then the following # will not work and the SSH key will not be available! user@sl-it-p-ssh1:~$ ssh -l user_m med-login1.bihealth.org MDC VPN \u00b6 You can find the instructions for getting MDC VPN access here in the MDC intranet below the \"VPN\" heading. Please contact helpdesk@mdc-berlin.de for getting VPN access. Install the VPN client and then start it. Once VPN has been activated you can SSH to the HPC just as from your workstation. host:~$ ssh -l user_m med-login1.bihealth.org For Charite Users \u00b6 The general prerequisite is to register the SSH keys with Charite IT via zugang.charite.de . You will then have to apply for (1) general VPN access and (2) extended VPN access to BIH HPC. Finally, you will be able to connect to BIH HPC from VPN. General Charite VPN Access \u00b6 You need to apply for Charite VPN access, if you haven't done so already. The form can be found in the Charite Intranet and contains further instructions. Zusatzantrag B (Recommended) \u00b6 You can find Zusatzantrag B in the Charite intranet. Fill it out and ship it in addition to the general VPN access form from above. Charite Helpdesk can help you with any questions. Once you have been granted VPN access, start the client and connect to VPN. You will then be able to connect from your client in the VPN just as you do from your workstation. host:~$ ssh -l jdoe_c med-login1.bihealth.org Charite VDI \u00b6 Alternative to using Zusatzantrag B, you can also get access to the Charite VDI (Virtual Desktop Infrastructure). Here, you connect to a virtual desktop computer which is in the Charite network. From there, you can connect to the BIH HPC system. You need to apply for extended VPN access to be able to access the BIH VDI. The form can be found here . It is important to tick Dienst(e) , enter HTTPS and as target view.bihealth.org . Please write to helpdesk@charite.de with the request to access the BIH VDI . When the access has been set up, follow the instructions on client configuration for Windows, after logging in to the BIH VDI .","title":"From External"},{"location":"connecting/from-external/#connecting-from-external-networks","text":"This page describes how to connect to the BIH HPC from external networks (e.g., another university or from your home). The options differ depending on your home organization and are described in detail below. MDC users can use the MDC SSH gateway/hop node, or MDC VPN. Charite users can use the Charite VPN with \"VPN Zusatzantrag B\". Getting Help with VPN and Gateway Nodes Please note that the VPNs and gateway nodes are maintained by the central IT departments of Charite/MDC. BIH HPC IT cannot assist you in problems with these serves. Authorative information and documentation is provided by the central IT departments as well. SSH Key Gotchas You should use separate SSH key pairs for your workstation, laptop, home computer etc. As a reminder, you will have to register the SSH keys with your home IT organization ( MDC or Charite ). When using gateway nodes, please make sure to use SSH key agents and agent forwarding ( ssh flag \" -A \").","title":"Connecting from External Networks"},{"location":"connecting/from-external/#for-mdc-users","text":"The general prerequisite is to register the SSH keys with MDC IT via \"persdb\".","title":"For MDC Users"},{"location":"connecting/from-external/#mdc-gateway-node","text":"The host name of the MDC gateway node is ssh1.mdc-berlin.de . You will connect to this node with your plain MDC user name , e.g., doej without the \" _m \" suffix. Do not forget the \" -A \" flag for SSH agent key forwarding. Once you are on the gateway node, connect as if you were on your workstation: # SSH key agent must be active at this point! host:~$ ssh -A -l user ssh1.mdc-berlin.de # If the SSH key agent does not run on your client host then the following # will not work and the SSH key will not be available! user@sl-it-p-ssh1:~$ ssh -l user_m med-login1.bihealth.org","title":"MDC Gateway Node"},{"location":"connecting/from-external/#mdc-vpn","text":"You can find the instructions for getting MDC VPN access here in the MDC intranet below the \"VPN\" heading. Please contact helpdesk@mdc-berlin.de for getting VPN access. Install the VPN client and then start it. Once VPN has been activated you can SSH to the HPC just as from your workstation. host:~$ ssh -l user_m med-login1.bihealth.org","title":"MDC VPN"},{"location":"connecting/from-external/#for-charite-users","text":"The general prerequisite is to register the SSH keys with Charite IT via zugang.charite.de . You will then have to apply for (1) general VPN access and (2) extended VPN access to BIH HPC. Finally, you will be able to connect to BIH HPC from VPN.","title":"For Charite Users"},{"location":"connecting/from-external/#general-charite-vpn-access","text":"You need to apply for Charite VPN access, if you haven't done so already. The form can be found in the Charite Intranet and contains further instructions.","title":"General Charite VPN Access"},{"location":"connecting/from-external/#zusatzantrag-b-recommended","text":"You can find Zusatzantrag B in the Charite intranet. Fill it out and ship it in addition to the general VPN access form from above. Charite Helpdesk can help you with any questions. Once you have been granted VPN access, start the client and connect to VPN. You will then be able to connect from your client in the VPN just as you do from your workstation. host:~$ ssh -l jdoe_c med-login1.bihealth.org","title":"Zusatzantrag B (Recommended)"},{"location":"connecting/from-external/#charite-vdi","text":"Alternative to using Zusatzantrag B, you can also get access to the Charite VDI (Virtual Desktop Infrastructure). Here, you connect to a virtual desktop computer which is in the Charite network. From there, you can connect to the BIH HPC system. You need to apply for extended VPN access to be able to access the BIH VDI. The form can be found here . It is important to tick Dienst(e) , enter HTTPS and as target view.bihealth.org . Please write to helpdesk@charite.de with the request to access the BIH VDI . When the access has been set up, follow the instructions on client configuration for Windows, after logging in to the BIH VDI .","title":"Charite VDI"},{"location":"connecting/ssh-client-windows/","text":"Installing SSH Client for Windows \u00b6 We recommend to use the program MobaXterm on Windows. MobaXterm is a software that allows you to connect to an SSH server, much like PuTTy, but also maintains your SSH key. Alternative SSH Clients for Windows Another popular option is PuTTy but many users have problems configuring it correctly with SSH keys. On Windows 10, you can also install Windows Subsystem for Linux , e.g., together with WSL Terminal . This is not for the faint of heart (but great if you're a Unix head). Navigate to https://mobaxterm.mobatek.net/download-home-edition.html Download either the Portable edition (blue button lefthand-side, if you have no admin rights, e.g. as Charite or MDC user) or the Installer edition (green button righthand-side). Install or unpack MobaXterm and start the software. As a Charite user, please cancel any firewall warnings that pop up.","title":"Windows SSH Client"},{"location":"connecting/ssh-client-windows/#installing-ssh-client-for-windows","text":"We recommend to use the program MobaXterm on Windows. MobaXterm is a software that allows you to connect to an SSH server, much like PuTTy, but also maintains your SSH key. Alternative SSH Clients for Windows Another popular option is PuTTy but many users have problems configuring it correctly with SSH keys. On Windows 10, you can also install Windows Subsystem for Linux , e.g., together with WSL Terminal . This is not for the faint of heart (but great if you're a Unix head). Navigate to https://mobaxterm.mobatek.net/download-home-edition.html Download either the Portable edition (blue button lefthand-side, if you have no admin rights, e.g. as Charite or MDC user) or the Installer edition (green button righthand-side). Install or unpack MobaXterm and start the software. As a Charite user, please cancel any firewall warnings that pop up.","title":"Installing SSH Client for Windows"},{"location":"connecting/configure-ssh/connection-problems/","text":"Debugging Connection Problems \u00b6 When you encounter problems with the login to the cluster although we indicated that you should have access, depending on the issue, here is a list of how to solve the problem: I'm unable to connect to the cluster \u00b6 When you can't reach the cluster, please make sure that you are in the right network. Connections are only possible from within Charit\u00e9 or MDC network . Connections from Eduroam or outside do not work unless you access via the MDC jailnode (only for users with active MDC account). Solution : Connect from within Charite or MDC. I can connect, but it seems that my account has no access yet \u00b6 You're logging into BIH HPC cluster! (med-login1) ***Your account has not been granted cluster access yet.*** If you think that you should have access, please contact hpc-helpdesk@bihealth.de for assistance. For applying for cluster access, contact hpc-gatekeeper@bihealth.de. user@med-login1's password: Hint This is the most common error , and the main cause for this is a wrong username. Please take a couple of minutes to read the article about how usernames are constructed ! If you encounter this message although we told you that you have access and you checked the username as mentioned above , please write to hpc-gatekeeper@bihealth.de , always indicating the message you get and a detailed description of what you did. I'm getting a passPHRASE prompt \u00b6 You're logging into BIH HPC cluster! (med-login1) *** It looks like your account has access. *** Login is based on **SSH keys only**, if you are getting a password prompt then please contact hpc-helpdesk@bihealth.de for assistance. Enter passphrase for key '/home/USER/.ssh/id_rsa': Here you have to enter the passphrase that was used for encrypting your private key . Read SSH Basics for further information of what is going on here. I can connect, but I get a passWORD prompt \u00b6 You're logging into BIH HPC cluster! (med-login1) *** It looks like your account has access. *** Login is based on **SSH keys only**, if you are getting a password prompt then please contact hpc-helpdesk@bihealth.de for assistance. user@med-login1's password: This is diffeerent from passPHRASE prompt Please see I'm getting a passPHRASE prompt for more information. When you encounter this message during a login attempt, there is an issue with your SSH key. In this case, please connect with increased verbosity to the cluster ( ssh -vvv ... ) and mail the output and a detailed description to hpc-helpdesk@bihealth.de . If you are asked to write to helpdesk If you have an existent conversation with hpc-gatekeeper@bihealth.de and you are asked to open a ticket with hpc-helpdesk@bihealth.de : Please don't continue your conversation with hpc-helpdesk@bihealth.de , using the same ticket (i.e. same subject). The ticket system will automatically assign your reply to the existing ticket in hpc-gatekeeper. To avoid this, simply start with a fresh email.","title":"Connection Problems"},{"location":"connecting/configure-ssh/connection-problems/#debugging-connection-problems","text":"When you encounter problems with the login to the cluster although we indicated that you should have access, depending on the issue, here is a list of how to solve the problem:","title":"Debugging Connection Problems"},{"location":"connecting/configure-ssh/connection-problems/#im-unable-to-connect-to-the-cluster","text":"When you can't reach the cluster, please make sure that you are in the right network. Connections are only possible from within Charit\u00e9 or MDC network . Connections from Eduroam or outside do not work unless you access via the MDC jailnode (only for users with active MDC account). Solution : Connect from within Charite or MDC.","title":"I'm unable to connect to the cluster"},{"location":"connecting/configure-ssh/connection-problems/#i-can-connect-but-it-seems-that-my-account-has-no-access-yet","text":"You're logging into BIH HPC cluster! (med-login1) ***Your account has not been granted cluster access yet.*** If you think that you should have access, please contact hpc-helpdesk@bihealth.de for assistance. For applying for cluster access, contact hpc-gatekeeper@bihealth.de. user@med-login1's password: Hint This is the most common error , and the main cause for this is a wrong username. Please take a couple of minutes to read the article about how usernames are constructed ! If you encounter this message although we told you that you have access and you checked the username as mentioned above , please write to hpc-gatekeeper@bihealth.de , always indicating the message you get and a detailed description of what you did.","title":"I can connect, but it seems that my account has no access yet"},{"location":"connecting/configure-ssh/connection-problems/#im-getting-a-passphrase-prompt","text":"You're logging into BIH HPC cluster! (med-login1) *** It looks like your account has access. *** Login is based on **SSH keys only**, if you are getting a password prompt then please contact hpc-helpdesk@bihealth.de for assistance. Enter passphrase for key '/home/USER/.ssh/id_rsa': Here you have to enter the passphrase that was used for encrypting your private key . Read SSH Basics for further information of what is going on here.","title":"I'm getting a passPHRASE prompt"},{"location":"connecting/configure-ssh/connection-problems/#i-can-connect-but-i-get-a-password-prompt","text":"You're logging into BIH HPC cluster! (med-login1) *** It looks like your account has access. *** Login is based on **SSH keys only**, if you are getting a password prompt then please contact hpc-helpdesk@bihealth.de for assistance. user@med-login1's password: This is diffeerent from passPHRASE prompt Please see I'm getting a passPHRASE prompt for more information. When you encounter this message during a login attempt, there is an issue with your SSH key. In this case, please connect with increased verbosity to the cluster ( ssh -vvv ... ) and mail the output and a detailed description to hpc-helpdesk@bihealth.de . If you are asked to write to helpdesk If you have an existent conversation with hpc-gatekeeper@bihealth.de and you are asked to open a ticket with hpc-helpdesk@bihealth.de : Please don't continue your conversation with hpc-helpdesk@bihealth.de , using the same ticket (i.e. same subject). The ticket system will automatically assign your reply to the existing ticket in hpc-gatekeeper. To avoid this, simply start with a fresh email.","title":"I can connect, but I get a passWORD prompt"},{"location":"connecting/configure-ssh/linux/","text":"Connecting via SSH on Unix \u00b6 Hint Please read the pre-requisites . Especially pay attention to the username . Activating your Key in the SSH Key Agent \u00b6 Activate the key (if this is your first SSH key then it will be enabled by default) by making sure ssh-agent runs in the background # eval \"$(ssh-agent -s)\" and adding the key # ssh-add or if you created another key, specify the file name, e.g. ~/.ssh/mdc_id_rsa # ssh-add ~/.ssh/mdc_id_rsa MacOS \u00b6 If you run into problems that your key is not accepted when connecting from MacOS, please use: # ssh-add -K Connect to the cluster \u00b6 If you are within CUBI , Charite or the MDC , then one of the following commands should work now ( <USERNAME> is the cluster username ). This will connect you to the login node. # ssh -A -t -l <USERNAME> med-login<X>.bihealth.org <X> can be either 1 or 2 Use med-login<X> instead of med-transfer<X>.bihealth.org for file transfers! Warning Do not perform any computation on the login nodes Connecting from outside of MDC network (for MDC Users only) \u00b6 Danger If you are outside of MDC or CUBI then use the following two commands to first connect from your client to the SSH gateway ( ssh1 aka jail1 ) and then connect to the login node. Charite users have no possibility to connect from outside. Note that for logging into the jail, the <MDC_USER> is required. Make sure to add your key, otherwise the forwarding does not work: $ ssh-add Connect to the hop-node: $ ssh -A -t -l <MDC_USER> ssh1.mdc-berlin.de ... jail1 $ ssh -A -t -l <USERNAME> med-login<X>.bihealth.org <X> can be either 1 or 2 On the cluster, the following brings you to a cluster node where you can compute as much as the node can chew. med-login1:~$ srun --pty bash -i med0124:~$ Connecting with another computer/laptop \u00b6 If you need to connect to the cluster from another computer than the one that contains the SSH keys that you submitted for the cluster login, you have two possibilities. Generate another SSH key pair and submit the public part as described beforehand. Copy your private part of the SSH key ( ~/.ssh/id_rsa ) to the second computer into the same location. Danger Do not leave the key on any USB stick. Delete it after file transfer. This is a sensible part of data. Make sure that the files are only readable for you. $ cd ~/.ssh $ chmod g-rwx id_rsa* $ ssh-add id_rsa File System mount via sshfs \u00b6 $ sshfs -o follow_symlinks -l <USERNAME>@med-transfer<X>.bihealth.org:/ <MOUNTPOINT> <X> can be either 1 or 2 med-transfer<X>:/ follows the structure <host>:<directory> ; in our case it refers to the cluster root folder but can be any path available to you on the cluster and gives the folder that will be mounted. <MOUNTPOINT> must be an empty but existing and readable directory on your local computer On MacOS, make sure you have both OSXFUSE and SSHFS installed. You can get both from here: https://osxfuse.github.io/ or the most recent version via Homebrew: $ brew cask install osxfuse; brew install sshfs; brew link --overwrite sshfs The last command is optional and unlinks any pre-existing links to older versions of sshfs. Now you can run $ sshfs -o follow_symlinks <USERNAME>@med-transfer<X>.bihealth.org:<directory_relative_to_Cluster_root> <MOUNTPOINT> -o volname=<BIH-FOLDER> -o allow_other,noapplexattr,noappledouble Configure SSH Client \u00b6 Add the following lines to your ~/.ssh/config file for more comfortable access to the cluster. Replace MDC_USER_NAME with your MDC user name. Host bihcluster ForwardAgent yes ForwardX11 yes HostName med-login1.bihealth.org User MDC_USER_NAME RequestTTY yes Host bihcluster2 ForwardAgent yes ForwardX11 yes HostName med-login2.bihealth.org User MDC_USER_NAME RequestTTY yes Host bihcluster3 ForwardAgent yes ForwardX11 yes HostName med-login3.bihealth.org User MDC_USER_NAME RequestTTY yes Now, you can do type the following (and you don't have to remember the IP of the login node any more). $ ssh bihcluster You can also chain the commands to directly execute srun after your SSH connection. $ ssh bihcluster srun --pty bash ... med0123 $ The configuration works for you inside CUBI or MDC. If you are located anywhere else, you can use the following ~/.ssh/config lines. Host mdcjail ForwardAgent yes ForwardX11 yes HostName ssh1.mdc-berlin.de User MDC_USER_NAME RequestTTY yes Now, do # ssh mdcjail ssh -A -t -l MDC_USER med-login<X> <X> can be either 1 or 2 X11 \u00b6 Do you really need to run a graphical application on the cluster? Please note that running more complex Java applications, such as IGV may be not very efficient because of the connection speed. In most cases you can run them on your local workstation by mounting them via SSHFS . Connect to one of the login nodes using X11 forwarding: # ssh -X -C -A -t -l <USERNAME> med-login<X>.bihealth.org <X> can be either 1 or 2 Once you get a login prompt, you can use the srun command with the --x11 parameter to open a X11 session to a cluster node: # srun --pty --x11 bash And finally you can start your X11 application, e.g.: # xterm After a while Visual Terminal should start:","title":"Linux"},{"location":"connecting/configure-ssh/linux/#connecting-via-ssh-on-unix","text":"Hint Please read the pre-requisites . Especially pay attention to the username .","title":"Connecting via SSH on Unix"},{"location":"connecting/configure-ssh/linux/#activating-your-key-in-the-ssh-key-agent","text":"Activate the key (if this is your first SSH key then it will be enabled by default) by making sure ssh-agent runs in the background # eval \"$(ssh-agent -s)\" and adding the key # ssh-add or if you created another key, specify the file name, e.g. ~/.ssh/mdc_id_rsa # ssh-add ~/.ssh/mdc_id_rsa","title":"Activating your Key in the SSH Key Agent"},{"location":"connecting/configure-ssh/linux/#macos","text":"If you run into problems that your key is not accepted when connecting from MacOS, please use: # ssh-add -K","title":"MacOS"},{"location":"connecting/configure-ssh/linux/#connect-to-the-cluster","text":"If you are within CUBI , Charite or the MDC , then one of the following commands should work now ( <USERNAME> is the cluster username ). This will connect you to the login node. # ssh -A -t -l <USERNAME> med-login<X>.bihealth.org <X> can be either 1 or 2 Use med-login<X> instead of med-transfer<X>.bihealth.org for file transfers! Warning Do not perform any computation on the login nodes","title":"Connect to the cluster"},{"location":"connecting/configure-ssh/linux/#connecting-from-outside-of-mdc-network-for-mdc-users-only","text":"Danger If you are outside of MDC or CUBI then use the following two commands to first connect from your client to the SSH gateway ( ssh1 aka jail1 ) and then connect to the login node. Charite users have no possibility to connect from outside. Note that for logging into the jail, the <MDC_USER> is required. Make sure to add your key, otherwise the forwarding does not work: $ ssh-add Connect to the hop-node: $ ssh -A -t -l <MDC_USER> ssh1.mdc-berlin.de ... jail1 $ ssh -A -t -l <USERNAME> med-login<X>.bihealth.org <X> can be either 1 or 2 On the cluster, the following brings you to a cluster node where you can compute as much as the node can chew. med-login1:~$ srun --pty bash -i med0124:~$","title":"Connecting from outside of MDC network (for MDC Users only)"},{"location":"connecting/configure-ssh/linux/#connecting-with-another-computerlaptop","text":"If you need to connect to the cluster from another computer than the one that contains the SSH keys that you submitted for the cluster login, you have two possibilities. Generate another SSH key pair and submit the public part as described beforehand. Copy your private part of the SSH key ( ~/.ssh/id_rsa ) to the second computer into the same location. Danger Do not leave the key on any USB stick. Delete it after file transfer. This is a sensible part of data. Make sure that the files are only readable for you. $ cd ~/.ssh $ chmod g-rwx id_rsa* $ ssh-add id_rsa","title":"Connecting with another computer/laptop"},{"location":"connecting/configure-ssh/linux/#file-system-mount-via-sshfs","text":"$ sshfs -o follow_symlinks -l <USERNAME>@med-transfer<X>.bihealth.org:/ <MOUNTPOINT> <X> can be either 1 or 2 med-transfer<X>:/ follows the structure <host>:<directory> ; in our case it refers to the cluster root folder but can be any path available to you on the cluster and gives the folder that will be mounted. <MOUNTPOINT> must be an empty but existing and readable directory on your local computer On MacOS, make sure you have both OSXFUSE and SSHFS installed. You can get both from here: https://osxfuse.github.io/ or the most recent version via Homebrew: $ brew cask install osxfuse; brew install sshfs; brew link --overwrite sshfs The last command is optional and unlinks any pre-existing links to older versions of sshfs. Now you can run $ sshfs -o follow_symlinks <USERNAME>@med-transfer<X>.bihealth.org:<directory_relative_to_Cluster_root> <MOUNTPOINT> -o volname=<BIH-FOLDER> -o allow_other,noapplexattr,noappledouble","title":"File System mount via sshfs"},{"location":"connecting/configure-ssh/linux/#configure-ssh-client","text":"Add the following lines to your ~/.ssh/config file for more comfortable access to the cluster. Replace MDC_USER_NAME with your MDC user name. Host bihcluster ForwardAgent yes ForwardX11 yes HostName med-login1.bihealth.org User MDC_USER_NAME RequestTTY yes Host bihcluster2 ForwardAgent yes ForwardX11 yes HostName med-login2.bihealth.org User MDC_USER_NAME RequestTTY yes Host bihcluster3 ForwardAgent yes ForwardX11 yes HostName med-login3.bihealth.org User MDC_USER_NAME RequestTTY yes Now, you can do type the following (and you don't have to remember the IP of the login node any more). $ ssh bihcluster You can also chain the commands to directly execute srun after your SSH connection. $ ssh bihcluster srun --pty bash ... med0123 $ The configuration works for you inside CUBI or MDC. If you are located anywhere else, you can use the following ~/.ssh/config lines. Host mdcjail ForwardAgent yes ForwardX11 yes HostName ssh1.mdc-berlin.de User MDC_USER_NAME RequestTTY yes Now, do # ssh mdcjail ssh -A -t -l MDC_USER med-login<X> <X> can be either 1 or 2","title":"Configure SSH Client"},{"location":"connecting/configure-ssh/linux/#x11","text":"Do you really need to run a graphical application on the cluster? Please note that running more complex Java applications, such as IGV may be not very efficient because of the connection speed. In most cases you can run them on your local workstation by mounting them via SSHFS . Connect to one of the login nodes using X11 forwarding: # ssh -X -C -A -t -l <USERNAME> med-login<X>.bihealth.org <X> can be either 1 or 2 Once you get a login prompt, you can use the srun command with the --x11 parameter to open a X11 session to a cluster node: # srun --pty --x11 bash And finally you can start your X11 application, e.g.: # xterm After a while Visual Terminal should start:","title":"X11"},{"location":"connecting/configure-ssh/prerequisites/","text":"You must have a valid Charite or MDC username. You have applied for and been granted access to the cluster by the gatekeeper . You have an SSH key generated and submitted . What is my username? \u00b6 The username for accessing the cluster is composed of your username at your primary organization (Charite/MDC) and a suffix: Charite user: <Charite username>_c , e.g. doej_c MDC user: <MDC username>_m , e.g. jdoe_m Hint There are two login nodes, med-login1 and med-login2 . There are two for redundancy reasons. Please do not perform big file transfers or an sshfs mount via the login nodes. For this purpose, we have med-transfer1 and med-transfer2 . Note Please Note that the cluster login is independent of access to the MDC jail node ssh1.mdc-berlin.de. Access to the cluster is granted by BIH HPC IT through hpc-gatekeeper@bihealth.de Access to the MDC jail node is managed by MDC IT","title":"Pre-Requisites"},{"location":"connecting/configure-ssh/prerequisites/#what-is-my-username","text":"The username for accessing the cluster is composed of your username at your primary organization (Charite/MDC) and a suffix: Charite user: <Charite username>_c , e.g. doej_c MDC user: <MDC username>_m , e.g. jdoe_m Hint There are two login nodes, med-login1 and med-login2 . There are two for redundancy reasons. Please do not perform big file transfers or an sshfs mount via the login nodes. For this purpose, we have med-transfer1 and med-transfer2 . Note Please Note that the cluster login is independent of access to the MDC jail node ssh1.mdc-berlin.de. Access to the cluster is granted by BIH HPC IT through hpc-gatekeeper@bihealth.de Access to the MDC jail node is managed by MDC IT","title":"What is my username?"},{"location":"connecting/configure-ssh/windows/","text":"Connecting via SSH on Windows \u00b6 Hint Please read the pre-requisites . Especially pay attention to the username . Install an SSH client for Windows . Connecting from within MDC/Charite Network \u00b6 Click on Session . Click on SSH . In Basic SSH settings , enter a hostname ( med-loginX.bihealth.org , where X is 1 or 2), check Specify username and enter your username in the textfield. Select the tab Advanced SSH settings , check Use private key and select your private SSH key file (possible choices described with the next to figures). Select the id_rsa file generated in Linux OR select the id_rsa.ppk file generated in Windows with MobaXterm. Afterwards hit the OK button and MobaXterm will connect. The session will be stored automatically and you can establish new connections later on, or also multiple ones at the same time, if you like. Connecting via MDC Jail Node \u00b6 This requires an active MDC account! Additional to the steps above, click on the tab Network settings . Check Connect through SSH gateway (jump host) and in the text field Gateway SSH server enter ssh1.mdc-berlin.de and in the field User your MDC username. Check Use private key and select the SSH key that you uploaded to the MDC persdb (this might differ from your cluster key!). Click OK X11 \u00b6 Do you really need to run a graphical application on the cluster? Please note that running more complex Java applications, such as IGV may be not very efficient because of the connection speed. In most cases you can run them on your local workstation by mounting them via SSHFS . Start MobaXterm, it should automatically fetch your saved Putty sessions as you can see on screen below: Connect to one of the login nodes, by double-click on saved profile, and then use srun --pty --x11 bash command to start X11 session to one of the nodes: Finally, start X11 application (below example of starting Visual Terminal):","title":"Windows"},{"location":"connecting/configure-ssh/windows/#connecting-via-ssh-on-windows","text":"Hint Please read the pre-requisites . Especially pay attention to the username . Install an SSH client for Windows .","title":"Connecting via SSH on Windows"},{"location":"connecting/configure-ssh/windows/#connecting-from-within-mdccharite-network","text":"Click on Session . Click on SSH . In Basic SSH settings , enter a hostname ( med-loginX.bihealth.org , where X is 1 or 2), check Specify username and enter your username in the textfield. Select the tab Advanced SSH settings , check Use private key and select your private SSH key file (possible choices described with the next to figures). Select the id_rsa file generated in Linux OR select the id_rsa.ppk file generated in Windows with MobaXterm. Afterwards hit the OK button and MobaXterm will connect. The session will be stored automatically and you can establish new connections later on, or also multiple ones at the same time, if you like.","title":"Connecting from within MDC/Charite Network"},{"location":"connecting/configure-ssh/windows/#connecting-via-mdc-jail-node","text":"This requires an active MDC account! Additional to the steps above, click on the tab Network settings . Check Connect through SSH gateway (jump host) and in the text field Gateway SSH server enter ssh1.mdc-berlin.de and in the field User your MDC username. Check Use private key and select the SSH key that you uploaded to the MDC persdb (this might differ from your cluster key!). Click OK","title":"Connecting via MDC Jail Node"},{"location":"connecting/configure-ssh/windows/#x11","text":"Do you really need to run a graphical application on the cluster? Please note that running more complex Java applications, such as IGV may be not very efficient because of the connection speed. In most cases you can run them on your local workstation by mounting them via SSHFS . Start MobaXterm, it should automatically fetch your saved Putty sessions as you can see on screen below: Connect to one of the login nodes, by double-click on saved profile, and then use srun --pty --x11 bash command to start X11 session to one of the nodes: Finally, start X11 application (below example of starting Visual Terminal):","title":"X11"},{"location":"connecting/generate-key/linux/","text":"Generating an SSH Key in Linux \u00b6 You might already have one, check whether the file ~/.ssh/id_rsa.pub is present. Otherwise, create key using the following command (marking your key with your email address will make it easier to reidentify your key later on): host:~$ ssh-keygen -t rsa -C \"your_email@example.com\" Use the default location for your key Enter the passphrase twice to encrypt your key What is your key's passphrase? You should set a passphrase when generating your private key. This passphrase is used for encrypting you private key to protect it against the private key file theft/being lost. When using the key for login, you will have to enter it (or the first time you load it into the SSH key agent). Note that when being asked for the passphrase this does not occur on the cluster (and is thus unrelated to it) but on your local computer. Also see SSH Basics for more information. The whole session should look something like this: host:~$ ssh-keygen -t rsa -C \"your_email@example.com\" Generating public/private rsa key pair. Enter file in which to save the key ( /home/USER/.ssh/id_rsa ) : Created directory '/home/USER/.ssh' . Enter passphrase ( empty for no passphrase ) : Enter same passphrase again: Your identification has been saved in /home/USER/.ssh/id_rsa. Your public key has been saved in /home/USER/.ssh/id_rsa.pub. The key fingerprint is: 55 :dd:8f:88:84:1b:b6:f0:19:d3:fb:19:8e:7a:9e:7d your_email@example.com The key ' s randomart image is: +-- [ RSA 2048 ] ----+ | o .. . | | . * o. . . | | + O.o . .. | | = .o o . . | | S + o | | . + | | . | | . .o E | | oo .. | The file content of ~/.ssh/id_rsa.pub should look something like this): ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC/Rdd5rvf4BT38jsBlRrXpd1KDvjE1iZZlEmkB6809QK7hV6RCG13VcyPTIHSQePycfcUv5q1Jdy28MpacL/nv1UR/o35xPBn2HkgB4OqnKtt86soCGMd9/YzQP5lY7V60kPBJbrXDApeqf+H1GALsFNQM6MCwicdE6zTqE1mzWVdhGymZR28hGJbV9H4snMDDc0tW4i3FHGrDdmb7wHM9THMx6OcCrnNyA9Sh2OyBH4MwItKfuqEg2rc56D7WAQ2JcmPQZTlBAYeFL/dYYKcXmbffEpXTbYh+7O0o9RAJ7T3uOUj/2IbSnsgg6fyw0Kotcg8iHAPvb61bZGPOEWZb your_email@example.com Submit Your Key \u00b6 As a next step you need to submit the SSH key use these links as: Charite user MDC user","title":"Linux"},{"location":"connecting/generate-key/linux/#generating-an-ssh-key-in-linux","text":"You might already have one, check whether the file ~/.ssh/id_rsa.pub is present. Otherwise, create key using the following command (marking your key with your email address will make it easier to reidentify your key later on): host:~$ ssh-keygen -t rsa -C \"your_email@example.com\" Use the default location for your key Enter the passphrase twice to encrypt your key What is your key's passphrase? You should set a passphrase when generating your private key. This passphrase is used for encrypting you private key to protect it against the private key file theft/being lost. When using the key for login, you will have to enter it (or the first time you load it into the SSH key agent). Note that when being asked for the passphrase this does not occur on the cluster (and is thus unrelated to it) but on your local computer. Also see SSH Basics for more information. The whole session should look something like this: host:~$ ssh-keygen -t rsa -C \"your_email@example.com\" Generating public/private rsa key pair. Enter file in which to save the key ( /home/USER/.ssh/id_rsa ) : Created directory '/home/USER/.ssh' . Enter passphrase ( empty for no passphrase ) : Enter same passphrase again: Your identification has been saved in /home/USER/.ssh/id_rsa. Your public key has been saved in /home/USER/.ssh/id_rsa.pub. The key fingerprint is: 55 :dd:8f:88:84:1b:b6:f0:19:d3:fb:19:8e:7a:9e:7d your_email@example.com The key ' s randomart image is: +-- [ RSA 2048 ] ----+ | o .. . | | . * o. . . | | + O.o . .. | | = .o o . . | | S + o | | . + | | . | | . .o E | | oo .. | The file content of ~/.ssh/id_rsa.pub should look something like this): ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC/Rdd5rvf4BT38jsBlRrXpd1KDvjE1iZZlEmkB6809QK7hV6RCG13VcyPTIHSQePycfcUv5q1Jdy28MpacL/nv1UR/o35xPBn2HkgB4OqnKtt86soCGMd9/YzQP5lY7V60kPBJbrXDApeqf+H1GALsFNQM6MCwicdE6zTqE1mzWVdhGymZR28hGJbV9H4snMDDc0tW4i3FHGrDdmb7wHM9THMx6OcCrnNyA9Sh2OyBH4MwItKfuqEg2rc56D7WAQ2JcmPQZTlBAYeFL/dYYKcXmbffEpXTbYh+7O0o9RAJ7T3uOUj/2IbSnsgg6fyw0Kotcg8iHAPvb61bZGPOEWZb your_email@example.com","title":"Generating an SSH Key in Linux"},{"location":"connecting/generate-key/linux/#submit-your-key","text":"As a next step you need to submit the SSH key use these links as: Charite user MDC user","title":"Submit Your Key"},{"location":"connecting/generate-key/windows/","text":"Generating an SSH Key in Windows \u00b6 Hint Please install an SSH client for Windows . Generate the Key \u00b6 Click on Tools and MobaKeyGen (SSH key generator) In the section Parameters make sure to set the following properties: Type of key to generate: RSA (this is the SSH-2 protocol) Number of bits in a generated key: 4096 If all is set, hit the Generate button. During generation, move the mouse cursor around in the blank area. When finished, make sure to protect your generated key with a passphrase. Save the private and public key. The default name under Linux for the public key is id_rsa.pub and id_rsa for the private key, but you can name them however you want (the .pub is NOT automatically added). Note that in the whole cluster wiki we will use this file naming convention. Also note that the private key will be stored in Putty format ( .ppk , this extension is added automatically). What is your key's passphrase? You should set a passphrase when generating your private key. This passphrase is used for encrypting you private key to protect it against the private key file theft/being lost. When using the key for login, you will have to enter it (or the first time you load it into the SSH key agent). Note that when being asked for the passphrase this does not occur on the cluster (and is thus unrelated to it) but on your local computer. Also see SSH Basics for more information. The gibberish in the textbox is your public key in the format how it has to be submitted to the MDC and Charite (links for this step below). Thus, copy this text and paste it to the SSH-key-submission-web-service of your institution. Store the private key additionally in the OpenSSH format. To do so, click Conversions and select Export OpenSSH key . To be consistent, give the file the same name as your .ppk private key file above (just without the .ppk ). Summary \u00b6 To summarize, you should end up with three files: id_rsa.pub The public key file, it is not required if you copy and submit the SSH public key as described above and in the links below. id_rsa.ppk This file is only needed if you plan to use Putty. id_rsa This is your private key and the one and only most important file to access the cluster. It will be added to the sessions in MobaXterm and WinSSHFS (if required). Submit Your Key \u00b6 As a next step you need to submit the SSH key use these links as: Charite user MDC user","title":"Windows"},{"location":"connecting/generate-key/windows/#generating-an-ssh-key-in-windows","text":"Hint Please install an SSH client for Windows .","title":"Generating an SSH Key in Windows"},{"location":"connecting/generate-key/windows/#generate-the-key","text":"Click on Tools and MobaKeyGen (SSH key generator) In the section Parameters make sure to set the following properties: Type of key to generate: RSA (this is the SSH-2 protocol) Number of bits in a generated key: 4096 If all is set, hit the Generate button. During generation, move the mouse cursor around in the blank area. When finished, make sure to protect your generated key with a passphrase. Save the private and public key. The default name under Linux for the public key is id_rsa.pub and id_rsa for the private key, but you can name them however you want (the .pub is NOT automatically added). Note that in the whole cluster wiki we will use this file naming convention. Also note that the private key will be stored in Putty format ( .ppk , this extension is added automatically). What is your key's passphrase? You should set a passphrase when generating your private key. This passphrase is used for encrypting you private key to protect it against the private key file theft/being lost. When using the key for login, you will have to enter it (or the first time you load it into the SSH key agent). Note that when being asked for the passphrase this does not occur on the cluster (and is thus unrelated to it) but on your local computer. Also see SSH Basics for more information. The gibberish in the textbox is your public key in the format how it has to be submitted to the MDC and Charite (links for this step below). Thus, copy this text and paste it to the SSH-key-submission-web-service of your institution. Store the private key additionally in the OpenSSH format. To do so, click Conversions and select Export OpenSSH key . To be consistent, give the file the same name as your .ppk private key file above (just without the .ppk ).","title":"Generate the Key"},{"location":"connecting/generate-key/windows/#summary","text":"To summarize, you should end up with three files: id_rsa.pub The public key file, it is not required if you copy and submit the SSH public key as described above and in the links below. id_rsa.ppk This file is only needed if you plan to use Putty. id_rsa This is your private key and the one and only most important file to access the cluster. It will be added to the sessions in MobaXterm and WinSSHFS (if required).","title":"Summary"},{"location":"connecting/generate-key/windows/#submit-your-key","text":"As a next step you need to submit the SSH key use these links as: Charite user MDC user","title":"Submit Your Key"},{"location":"connecting/submit-key/charite/","text":"Submitting an SSH Key to Charite \u00b6 As of February 2020, SSH key submission not accepted via email anymore. Instead, use the process outline here. For any help, please contact helpdesk@charite.de (as this site is maintained by Charite GB IT). Charite Zugangsportal \u00b6 Key are submitted in the Charite Zugangsportal. As of Feb 4, you have to use the \"test\" version for this. Go to zugang.charite.de and login. Follow through the login page until you reach the main menu (it's tedious but we belive in you ;) Click the \"SSH Keys\" button. Paste your SSH key (starting with ssh-rsa ) and ending with the label (usually your email, e.g., john.doe@charite.de ) into the box (1) and press append (2). By default, the key can be found in the file ~/.ssh/id_rsa.pub in Linux. If you generated the key in Windows, please paste the copied key from the text box. Repeat as necessary. Optionally, go back to the main menu (3) when done. If you have generated your SSH key with PuTTy, you must right click on the ppk-file, then choose \"Edit with PuTTYgen\" in the right click menu. Enter your passphrase. Then copy the SSH key out of the upper box (already highlighted in blue). Check if the key has been added After you clicked append , your key will be printed back to you (as shown in the blurred picture above). If your key is not printed back to you then adding the SSH key to zugang.charite.de was not successful. In this case please contact helpdesk@charite.de for assistance as they (Charite GB IT) maintains that system and it is out of our (BIH HPC IT) control. Once your key has been added, it will take a few minutes for the changes to go live.","title":"Charite"},{"location":"connecting/submit-key/charite/#submitting-an-ssh-key-to-charite","text":"As of February 2020, SSH key submission not accepted via email anymore. Instead, use the process outline here. For any help, please contact helpdesk@charite.de (as this site is maintained by Charite GB IT).","title":"Submitting an SSH Key to Charite"},{"location":"connecting/submit-key/charite/#charite-zugangsportal","text":"Key are submitted in the Charite Zugangsportal. As of Feb 4, you have to use the \"test\" version for this. Go to zugang.charite.de and login. Follow through the login page until you reach the main menu (it's tedious but we belive in you ;) Click the \"SSH Keys\" button. Paste your SSH key (starting with ssh-rsa ) and ending with the label (usually your email, e.g., john.doe@charite.de ) into the box (1) and press append (2). By default, the key can be found in the file ~/.ssh/id_rsa.pub in Linux. If you generated the key in Windows, please paste the copied key from the text box. Repeat as necessary. Optionally, go back to the main menu (3) when done. If you have generated your SSH key with PuTTy, you must right click on the ppk-file, then choose \"Edit with PuTTYgen\" in the right click menu. Enter your passphrase. Then copy the SSH key out of the upper box (already highlighted in blue). Check if the key has been added After you clicked append , your key will be printed back to you (as shown in the blurred picture above). If your key is not printed back to you then adding the SSH key to zugang.charite.de was not successful. In this case please contact helpdesk@charite.de for assistance as they (Charite GB IT) maintains that system and it is out of our (BIH HPC IT) control. Once your key has been added, it will take a few minutes for the changes to go live.","title":"Charite Zugangsportal"},{"location":"connecting/submit-key/mdc/","text":"Submitting an SSH Key to MDC \u00b6 For MDC users, SSH keys are submitted through the MDC PersDB interface (see below). PersDB is not maintained by BIH HPC IT but by MDC IT. Warning The SSH keys are only activated over night (but automatically). This is out of our control. Contact helpdesk@mdc-berlin.de for more information. Detour: Using MDC VMWare View to get into MDC Intranet \u00b6 In case you are not within the MDC network, connect to MDC VMWare view first and use the web brower in the Window session. Go to the MDC VMWare View Click \"VMWare Web Viewer\" Login with MDC username and password. Select Windows 7. Open Firefox or Internet Browser Enter MDC PersDB \u00b6 If you are inside MDC network, you can start here, OR If you have the MDC VMWare Web Viewer open, start here. Log into MDC PersDB \u00b6 Open https://persdb.mdc-berlin.net/login Login with MDC username and password again Click on \"Mein Profil\" \u00b6 Click on \"Zusaetzliches (ssh public key) -> Bearbeiten\" \u00b6 This is the middle item. Click \"Neue zusaetzliche Eigenschaft anlegen\" \u00b6 Most probably, you don't have any entries yet. Activate the VMWare View Menu \u00b6 This is the only way to get your SSH key into the clipboard of the Windows instance that has MDC PersDB open. :rolleyes: Activate Clipboard Window \u00b6 Click (middle) clipboard button. The clipboard window appears. Close the VMWare View window again. Register SSH key \u00b6 Paste SSH key from ~/.ssh/id_rsa.pub into the clipboard window. Ensure that the whole file contents is there (should end with your email address). If you generated the key in Windows, please paste the copied key from the text box. Left-click the \"Inhalt\" text box to put the cursor there Right-click the \"Inhalt\" text box, make the context menu appear, and click \"Einfuegen\" Click Submit You're Done \u00b6 Thus, you will only be able to connect the next day. - Bask in the glory of having completed this process.","title":"MDC"},{"location":"connecting/submit-key/mdc/#submitting-an-ssh-key-to-mdc","text":"For MDC users, SSH keys are submitted through the MDC PersDB interface (see below). PersDB is not maintained by BIH HPC IT but by MDC IT. Warning The SSH keys are only activated over night (but automatically). This is out of our control. Contact helpdesk@mdc-berlin.de for more information.","title":"Submitting an SSH Key to MDC"},{"location":"connecting/submit-key/mdc/#detour-using-mdc-vmware-view-to-get-into-mdc-intranet","text":"In case you are not within the MDC network, connect to MDC VMWare view first and use the web brower in the Window session. Go to the MDC VMWare View Click \"VMWare Web Viewer\" Login with MDC username and password. Select Windows 7. Open Firefox or Internet Browser","title":"Detour: Using MDC VMWare View to get into MDC Intranet"},{"location":"connecting/submit-key/mdc/#enter-mdc-persdb","text":"If you are inside MDC network, you can start here, OR If you have the MDC VMWare Web Viewer open, start here.","title":"Enter MDC PersDB"},{"location":"connecting/submit-key/mdc/#log-into-mdc-persdb","text":"Open https://persdb.mdc-berlin.net/login Login with MDC username and password again","title":"Log into MDC PersDB"},{"location":"connecting/submit-key/mdc/#click-on-mein-profil","text":"","title":"Click on \"Mein Profil\""},{"location":"connecting/submit-key/mdc/#click-on-zusaetzliches-ssh-public-key-bearbeiten","text":"This is the middle item.","title":"Click on \"Zusaetzliches (ssh public key) -&gt; Bearbeiten\""},{"location":"connecting/submit-key/mdc/#click-neue-zusaetzliche-eigenschaft-anlegen","text":"Most probably, you don't have any entries yet.","title":"Click \"Neue zusaetzliche Eigenschaft anlegen\""},{"location":"connecting/submit-key/mdc/#activate-the-vmware-view-menu","text":"This is the only way to get your SSH key into the clipboard of the Windows instance that has MDC PersDB open. :rolleyes:","title":"Activate the VMWare View Menu"},{"location":"connecting/submit-key/mdc/#activate-clipboard-window","text":"Click (middle) clipboard button. The clipboard window appears. Close the VMWare View window again.","title":"Activate Clipboard Window"},{"location":"connecting/submit-key/mdc/#register-ssh-key","text":"Paste SSH key from ~/.ssh/id_rsa.pub into the clipboard window. Ensure that the whole file contents is there (should end with your email address). If you generated the key in Windows, please paste the copied key from the text box. Left-click the \"Inhalt\" text box to put the cursor there Right-click the \"Inhalt\" text box, make the context menu appear, and click \"Einfuegen\" Click Submit","title":"Register SSH key"},{"location":"connecting/submit-key/mdc/#youre-done","text":"Thus, you will only be able to connect the next day. - Bask in the glory of having completed this process.","title":"You're Done"},{"location":"cubit/","text":"Overview \u00b6 The static data installation can be found at /fast/projects/cubit/18.12/static_data . The static data directory contains a sub-directory for the genomes, the precomputed index files for several different popular mapping tools and associated annotation (GFF and GTF files) from Ensembl and GENCODE for each of the available genomes. The top-level directory structure is as follows: static_data/ annotations app_support db exome_panel exon_list precomputed reference","title":"Overview"},{"location":"cubit/#overview","text":"The static data installation can be found at /fast/projects/cubit/18.12/static_data . The static data directory contains a sub-directory for the genomes, the precomputed index files for several different popular mapping tools and associated annotation (GFF and GTF files) from Ensembl and GENCODE for each of the available genomes. The top-level directory structure is as follows: static_data/ annotations app_support db exome_panel exon_list precomputed reference","title":"Overview"},{"location":"cubit/annotations/","text":"Annotation Data \u00b6 The following Ensembl and GENCODE versions corresponding to the indicated reference genomes will be made available on the cluster. Database Version Reference Genome Ensembl 65 NCBIM37 (Ensembl release corresponding to GENCODE M1) Ensembl 67 NCBIM37 (Ensembl release for sanger mouse genome assembly) Ensembl 68 GRCm38 (Ensembl release for sanger mouse genome assembly) Ensembl 74 GRCh37 (Ensembl release for GENCODE 19) Ensembl 75 GRCh37 (Latest release for GRCh37) Ensembl 79 GRCh38 (Ensembl release for GENCODE 22) Ensembl 80 GRCh38 (Ensembl release corresponding to GENCODE 22) Ensembl 80 GRCm38 (Ensembl release corresponding to GENCODE M1) GENCODE M1 NCBIM37 (No gff3 file) GENCODE M5 GRCm38 GENCODE 19 current for GRCh37 GENCODE 22 current for GRCh38 The annotation files associated with the indicated genomes can be accessed in the following directories: static_data/annotation \u251c\u2500\u2500 ENSEMBL \u2502 \u251c\u2500\u2500 65 \u2502 \u2502 \u2514\u2500\u2500 NCBIM37 \u2502 \u251c\u2500\u2500 67 \u2502 \u2502 \u2514\u2500\u2500 NCBIM37 \u2502 \u251c\u2500\u2500 68 \u2502 \u2502 \u2514\u2500\u2500 GRCm38 \u2502 \u251c\u2500\u2500 74 \u2502 \u2502 \u2514\u2500\u2500 GRCh37 \u2502 \u251c\u2500\u2500 75 \u2502 \u2502 \u2514\u2500\u2500 GRCh37 \u2502 \u251c\u2500\u2500 79 \u2502 \u2502 \u2514\u2500\u2500 GRCh38 \u2502 \u2514\u2500\u2500 80 \u2502 \u251c\u2500\u2500 GRCh38 \u2502 \u2514\u2500\u2500 GRCm38 \u2514\u2500\u2500 GENCODE \u251c\u2500\u2500 19 \u2502 \u2514\u2500\u2500 GRCh37 \u251c\u2500\u2500 22 \u2502 \u2514\u2500\u2500 GRCh38 \u251c\u2500\u2500 M1 \u2502 \u2514\u2500\u2500 NCBIM37 \u2514\u2500\u2500 M5 \u2514\u2500\u2500 GRCm38","title":"Annotations"},{"location":"cubit/annotations/#annotation-data","text":"The following Ensembl and GENCODE versions corresponding to the indicated reference genomes will be made available on the cluster. Database Version Reference Genome Ensembl 65 NCBIM37 (Ensembl release corresponding to GENCODE M1) Ensembl 67 NCBIM37 (Ensembl release for sanger mouse genome assembly) Ensembl 68 GRCm38 (Ensembl release for sanger mouse genome assembly) Ensembl 74 GRCh37 (Ensembl release for GENCODE 19) Ensembl 75 GRCh37 (Latest release for GRCh37) Ensembl 79 GRCh38 (Ensembl release for GENCODE 22) Ensembl 80 GRCh38 (Ensembl release corresponding to GENCODE 22) Ensembl 80 GRCm38 (Ensembl release corresponding to GENCODE M1) GENCODE M1 NCBIM37 (No gff3 file) GENCODE M5 GRCm38 GENCODE 19 current for GRCh37 GENCODE 22 current for GRCh38 The annotation files associated with the indicated genomes can be accessed in the following directories: static_data/annotation \u251c\u2500\u2500 ENSEMBL \u2502 \u251c\u2500\u2500 65 \u2502 \u2502 \u2514\u2500\u2500 NCBIM37 \u2502 \u251c\u2500\u2500 67 \u2502 \u2502 \u2514\u2500\u2500 NCBIM37 \u2502 \u251c\u2500\u2500 68 \u2502 \u2502 \u2514\u2500\u2500 GRCm38 \u2502 \u251c\u2500\u2500 74 \u2502 \u2502 \u2514\u2500\u2500 GRCh37 \u2502 \u251c\u2500\u2500 75 \u2502 \u2502 \u2514\u2500\u2500 GRCh37 \u2502 \u251c\u2500\u2500 79 \u2502 \u2502 \u2514\u2500\u2500 GRCh38 \u2502 \u2514\u2500\u2500 80 \u2502 \u251c\u2500\u2500 GRCh38 \u2502 \u2514\u2500\u2500 GRCm38 \u2514\u2500\u2500 GENCODE \u251c\u2500\u2500 19 \u2502 \u2514\u2500\u2500 GRCh37 \u251c\u2500\u2500 22 \u2502 \u2514\u2500\u2500 GRCh38 \u251c\u2500\u2500 M1 \u2502 \u2514\u2500\u2500 NCBIM37 \u2514\u2500\u2500 M5 \u2514\u2500\u2500 GRCm38","title":"Annotation Data"},{"location":"cubit/app-support/","text":"Cubit Static Data: Application Support \u00b6 The static_data/app_support directory contains all data files that are shipped with a software package installed in cubit. For blast this is not complete and more databases can be added upon request. static_data/app_support \u251c\u2500\u2500 blast \u2502 \u2514\u2500\u2500 variable \u2502 \u251c\u2500\u2500 nt \u2502 \u2514\u2500\u2500 refseq_protein \u251c\u2500\u2500 Delly \u2502 \u251c\u2500\u2500 0.6.5 \u2502 \u251c\u2500\u2500 0.6.7 \u2502 \u251c\u2500\u2500 0.7.1 \u2502 \u251c\u2500\u2500 0.7.2 \u2502 \u251c\u2500\u2500 0.7.3 \u2502 \u2514\u2500\u2500 0.7.5 \u251c\u2500\u2500 GATK_bundle \u2502 \u2514\u2500\u2500 2.8 \u2502 \u251c\u2500\u2500 b37 \u2502 \u2514\u2500\u2500 hg19 \u251c\u2500\u2500 Jannovar \u2502 \u251c\u2500\u2500 0.14 \u2502 \u2514\u2500\u2500 0.16 \u251c\u2500\u2500 kraken \u2502 \u2514\u2500\u2500 0.10.5-cubi20160426 \u2502 \u251c\u2500\u2500 bacvir \u2502 \u2514\u2500\u2500 minikraken_20141208 \u251c\u2500\u2500 Oncotator \u2502 \u2514\u2500\u2500 v1_ds_Jan262015 \u2502 \u251c\u2500\u2500 1000genome_db \u2502 \u251c\u2500\u2500 achilles \u2502 \u251c\u2500\u2500 cancer_gene_census \u2502 \u251c\u2500\u2500 ccle_by_gene \u2502 \u251c\u2500\u2500 ccle_by_gp \u2502 \u251c\u2500\u2500 clinvar \u2502 \u251c\u2500\u2500 cosmic \u2502 \u251c\u2500\u2500 cosmic_fusion \u2502 \u251c\u2500\u2500 cosmic_tissue \u2502 \u251c\u2500\u2500 dbNSFP_ds \u2502 \u251c\u2500\u2500 dbsnp \u2502 \u251c\u2500\u2500 dna_repair_genes \u2502 \u251c\u2500\u2500 esp6500SI_v2 \u2502 \u251c\u2500\u2500 esp6500SI_v2_coverage \u2502 \u251c\u2500\u2500 familial \u2502 \u251c\u2500\u2500 gencode_out2 \u2502 \u251c\u2500\u2500 gencode_xrefseq \u2502 \u251c\u2500\u2500 hgnc \u2502 \u251c\u2500\u2500 mutsig \u2502 \u251c\u2500\u2500 oreganno \u2502 \u251c\u2500\u2500 override_lists \u2502 \u251c\u2500\u2500 ref_hg \u2502 \u251c\u2500\u2500 simple_uniprot \u2502 \u251c\u2500\u2500 so_terms \u2502 \u251c\u2500\u2500 tcgascape \u2502 \u251c\u2500\u2500 tumorscape \u2502 \u251c\u2500\u2500 uniprot_aa_annotation \u2502 \u2514\u2500\u2500 uniprot_aa_xform \u2514\u2500\u2500 SnpEff \u2514\u2500\u2500 4.1 \u2514\u2500\u2500 data \u251c\u2500\u2500 GRCh37.75 \u251c\u2500\u2500 GRCh38.79 \u251c\u2500\u2500 GRCm38.79 \u251c\u2500\u2500 hg19 \u251c\u2500\u2500 hg38 \u2514\u2500\u2500 mm10","title":"Application Support"},{"location":"cubit/app-support/#cubit-static-data-application-support","text":"The static_data/app_support directory contains all data files that are shipped with a software package installed in cubit. For blast this is not complete and more databases can be added upon request. static_data/app_support \u251c\u2500\u2500 blast \u2502 \u2514\u2500\u2500 variable \u2502 \u251c\u2500\u2500 nt \u2502 \u2514\u2500\u2500 refseq_protein \u251c\u2500\u2500 Delly \u2502 \u251c\u2500\u2500 0.6.5 \u2502 \u251c\u2500\u2500 0.6.7 \u2502 \u251c\u2500\u2500 0.7.1 \u2502 \u251c\u2500\u2500 0.7.2 \u2502 \u251c\u2500\u2500 0.7.3 \u2502 \u2514\u2500\u2500 0.7.5 \u251c\u2500\u2500 GATK_bundle \u2502 \u2514\u2500\u2500 2.8 \u2502 \u251c\u2500\u2500 b37 \u2502 \u2514\u2500\u2500 hg19 \u251c\u2500\u2500 Jannovar \u2502 \u251c\u2500\u2500 0.14 \u2502 \u2514\u2500\u2500 0.16 \u251c\u2500\u2500 kraken \u2502 \u2514\u2500\u2500 0.10.5-cubi20160426 \u2502 \u251c\u2500\u2500 bacvir \u2502 \u2514\u2500\u2500 minikraken_20141208 \u251c\u2500\u2500 Oncotator \u2502 \u2514\u2500\u2500 v1_ds_Jan262015 \u2502 \u251c\u2500\u2500 1000genome_db \u2502 \u251c\u2500\u2500 achilles \u2502 \u251c\u2500\u2500 cancer_gene_census \u2502 \u251c\u2500\u2500 ccle_by_gene \u2502 \u251c\u2500\u2500 ccle_by_gp \u2502 \u251c\u2500\u2500 clinvar \u2502 \u251c\u2500\u2500 cosmic \u2502 \u251c\u2500\u2500 cosmic_fusion \u2502 \u251c\u2500\u2500 cosmic_tissue \u2502 \u251c\u2500\u2500 dbNSFP_ds \u2502 \u251c\u2500\u2500 dbsnp \u2502 \u251c\u2500\u2500 dna_repair_genes \u2502 \u251c\u2500\u2500 esp6500SI_v2 \u2502 \u251c\u2500\u2500 esp6500SI_v2_coverage \u2502 \u251c\u2500\u2500 familial \u2502 \u251c\u2500\u2500 gencode_out2 \u2502 \u251c\u2500\u2500 gencode_xrefseq \u2502 \u251c\u2500\u2500 hgnc \u2502 \u251c\u2500\u2500 mutsig \u2502 \u251c\u2500\u2500 oreganno \u2502 \u251c\u2500\u2500 override_lists \u2502 \u251c\u2500\u2500 ref_hg \u2502 \u251c\u2500\u2500 simple_uniprot \u2502 \u251c\u2500\u2500 so_terms \u2502 \u251c\u2500\u2500 tcgascape \u2502 \u251c\u2500\u2500 tumorscape \u2502 \u251c\u2500\u2500 uniprot_aa_annotation \u2502 \u2514\u2500\u2500 uniprot_aa_xform \u2514\u2500\u2500 SnpEff \u2514\u2500\u2500 4.1 \u2514\u2500\u2500 data \u251c\u2500\u2500 GRCh37.75 \u251c\u2500\u2500 GRCh38.79 \u251c\u2500\u2500 GRCm38.79 \u251c\u2500\u2500 hg19 \u251c\u2500\u2500 hg38 \u2514\u2500\u2500 mm10","title":"Cubit Static Data: Application Support"},{"location":"cubit/databases/","text":"Databases \u00b6 The file formats in the static_data/db folder are mostly .vcf or .bed files. We provide the following databases: Database Version Reference genome COSMIC v72 GRCh37 dbNSFP 2.9 GRCh37/hg19 dbSNP b128 mm9 dbSNP b128 NCBIM37 dbSNP b142 GRCh37 dbSNP b144 GRCh38 dbSNP b147 GRCh37 dbSNP b147 GRCh38 DGV 2015-07-23 GRCh37 ExAC release0.3 GRCh37/hg19 ExAC release0.3.1 GRCh37/hg19 giab NA12878_HG001/NISTv2.19 GRCh37 goldenpath variable GRCh37 goldenpath variable hg19 goldenpath variable mm9 goldenpath variable NCBIM37 SangerMousegenomesProject REL-1211-SNPs_Indels mm9 SangerMousegenomesProject REL-1211-SNPs_Indels NCBIM37 UK10K cohort REL-2012-06-02 GRCh37 The directory structure is as follows: static_data/db \u251c\u2500\u2500 COSMIC \u2502 \u2514\u2500\u2500 v72 \u2502 \u2514\u2500\u2500 GRCh37 \u251c\u2500\u2500 dbNSFP \u2502 \u2514\u2500\u2500 2.9 \u251c\u2500\u2500 dbSNP \u2502 \u251c\u2500\u2500 b128 \u2502 \u2502 \u251c\u2500\u2500 mm9 \u2502 \u2502 \u2514\u2500\u2500 NCBIM37 \u2502 \u251c\u2500\u2500 b142 \u2502 \u2502 \u2514\u2500\u2500 GRCh37 \u2502 \u251c\u2500\u2500 b144 \u2502 \u2502 \u2514\u2500\u2500 GRCh38 \u2502 \u2514\u2500\u2500 b147 \u2502 \u251c\u2500\u2500 GRCh37 \u2502 \u2514\u2500\u2500 GRCh38 \u251c\u2500\u2500 DGV \u2502 \u2514\u2500\u2500 2015-07-23 \u2502 \u2514\u2500\u2500 GRCh37 \u251c\u2500\u2500 ExAC \u2502 \u251c\u2500\u2500 release0.3 \u2502 \u2514\u2500\u2500 release0.3.1 \u251c\u2500\u2500 giab \u2502 \u2514\u2500\u2500 NA12878_HG001 \u2502 \u2514\u2500\u2500 NISTv2.19 \u251c\u2500\u2500 goldenpath \u2502 \u2514\u2500\u2500 variable \u2502 \u251c\u2500\u2500 GRCh37 \u2502 \u251c\u2500\u2500 hg19 \u2502 \u251c\u2500\u2500 mm9 \u2502 \u2514\u2500\u2500 NCBIM37 \u251c\u2500\u2500 SangerMouseGenomesProject \u2502 \u2514\u2500\u2500 REL-1211-SNPs_Indels \u2502 \u251c\u2500\u2500 mm9 \u2502 \u2514\u2500\u2500 NCBIM37 \u2514\u2500\u2500 UK10K_cohort \u2514\u2500\u2500 REL-2012-06-02","title":"Databases"},{"location":"cubit/databases/#databases","text":"The file formats in the static_data/db folder are mostly .vcf or .bed files. We provide the following databases: Database Version Reference genome COSMIC v72 GRCh37 dbNSFP 2.9 GRCh37/hg19 dbSNP b128 mm9 dbSNP b128 NCBIM37 dbSNP b142 GRCh37 dbSNP b144 GRCh38 dbSNP b147 GRCh37 dbSNP b147 GRCh38 DGV 2015-07-23 GRCh37 ExAC release0.3 GRCh37/hg19 ExAC release0.3.1 GRCh37/hg19 giab NA12878_HG001/NISTv2.19 GRCh37 goldenpath variable GRCh37 goldenpath variable hg19 goldenpath variable mm9 goldenpath variable NCBIM37 SangerMousegenomesProject REL-1211-SNPs_Indels mm9 SangerMousegenomesProject REL-1211-SNPs_Indels NCBIM37 UK10K cohort REL-2012-06-02 GRCh37 The directory structure is as follows: static_data/db \u251c\u2500\u2500 COSMIC \u2502 \u2514\u2500\u2500 v72 \u2502 \u2514\u2500\u2500 GRCh37 \u251c\u2500\u2500 dbNSFP \u2502 \u2514\u2500\u2500 2.9 \u251c\u2500\u2500 dbSNP \u2502 \u251c\u2500\u2500 b128 \u2502 \u2502 \u251c\u2500\u2500 mm9 \u2502 \u2502 \u2514\u2500\u2500 NCBIM37 \u2502 \u251c\u2500\u2500 b142 \u2502 \u2502 \u2514\u2500\u2500 GRCh37 \u2502 \u251c\u2500\u2500 b144 \u2502 \u2502 \u2514\u2500\u2500 GRCh38 \u2502 \u2514\u2500\u2500 b147 \u2502 \u251c\u2500\u2500 GRCh37 \u2502 \u2514\u2500\u2500 GRCh38 \u251c\u2500\u2500 DGV \u2502 \u2514\u2500\u2500 2015-07-23 \u2502 \u2514\u2500\u2500 GRCh37 \u251c\u2500\u2500 ExAC \u2502 \u251c\u2500\u2500 release0.3 \u2502 \u2514\u2500\u2500 release0.3.1 \u251c\u2500\u2500 giab \u2502 \u2514\u2500\u2500 NA12878_HG001 \u2502 \u2514\u2500\u2500 NISTv2.19 \u251c\u2500\u2500 goldenpath \u2502 \u2514\u2500\u2500 variable \u2502 \u251c\u2500\u2500 GRCh37 \u2502 \u251c\u2500\u2500 hg19 \u2502 \u251c\u2500\u2500 mm9 \u2502 \u2514\u2500\u2500 NCBIM37 \u251c\u2500\u2500 SangerMouseGenomesProject \u2502 \u2514\u2500\u2500 REL-1211-SNPs_Indels \u2502 \u251c\u2500\u2500 mm9 \u2502 \u2514\u2500\u2500 NCBIM37 \u2514\u2500\u2500 UK10K_cohort \u2514\u2500\u2500 REL-2012-06-02","title":"Databases"},{"location":"cubit/exomes-panels/","text":"Exomes and Panels \u00b6 These exome panel data are proprietary and downloaded after registration. In case you want to use them, be sure you have access to them by creating an account at Agilent or Roche to not run into legal trouble. static_data/exome_panel \u251c\u2500\u2500 Agilent \u2502 \u251c\u2500\u2500 SureSelect_Human_All_Exon_V4 \u2502 \u2502 \u251c\u2500\u2500 GRCh37 \u2502 \u2502 \u2514\u2500\u2500 hg19 \u2502 \u251c\u2500\u2500 SureSelect_Human_All_Exon_V5 \u2502 \u2502 \u251c\u2500\u2500 GRCh37 \u2502 \u2502 \u2514\u2500\u2500 hg19 \u2502 \u251c\u2500\u2500 SureSelect_Human_All_Exon_V6 \u2502 \u2502 \u251c\u2500\u2500 GRCh37 \u2502 \u2502 \u2514\u2500\u2500 hg19 \u2502 \u2514\u2500\u2500 SureSelect_Mouse_All_Exon_V1 \u2502 \u251c\u2500\u2500 mm9 \u2502 \u2514\u2500\u2500 NCBIM37 \u2514\u2500\u2500 Roche \u2514\u2500\u2500 SeqCap_EZ_MedExome \u2514\u2500\u2500 GRCh37","title":"Exomes and Panels"},{"location":"cubit/exomes-panels/#exomes-and-panels","text":"These exome panel data are proprietary and downloaded after registration. In case you want to use them, be sure you have access to them by creating an account at Agilent or Roche to not run into legal trouble. static_data/exome_panel \u251c\u2500\u2500 Agilent \u2502 \u251c\u2500\u2500 SureSelect_Human_All_Exon_V4 \u2502 \u2502 \u251c\u2500\u2500 GRCh37 \u2502 \u2502 \u2514\u2500\u2500 hg19 \u2502 \u251c\u2500\u2500 SureSelect_Human_All_Exon_V5 \u2502 \u2502 \u251c\u2500\u2500 GRCh37 \u2502 \u2502 \u2514\u2500\u2500 hg19 \u2502 \u251c\u2500\u2500 SureSelect_Human_All_Exon_V6 \u2502 \u2502 \u251c\u2500\u2500 GRCh37 \u2502 \u2502 \u2514\u2500\u2500 hg19 \u2502 \u2514\u2500\u2500 SureSelect_Mouse_All_Exon_V1 \u2502 \u251c\u2500\u2500 mm9 \u2502 \u2514\u2500\u2500 NCBIM37 \u2514\u2500\u2500 Roche \u2514\u2500\u2500 SeqCap_EZ_MedExome \u2514\u2500\u2500 GRCh37","title":"Exomes and Panels"},{"location":"cubit/exon-lists/","text":"Exon Lists \u00b6 Here we provide exon lists for some human genome assemblies in the .bed -file format. Each file exists with the original coordinates contained and as a version with 10 bp padded on each site (suffix: _plus_10bp.bed ). The folder structure is self-explanatory: static_data/exon_list \u251c\u2500\u2500 CCDS \u2502 \u251c\u2500\u2500 15 \u2502 \u2502 \u251c\u2500\u2500 GRCh37 \u2502 \u2502 \u2514\u2500\u2500 hg19 \u2502 \u2514\u2500\u2500 18 \u2502 \u251c\u2500\u2500 GRCh38 \u2502 \u2514\u2500\u2500 hg38 \u2514\u2500\u2500 ENSEMBL \u251c\u2500\u2500 74 \u2502 \u2514\u2500\u2500 GRCh37 \u2514\u2500\u2500 75 \u2514\u2500\u2500 GRCh37","title":"Exon Lists"},{"location":"cubit/exon-lists/#exon-lists","text":"Here we provide exon lists for some human genome assemblies in the .bed -file format. Each file exists with the original coordinates contained and as a version with 10 bp padded on each site (suffix: _plus_10bp.bed ). The folder structure is self-explanatory: static_data/exon_list \u251c\u2500\u2500 CCDS \u2502 \u251c\u2500\u2500 15 \u2502 \u2502 \u251c\u2500\u2500 GRCh37 \u2502 \u2502 \u2514\u2500\u2500 hg19 \u2502 \u2514\u2500\u2500 18 \u2502 \u251c\u2500\u2500 GRCh38 \u2502 \u2514\u2500\u2500 hg38 \u2514\u2500\u2500 ENSEMBL \u251c\u2500\u2500 74 \u2502 \u2514\u2500\u2500 GRCh37 \u2514\u2500\u2500 75 \u2514\u2500\u2500 GRCh37","title":"Exon Lists"},{"location":"cubit/index-files/","text":"Precomputed Index Files \u00b6 Index files for BWA version 0.7.12 and 0.7.15, bowtie2 version 2.2.5 and STAR version 2.4.1d have been precomputed. The index corresponding to each genome is stored in the following directory structure with the above mentioned reference genomes as subfolders (listed here only for Bowtie/1.1.2 , same subfolders for the remaining programs): static_data/precomputed \u251c\u2500\u2500 Bowtie \u2502 \u2514\u2500\u2500 1.1.2 \u2502 \u251c\u2500\u2500 danRer10 \u2502 \u251c\u2500\u2500 dm6 \u2502 \u251c\u2500\u2500 ecoli \u2502 \u251c\u2500\u2500 GRCh37 \u2502 \u251c\u2500\u2500 GRCh38 \u2502 \u251c\u2500\u2500 GRCm38 \u2502 \u251c\u2500\u2500 hg18 \u2502 \u251c\u2500\u2500 hg19 \u2502 \u251c\u2500\u2500 hg38 \u2502 \u251c\u2500\u2500 mm10 \u2502 \u251c\u2500\u2500 mm9 \u2502 \u251c\u2500\u2500 NCBIM37 \u2502 \u251c\u2500\u2500 phix \u2502 \u251c\u2500\u2500 sacCer3 \u2502 \u251c\u2500\u2500 UniVec \u2502 \u2514\u2500\u2500 UniVec_Core \u251c\u2500\u2500 Bowtie2 \u2502 \u2514\u2500\u2500 2.2.5 \u2502 \u2514\u2500\u2500 [see Bowtie/1.1.2] \u251c\u2500\u2500 BWA \u2502 \u251c\u2500\u2500 0.7.12 \u2502 \u2502 \u2514\u2500\u2500 [see Bowtie/1.1.2] \u2502 \u2514\u2500\u2500 0.7.15 \u2502 \u2514\u2500\u2500 [see Bowtie/1.1.2] \u2514\u2500\u2500 STAR \u2514\u2500\u2500 2.4.1d \u2514\u2500\u2500 default \u2514\u2500\u2500 [see Bowtie/1.1.2]","title":"Precomputed Indexes"},{"location":"cubit/index-files/#precomputed-index-files","text":"Index files for BWA version 0.7.12 and 0.7.15, bowtie2 version 2.2.5 and STAR version 2.4.1d have been precomputed. The index corresponding to each genome is stored in the following directory structure with the above mentioned reference genomes as subfolders (listed here only for Bowtie/1.1.2 , same subfolders for the remaining programs): static_data/precomputed \u251c\u2500\u2500 Bowtie \u2502 \u2514\u2500\u2500 1.1.2 \u2502 \u251c\u2500\u2500 danRer10 \u2502 \u251c\u2500\u2500 dm6 \u2502 \u251c\u2500\u2500 ecoli \u2502 \u251c\u2500\u2500 GRCh37 \u2502 \u251c\u2500\u2500 GRCh38 \u2502 \u251c\u2500\u2500 GRCm38 \u2502 \u251c\u2500\u2500 hg18 \u2502 \u251c\u2500\u2500 hg19 \u2502 \u251c\u2500\u2500 hg38 \u2502 \u251c\u2500\u2500 mm10 \u2502 \u251c\u2500\u2500 mm9 \u2502 \u251c\u2500\u2500 NCBIM37 \u2502 \u251c\u2500\u2500 phix \u2502 \u251c\u2500\u2500 sacCer3 \u2502 \u251c\u2500\u2500 UniVec \u2502 \u2514\u2500\u2500 UniVec_Core \u251c\u2500\u2500 Bowtie2 \u2502 \u2514\u2500\u2500 2.2.5 \u2502 \u2514\u2500\u2500 [see Bowtie/1.1.2] \u251c\u2500\u2500 BWA \u2502 \u251c\u2500\u2500 0.7.12 \u2502 \u2502 \u2514\u2500\u2500 [see Bowtie/1.1.2] \u2502 \u2514\u2500\u2500 0.7.15 \u2502 \u2514\u2500\u2500 [see Bowtie/1.1.2] \u2514\u2500\u2500 STAR \u2514\u2500\u2500 2.4.1d \u2514\u2500\u2500 default \u2514\u2500\u2500 [see Bowtie/1.1.2]","title":"Precomputed Index Files"},{"location":"cubit/references/","text":"Reference Sequences \u00b6 NCBI mouse reference genome assemblies \u00b6 We provide the NCBI mouse reference assembly used by the Sanger Mouse Genomics group for NCBIM37 and GRCm38. This is a reliable source where the appropriate contigs have already been selected by experts. NCBIM37 is annotated with Ensembl release 67 and GRCm38 with Ensembl release 68. UCSC mouse reference genome assemblies \u00b6 The assembly sequence is in one file per chromosome and is available for mm9 and mm10. We concatenated all the chromosome files to one final fasta file for each genome assembly. NCBI human reference genome assemblies \u00b6 GRCh37: We provide the version used by the 1000genomes project as it is widely used and recommended. The chromosomes and contigs are already concatenated. g1k_phase1/hs37: This reference sequence contains the autosomal and both sex chromosomes, an updated mitochondrial chromosome as well as \"non-chromosomal supercontigs\". The README explains the method of construction. g1k_phase2/hs37d5: In addition to these sequences the phase 2 reference sequence contains the herpes virus genome and decoy sequences for improving SNP calling. GRCh38: The GRCh38 assembly offers an \"analysis set\" that was created to accommodate next generation sequencing read alignment pipelines. We provide the three analysis sets from the NCBI. hs38/no_alt_analysis_set: The chromosomes, mitochondrial genome, unlocalized scaffolds, unplaced scaffolds and the Epstein-Barr virus sequence which has been added as a decoy to attract contamination in samples. hs38a/full_analysis_set: the alternate locus scaffolds in addition to all the sequences present in the no_alt_analysis_set. hs38DH/full_plus_hs38d1_analysis_set: contains the human decoy sequences from hs38d1 in addition to all the sequences present in the full_analysis set. More detailed information is available in the README . UCSC human reference genome assemblies \u00b6 The assembly sequence is in one file per chromosome is available for hg18, hg19 and hg38. We concatenated all the chromosome files to one final fasta file for each genome assembly. Additionally, in the subfolder chromosomes we keep the chromosome fasta files separately for hg18 and hg19. Other reference genomes \u00b6 danRer10: UCSC/GRC zebrafish build 10 dm6: UCSC/GRC Drosophila melanogaster build 6 ecoli: GCA_000005845.2_ASM584v2: Genbank Escherichia coli K-12 subst. MG1655 genome genomemedley: 1: Concatenated genome of hg19, dm6, mm10; Chromosomes are tagged with corresponding organism PhiX: Control genome that is used by Illumina for sequencing runs sacCer3: UCSC's Saccharomyces cerevisiae genome build 3 UniVec: 9: NCBI's non redundant reference of vector sequences, adapters, linkers and primers commonly used in the process of cloning cDNA or genomic DNA (build 9) UniVec_Core 9: A subset of UniVec build 9 The following directory structure indicates the available genomes. Where there isn't a name for the data set, either the source (e.g. sanger - from the Sanger Mouse Genomes project) or the download date is used to name the sub-directory. static_data/reference \u251c\u2500\u2500 danRer10 \u2502 \u2514\u2500\u2500 ucsc \u251c\u2500\u2500 dm6 \u2502 \u2514\u2500\u2500 ucsc \u251c\u2500\u2500 ecoli \u2502 \u2514\u2500\u2500 GCA_000005845.2_ASM584v2 \u251c\u2500\u2500 genomemedley \u2502 \u2514\u2500\u2500 1 \u251c\u2500\u2500 GRCh37 \u2502 \u251c\u2500\u2500 g1k_phase1 \u2502 \u251c\u2500\u2500 g1k_phase2 \u2502 \u251c\u2500\u2500 hs37 \u2502 \u2514\u2500\u2500 hs37d5 \u251c\u2500\u2500 GRCh38 \u2502 \u251c\u2500\u2500 hs38 \u2502 \u251c\u2500\u2500 hs38a \u2502 \u2514\u2500\u2500 hs38DH \u251c\u2500\u2500 GRCm38 \u2502 \u2514\u2500\u2500 sanger \u251c\u2500\u2500 hg18 \u2502 \u2514\u2500\u2500 ucsc \u251c\u2500\u2500 hg19 \u2502 \u2514\u2500\u2500 ucsc \u251c\u2500\u2500 hg38 \u2502 \u2514\u2500\u2500 ucsc \u251c\u2500\u2500 mm10 \u2502 \u2514\u2500\u2500 ucsc \u251c\u2500\u2500 mm9 \u2502 \u2514\u2500\u2500 ucsc \u251c\u2500\u2500 NCBIM37 \u2502 \u2514\u2500\u2500 sanger \u251c\u2500\u2500 phix \u2502 \u2514\u2500\u2500 illumina \u251c\u2500\u2500 sacCer3 \u2502 \u2514\u2500\u2500 ucsc \u251c\u2500\u2500 UniVec \u2502 \u2514\u2500\u2500 9 \u2514\u2500\u2500 UniVec_Core \u2514\u2500\u2500 9","title":"Reference Sequences"},{"location":"cubit/references/#reference-sequences","text":"","title":"Reference Sequences"},{"location":"cubit/references/#ncbi-mouse-reference-genome-assemblies","text":"We provide the NCBI mouse reference assembly used by the Sanger Mouse Genomics group for NCBIM37 and GRCm38. This is a reliable source where the appropriate contigs have already been selected by experts. NCBIM37 is annotated with Ensembl release 67 and GRCm38 with Ensembl release 68.","title":"NCBI mouse reference genome assemblies"},{"location":"cubit/references/#ucsc-mouse-reference-genome-assemblies","text":"The assembly sequence is in one file per chromosome and is available for mm9 and mm10. We concatenated all the chromosome files to one final fasta file for each genome assembly.","title":"UCSC mouse reference genome assemblies"},{"location":"cubit/references/#ncbi-human-reference-genome-assemblies","text":"GRCh37: We provide the version used by the 1000genomes project as it is widely used and recommended. The chromosomes and contigs are already concatenated. g1k_phase1/hs37: This reference sequence contains the autosomal and both sex chromosomes, an updated mitochondrial chromosome as well as \"non-chromosomal supercontigs\". The README explains the method of construction. g1k_phase2/hs37d5: In addition to these sequences the phase 2 reference sequence contains the herpes virus genome and decoy sequences for improving SNP calling. GRCh38: The GRCh38 assembly offers an \"analysis set\" that was created to accommodate next generation sequencing read alignment pipelines. We provide the three analysis sets from the NCBI. hs38/no_alt_analysis_set: The chromosomes, mitochondrial genome, unlocalized scaffolds, unplaced scaffolds and the Epstein-Barr virus sequence which has been added as a decoy to attract contamination in samples. hs38a/full_analysis_set: the alternate locus scaffolds in addition to all the sequences present in the no_alt_analysis_set. hs38DH/full_plus_hs38d1_analysis_set: contains the human decoy sequences from hs38d1 in addition to all the sequences present in the full_analysis set. More detailed information is available in the README .","title":"NCBI human reference genome assemblies"},{"location":"cubit/references/#ucsc-human-reference-genome-assemblies","text":"The assembly sequence is in one file per chromosome is available for hg18, hg19 and hg38. We concatenated all the chromosome files to one final fasta file for each genome assembly. Additionally, in the subfolder chromosomes we keep the chromosome fasta files separately for hg18 and hg19.","title":"UCSC human reference genome assemblies"},{"location":"cubit/references/#other-reference-genomes","text":"danRer10: UCSC/GRC zebrafish build 10 dm6: UCSC/GRC Drosophila melanogaster build 6 ecoli: GCA_000005845.2_ASM584v2: Genbank Escherichia coli K-12 subst. MG1655 genome genomemedley: 1: Concatenated genome of hg19, dm6, mm10; Chromosomes are tagged with corresponding organism PhiX: Control genome that is used by Illumina for sequencing runs sacCer3: UCSC's Saccharomyces cerevisiae genome build 3 UniVec: 9: NCBI's non redundant reference of vector sequences, adapters, linkers and primers commonly used in the process of cloning cDNA or genomic DNA (build 9) UniVec_Core 9: A subset of UniVec build 9 The following directory structure indicates the available genomes. Where there isn't a name for the data set, either the source (e.g. sanger - from the Sanger Mouse Genomes project) or the download date is used to name the sub-directory. static_data/reference \u251c\u2500\u2500 danRer10 \u2502 \u2514\u2500\u2500 ucsc \u251c\u2500\u2500 dm6 \u2502 \u2514\u2500\u2500 ucsc \u251c\u2500\u2500 ecoli \u2502 \u2514\u2500\u2500 GCA_000005845.2_ASM584v2 \u251c\u2500\u2500 genomemedley \u2502 \u2514\u2500\u2500 1 \u251c\u2500\u2500 GRCh37 \u2502 \u251c\u2500\u2500 g1k_phase1 \u2502 \u251c\u2500\u2500 g1k_phase2 \u2502 \u251c\u2500\u2500 hs37 \u2502 \u2514\u2500\u2500 hs37d5 \u251c\u2500\u2500 GRCh38 \u2502 \u251c\u2500\u2500 hs38 \u2502 \u251c\u2500\u2500 hs38a \u2502 \u2514\u2500\u2500 hs38DH \u251c\u2500\u2500 GRCm38 \u2502 \u2514\u2500\u2500 sanger \u251c\u2500\u2500 hg18 \u2502 \u2514\u2500\u2500 ucsc \u251c\u2500\u2500 hg19 \u2502 \u2514\u2500\u2500 ucsc \u251c\u2500\u2500 hg38 \u2502 \u2514\u2500\u2500 ucsc \u251c\u2500\u2500 mm10 \u2502 \u2514\u2500\u2500 ucsc \u251c\u2500\u2500 mm9 \u2502 \u2514\u2500\u2500 ucsc \u251c\u2500\u2500 NCBIM37 \u2502 \u2514\u2500\u2500 sanger \u251c\u2500\u2500 phix \u2502 \u2514\u2500\u2500 illumina \u251c\u2500\u2500 sacCer3 \u2502 \u2514\u2500\u2500 ucsc \u251c\u2500\u2500 UniVec \u2502 \u2514\u2500\u2500 9 \u2514\u2500\u2500 UniVec_Core \u2514\u2500\u2500 9","title":"Other reference genomes"},{"location":"first-steps/episode-0/","text":"First Steps: Episode 0 \u00b6 Episode Topic 0 How can I install the tools? 1 How can I use the static data? 2 How can I distribute my jobs on the cluster (Slurm)? 3 How can I organize my jobs with Snakemake? 4 How can I combine Snakemake and Slurm? Prerequisites \u00b6 This tutorial assumes familiarity with Linux/Unix operating systems. It also assumes that you have already connected to the cluster. We have collected some links to tutorials and manuals on the internet . Legend \u00b6 Before we start with our first steps tutorial, we would like to introduce the following convention that we use throughout the series: $ Commands are prefixed with a little dollar sign While file paths are highlighted like this: /fast/projects/cubit/current . Instant Gratification \u00b6 After connecting to the cluster, you are located on a login node. To get to your first compute node, type srun -p long --time 7-00 --pty bash -i which will launch an interactive Bash session on a free remote node running up to 7 days in the \"long\" partition. Typing exit will you bring back to the login node. $ srun -p long --time 7-00 --pty bash -i med0107 $ exit $ See? That was easy! Preparation \u00b6 In preparation for our first steps tutorial series, we would like you to install the software for this tutorial. In general the users on the cluster will manage their own software with the help of conda. If you haven't done so so far, please follow the instructions in installing conda first. The only premise is that you are able to log into the cluster . Make also sure that you are logged in to a computation node using ``. Now we will create a new environment, so as to not interfere with your current or planned software stack, and install into it all the software that we need during the tutorial. Run the following commands: $ conda create -n first-steps python=3 snakemake drmaa bwa delly samtools gatk4 $ source activate first-steps (first-steps) $ As you can see, we also installed a tool called DRMAA (which is in fact a software library). DRMAA is an API that provides more stable job distribution on the cluster than the native SGE implementation. We will use this during the tutorial and we recommend it for your every day usage. If you are interested, we have a wiki page about this library and how to use it. For now there is no need for you to set up anything more.","title":"Episode 0"},{"location":"first-steps/episode-0/#first-steps-episode-0","text":"Episode Topic 0 How can I install the tools? 1 How can I use the static data? 2 How can I distribute my jobs on the cluster (Slurm)? 3 How can I organize my jobs with Snakemake? 4 How can I combine Snakemake and Slurm?","title":"First Steps: Episode 0"},{"location":"first-steps/episode-0/#prerequisites","text":"This tutorial assumes familiarity with Linux/Unix operating systems. It also assumes that you have already connected to the cluster. We have collected some links to tutorials and manuals on the internet .","title":"Prerequisites"},{"location":"first-steps/episode-0/#legend","text":"Before we start with our first steps tutorial, we would like to introduce the following convention that we use throughout the series: $ Commands are prefixed with a little dollar sign While file paths are highlighted like this: /fast/projects/cubit/current .","title":"Legend"},{"location":"first-steps/episode-0/#instant-gratification","text":"After connecting to the cluster, you are located on a login node. To get to your first compute node, type srun -p long --time 7-00 --pty bash -i which will launch an interactive Bash session on a free remote node running up to 7 days in the \"long\" partition. Typing exit will you bring back to the login node. $ srun -p long --time 7-00 --pty bash -i med0107 $ exit $ See? That was easy!","title":"Instant Gratification"},{"location":"first-steps/episode-0/#preparation","text":"In preparation for our first steps tutorial series, we would like you to install the software for this tutorial. In general the users on the cluster will manage their own software with the help of conda. If you haven't done so so far, please follow the instructions in installing conda first. The only premise is that you are able to log into the cluster . Make also sure that you are logged in to a computation node using ``. Now we will create a new environment, so as to not interfere with your current or planned software stack, and install into it all the software that we need during the tutorial. Run the following commands: $ conda create -n first-steps python=3 snakemake drmaa bwa delly samtools gatk4 $ source activate first-steps (first-steps) $ As you can see, we also installed a tool called DRMAA (which is in fact a software library). DRMAA is an API that provides more stable job distribution on the cluster than the native SGE implementation. We will use this during the tutorial and we recommend it for your every day usage. If you are interested, we have a wiki page about this library and how to use it. For now there is no need for you to set up anything more.","title":"Preparation"},{"location":"first-steps/episode-1/","text":"First Steps: Episode 1 \u00b6 Episode Topic 0 How can I install the tools? 1 How can I use the static data? 2 How can I distribute my jobs on the cluster (Slurm)? 3 How can I organize my jobs with Snakemake? 4 How can I combine Snakemake and Slurm? This is part one of the \"First Steps\" BIH Cluster Tutorial. Here we will build a small pipeline with alignment and variant calling. The premise is that you have the tools installed as described in Episode 0 . Tutorial Input Files \u00b6 We will provide you with some example FASTQ files, but you can use your own if you like. You can find the data here: /fast/projects/cubit/work/tutorial/input/test_R1.fq.gz /fast/projects/cubit/work/tutorial/input/test_R2.fq.gz Creating a Project Directory \u00b6 First, you should create a folder where the output of this tutorial will go. It would be good to have it in your work directory in /fast/users/$USER , because it is faster and there is more space available. (first-steps) $ mkdir -p /fast/users/$USER/work/tutorial/episode1 (first-steps) $ pushd /fast/users/$USER/work/tutorial/episode1 Quotas / File System limits Note well that you have a quota of 1 GB in your home directory at /fast/users/$USER . The reason for this is that nightly snapshots and backups are created for this directory which are precious resources. This limit does not apply to your work directory at /fast/users/$USER/work . The limits are much higher here but no snapshots or backups are available. There is no limit on your scratch directory at /fast/users/$USER/scratch . However, files placed here are automatically removed after 4 weeks. This is only appropriate for files during download or temporary files. Creating a Directory for Temporary Files \u00b6 In general it is advisable to have a proper temporary directory available. You can create one in your ~/scratch folder and make it available to the system. (first-steps) $ export TMPDIR=/fast/users/$USER/scratch/tmp (first-steps) $ mkdir -p $TMPDIR Using the Cubit Static Data \u00b6 The static data is located in /fast/projects/cubit/current/static_data . For our small example, the required reference genome and index can be found at: /fast/projects/cubit/current/static_data/reference/GRCh37/g1k_phase1/human_g1k_v37.fasta /fast/projects/cubit/current/static_data/precomputed/BWA/0.7.15/GRCh37/g1k_phase1/human_g1k_v37.fasta Aligning the Reads \u00b6 Let's align our data: (first-steps) $ bwa mem -t 8 \\ -R \"@RG\\tID:FLOWCELL.LANE\\tPL:ILLUMINA\\tLB:test\\tSM:PA01\" \\ /fast/projects/cubit/current/static_data/precomputed/BWA/0.7.15/GRCh37/g1k_phase1/human_g1k_v37.fasta \\ /fast/projects/cubit/work/tutorial/input/test_R1.fq.gz \\ /fast/projects/cubit/work/tutorial/input/test_R2.fq.gz \\ | samtools view -b - \\ | samtools sort -O BAM -T $TMPDIR -o aln.bam (first-steps) $ samtools index aln.bam Perform Structural Variant Calling \u00b6 And do the structural variant calling: (first-steps) $ delly call \\ -g /fast/projects/cubit/current/static_data/reference/GRCh37/g1k_phase1/human_g1k_v37.fasta \\ aln.bam Note that delly will not find any variants. Small Variant Calling (SNV, indel) \u00b6 And now for the SNP calling (this step will take ~ 20 minutes): (first-steps) $ gatk HaplotypeCaller \\ # or gatk HaplotypeCaller \\ -R /fast/projects/cubit/current/static_data/reference/GRCh37/g1k_phase1/human_g1k_v37.fasta \\ -I aln.bam \\ -ploidy 2 \\ -O test.GATK.vcf Outlook: More Programs and Static Data \u00b6 So this is it! We used the tools that we installed previously, accessed the reference data and ran a simple alignment and variant calling pipeline. You can access a list of all static data through this wiki, follow this link to the Static Data . You can also have a peek via: (first-steps) $ tree -L 3 /fast/projects/cubit/current/static_data | less","title":"Episode 1"},{"location":"first-steps/episode-1/#first-steps-episode-1","text":"Episode Topic 0 How can I install the tools? 1 How can I use the static data? 2 How can I distribute my jobs on the cluster (Slurm)? 3 How can I organize my jobs with Snakemake? 4 How can I combine Snakemake and Slurm? This is part one of the \"First Steps\" BIH Cluster Tutorial. Here we will build a small pipeline with alignment and variant calling. The premise is that you have the tools installed as described in Episode 0 .","title":"First Steps: Episode 1"},{"location":"first-steps/episode-1/#tutorial-input-files","text":"We will provide you with some example FASTQ files, but you can use your own if you like. You can find the data here: /fast/projects/cubit/work/tutorial/input/test_R1.fq.gz /fast/projects/cubit/work/tutorial/input/test_R2.fq.gz","title":"Tutorial Input Files"},{"location":"first-steps/episode-1/#creating-a-project-directory","text":"First, you should create a folder where the output of this tutorial will go. It would be good to have it in your work directory in /fast/users/$USER , because it is faster and there is more space available. (first-steps) $ mkdir -p /fast/users/$USER/work/tutorial/episode1 (first-steps) $ pushd /fast/users/$USER/work/tutorial/episode1 Quotas / File System limits Note well that you have a quota of 1 GB in your home directory at /fast/users/$USER . The reason for this is that nightly snapshots and backups are created for this directory which are precious resources. This limit does not apply to your work directory at /fast/users/$USER/work . The limits are much higher here but no snapshots or backups are available. There is no limit on your scratch directory at /fast/users/$USER/scratch . However, files placed here are automatically removed after 4 weeks. This is only appropriate for files during download or temporary files.","title":"Creating a Project Directory"},{"location":"first-steps/episode-1/#creating-a-directory-for-temporary-files","text":"In general it is advisable to have a proper temporary directory available. You can create one in your ~/scratch folder and make it available to the system. (first-steps) $ export TMPDIR=/fast/users/$USER/scratch/tmp (first-steps) $ mkdir -p $TMPDIR","title":"Creating a Directory for Temporary Files"},{"location":"first-steps/episode-1/#using-the-cubit-static-data","text":"The static data is located in /fast/projects/cubit/current/static_data . For our small example, the required reference genome and index can be found at: /fast/projects/cubit/current/static_data/reference/GRCh37/g1k_phase1/human_g1k_v37.fasta /fast/projects/cubit/current/static_data/precomputed/BWA/0.7.15/GRCh37/g1k_phase1/human_g1k_v37.fasta","title":"Using the Cubit Static Data"},{"location":"first-steps/episode-1/#aligning-the-reads","text":"Let's align our data: (first-steps) $ bwa mem -t 8 \\ -R \"@RG\\tID:FLOWCELL.LANE\\tPL:ILLUMINA\\tLB:test\\tSM:PA01\" \\ /fast/projects/cubit/current/static_data/precomputed/BWA/0.7.15/GRCh37/g1k_phase1/human_g1k_v37.fasta \\ /fast/projects/cubit/work/tutorial/input/test_R1.fq.gz \\ /fast/projects/cubit/work/tutorial/input/test_R2.fq.gz \\ | samtools view -b - \\ | samtools sort -O BAM -T $TMPDIR -o aln.bam (first-steps) $ samtools index aln.bam","title":"Aligning the Reads"},{"location":"first-steps/episode-1/#perform-structural-variant-calling","text":"And do the structural variant calling: (first-steps) $ delly call \\ -g /fast/projects/cubit/current/static_data/reference/GRCh37/g1k_phase1/human_g1k_v37.fasta \\ aln.bam Note that delly will not find any variants.","title":"Perform Structural Variant Calling"},{"location":"first-steps/episode-1/#small-variant-calling-snv-indel","text":"And now for the SNP calling (this step will take ~ 20 minutes): (first-steps) $ gatk HaplotypeCaller \\ # or gatk HaplotypeCaller \\ -R /fast/projects/cubit/current/static_data/reference/GRCh37/g1k_phase1/human_g1k_v37.fasta \\ -I aln.bam \\ -ploidy 2 \\ -O test.GATK.vcf","title":"Small Variant Calling (SNV, indel)"},{"location":"first-steps/episode-1/#outlook-more-programs-and-static-data","text":"So this is it! We used the tools that we installed previously, accessed the reference data and ran a simple alignment and variant calling pipeline. You can access a list of all static data through this wiki, follow this link to the Static Data . You can also have a peek via: (first-steps) $ tree -L 3 /fast/projects/cubit/current/static_data | less","title":"Outlook: More Programs and Static Data"},{"location":"first-steps/episode-2/","text":"First Steps: Episode 2 \u00b6 Episode Topic 0 How can I install the tools? 1 How can I use the static data? 2 How can I distribute my jobs on the cluster (Slurm)? 3 How can I organize my jobs with Snakemake? 4 How can I combine Snakemake and Slurm? Welcome to the second episode of our tutorial series! Once you are logged in to the cluster, you have the possibility to distribute your jobs to all the nodes that are available. But how can you do this easily? The key command to this magic is sbatch . This tutorial will show you how you can use this efficiently. The sbatch Command \u00b6 So what is sbatch doing for you? You use the sbatch command in front of the script you actually want to run. sbatch then puts your job into the job queue. The job scheduler looks at the current status of the whole system and will assign the first job in the queue to a node that is free in terms of computational load. If all machines are busy, yours will wait. But your job will sooner or later get assigned to a free node. We strongly recommend using this process for starting your computationally intensive tasks because you will get the best performance for your job and the whole system won't be disturbed by jobs that are locally blocking nodes. Thus, everybody using the cluster benefits. You may have noticed that you run sbatch with a script, not with regular commands. The reason is that sbatch only accepts bash scripts. If you give sbatch a normal shell command or binary, it won't work. This means that we have to put the command(s) we want to use in a bash script. A skeleton script can be found at /fast/projects/cubit/work/tutorial/skeletons/submit_job.sh The content of the file: #!/bin/bash # Set a name for the job (-J or --job-name). #SBATCH --job-name=tutorial # Set the file to write the stdout and stderr to (if -e is not set; -o or --output). #SBATCH --output=logs/%x-%j.log # Set the number of cores (-n or --ntasks). #SBATCH --ntasks=2 # Force allocation of the two cores on ONE node. #SBATCH --nodes=1 # Set the memory per CPU. Units can be given in T|G|M|K. #SBATCH --mem-per-cpu=100M # Set the partition to be used (-p or --partition). #SBATCH --partition=medium # Set the expected running time of your job (-t or --time). # Formats are MM:SS, HH:MM:SS, Days-HH, Days-HH:MM, Days-HH:MM:SS #SBATCH --time=30:00 export TMPDIR = /fast/users/ ${ USER } /scratch/tmp mkdir -p ${ TMPDIR } The lines starting with #SBATCH are actually setting parameters for a sbatch command, so #SBATCH --job-name=tutorial is equal to sbatch --job-name=tutorial . Slurm will create a log file with a file name composed of the job name ( %x ) and the job ID ( %j ), e.g. logs/tutorial-XXXX.log . It will not automatically create the logs directory, we need to do this manually first. Here, we emphasize the importance of the log files! They are the first place to look if anything goes wrong. To start now with our tutorial, create a new tutorial directory with a log directory, e.g., (first-steps) $ mkdir -p /fast/users/$USER/work/tutorial/episode2/logs and copy the wrapper script to this directory: (first-steps) $ pushd /fast/users/$USER/work/tutorial/episode2 (first-steps) $ cp /fast/projects/cubit/work/tutorial/skeletons/submit_job.sh . (first-steps) $ chmod u+w submit_job.sh Now open this file and copy the same commands we executed in the last tutorial to this file. To keep it simple, we will put everything into one script. This is perfectly fine because the alignment and indexing are sequential. But there are two steps that could be run in parallel, namely the variant calling, because they don't depend on each other. We will learn how to do that in a later tutorial. Your file should look something like this: #!/bin/bash # Set a name for the job (-J or --job-name). #SBATCH --job-name=tutorial # Set the file to write the stdout and stderr to (if -e is not set; -o or --output). #SBATCH --output=logs/%x-%j.log # Set the number of cores (-n or --ntasks). #SBATCH --ntasks=2 # Force allocation of the two cores on ONE node. #SBATCH --nodes=1 # Set the memory per CPU. Units can be given in T|G|M|K. #SBATCH --mem-per-cpu=100M # Set the partition to be used (-p or --partition). #SBATCH --partition=medium # Set the expected running time of your job (-t or --time). # Formats are MM:SS, HH:MM:SS, Days-HH, Days-HH:MM, Days-HH:MM:SS #SBATCH --time=30:00 export TMPDIR = /fast/users/ ${ USER } /scratch/tmp mkdir -p ${ TMPDIR } BWAREF = /fast/projects/cubit/current/static_data/precomputed/BWA/0.7.15/GRCh37/g1k_phase1/human_g1k_v37.fasta REF = /fast/projects/cubit/current/static_data/reference/GRCh37/g1k_phase1/human_g1k_v37.fasta bwa mem -t 8 \\ -R \"@RG\\tID:FLOWCELL.LANE\\tPL:ILLUMINA\\tLB:test\\tSM:PA01\" \\ $BWAREF \\ /fast/projects/cubit/work/tutorial/input/test_R1.fq.gz \\ /fast/projects/cubit/work/tutorial/input/test_R2.fq.gz \\ | samtools view -b - \\ | samtools sort -O BAM -T $TMPDIR -o aln.bam samtools index aln.bam delly call -g \\ $REF \\ aln.bam gatk HaplotypeCaller \\ -R $REF \\ -I aln.bam \\ -ploidy 2 \\ -O test.GATK.vcf Let's run it (make sure that you are in the tutorial/episode2 directory!): (first-steps) $ sbatch submit_job.sh And wait for the response which will tell you that your job was submitted and which job id number it was assigned. Note that sbatch only tells you that the job has started, but nothing about finishing. You won't get any response at the terminal when the job finishes. It will take approximately 20 minutes to finish the job. Monitoring Jobs \u00b6 You'll probably want to see how your job is doing. You can get a list of your jobs using: (first-steps) $ squeue --me Note that logins are also considered as jobs. Identify your job by the <JOBID> (1 st column) or the name of the script (3 rd column). The most likely states you will see (5 th column of the table): PD pending, waiting to be submitted R running disappeared, either because of an error or because it finished In the 8 th column you can see that your job is very likely running on a different machine than the one you are on! Get more information about your jobs by either passing the job id: (first-steps) $ sstat <JOBID> And of course, watch what the logs are telling you: (first-steps) $ tail -f logs/tutorial-<JOBID>.log There will be no notification when your job is done, so it is best to watch the squeue command. To watch the sbatch command there is a linux command watch that you give a command to execute every few seconds. This is useful for looking for changes in the output of a command. The seconds between two executions can be set with the -n option. It is best to use -n 60 to minimize unnecessary load on the file system: (first-steps) $ watch -n 60 squeue --me If for some reason your job is hanging, you can delete your job using qcancel with your job-ID: (first-steps) $ qcancel <job-ID> Job Queues \u00b6 The cluster has a special way of organizing itself and by telling the cluster how long and with which priority you want your jobs to run, you can help it in this. There is a system set up on the cluster where you can enqueue your jobs to so-called partitions . partitions have different prioritites and are allowed for different running times. To get to know what partitions are available, and how to use them properly, we highly encourage you to read the cluster queues wiki page .","title":"Episode 2"},{"location":"first-steps/episode-2/#first-steps-episode-2","text":"Episode Topic 0 How can I install the tools? 1 How can I use the static data? 2 How can I distribute my jobs on the cluster (Slurm)? 3 How can I organize my jobs with Snakemake? 4 How can I combine Snakemake and Slurm? Welcome to the second episode of our tutorial series! Once you are logged in to the cluster, you have the possibility to distribute your jobs to all the nodes that are available. But how can you do this easily? The key command to this magic is sbatch . This tutorial will show you how you can use this efficiently.","title":"First Steps: Episode 2"},{"location":"first-steps/episode-2/#the-sbatch-command","text":"So what is sbatch doing for you? You use the sbatch command in front of the script you actually want to run. sbatch then puts your job into the job queue. The job scheduler looks at the current status of the whole system and will assign the first job in the queue to a node that is free in terms of computational load. If all machines are busy, yours will wait. But your job will sooner or later get assigned to a free node. We strongly recommend using this process for starting your computationally intensive tasks because you will get the best performance for your job and the whole system won't be disturbed by jobs that are locally blocking nodes. Thus, everybody using the cluster benefits. You may have noticed that you run sbatch with a script, not with regular commands. The reason is that sbatch only accepts bash scripts. If you give sbatch a normal shell command or binary, it won't work. This means that we have to put the command(s) we want to use in a bash script. A skeleton script can be found at /fast/projects/cubit/work/tutorial/skeletons/submit_job.sh The content of the file: #!/bin/bash # Set a name for the job (-J or --job-name). #SBATCH --job-name=tutorial # Set the file to write the stdout and stderr to (if -e is not set; -o or --output). #SBATCH --output=logs/%x-%j.log # Set the number of cores (-n or --ntasks). #SBATCH --ntasks=2 # Force allocation of the two cores on ONE node. #SBATCH --nodes=1 # Set the memory per CPU. Units can be given in T|G|M|K. #SBATCH --mem-per-cpu=100M # Set the partition to be used (-p or --partition). #SBATCH --partition=medium # Set the expected running time of your job (-t or --time). # Formats are MM:SS, HH:MM:SS, Days-HH, Days-HH:MM, Days-HH:MM:SS #SBATCH --time=30:00 export TMPDIR = /fast/users/ ${ USER } /scratch/tmp mkdir -p ${ TMPDIR } The lines starting with #SBATCH are actually setting parameters for a sbatch command, so #SBATCH --job-name=tutorial is equal to sbatch --job-name=tutorial . Slurm will create a log file with a file name composed of the job name ( %x ) and the job ID ( %j ), e.g. logs/tutorial-XXXX.log . It will not automatically create the logs directory, we need to do this manually first. Here, we emphasize the importance of the log files! They are the first place to look if anything goes wrong. To start now with our tutorial, create a new tutorial directory with a log directory, e.g., (first-steps) $ mkdir -p /fast/users/$USER/work/tutorial/episode2/logs and copy the wrapper script to this directory: (first-steps) $ pushd /fast/users/$USER/work/tutorial/episode2 (first-steps) $ cp /fast/projects/cubit/work/tutorial/skeletons/submit_job.sh . (first-steps) $ chmod u+w submit_job.sh Now open this file and copy the same commands we executed in the last tutorial to this file. To keep it simple, we will put everything into one script. This is perfectly fine because the alignment and indexing are sequential. But there are two steps that could be run in parallel, namely the variant calling, because they don't depend on each other. We will learn how to do that in a later tutorial. Your file should look something like this: #!/bin/bash # Set a name for the job (-J or --job-name). #SBATCH --job-name=tutorial # Set the file to write the stdout and stderr to (if -e is not set; -o or --output). #SBATCH --output=logs/%x-%j.log # Set the number of cores (-n or --ntasks). #SBATCH --ntasks=2 # Force allocation of the two cores on ONE node. #SBATCH --nodes=1 # Set the memory per CPU. Units can be given in T|G|M|K. #SBATCH --mem-per-cpu=100M # Set the partition to be used (-p or --partition). #SBATCH --partition=medium # Set the expected running time of your job (-t or --time). # Formats are MM:SS, HH:MM:SS, Days-HH, Days-HH:MM, Days-HH:MM:SS #SBATCH --time=30:00 export TMPDIR = /fast/users/ ${ USER } /scratch/tmp mkdir -p ${ TMPDIR } BWAREF = /fast/projects/cubit/current/static_data/precomputed/BWA/0.7.15/GRCh37/g1k_phase1/human_g1k_v37.fasta REF = /fast/projects/cubit/current/static_data/reference/GRCh37/g1k_phase1/human_g1k_v37.fasta bwa mem -t 8 \\ -R \"@RG\\tID:FLOWCELL.LANE\\tPL:ILLUMINA\\tLB:test\\tSM:PA01\" \\ $BWAREF \\ /fast/projects/cubit/work/tutorial/input/test_R1.fq.gz \\ /fast/projects/cubit/work/tutorial/input/test_R2.fq.gz \\ | samtools view -b - \\ | samtools sort -O BAM -T $TMPDIR -o aln.bam samtools index aln.bam delly call -g \\ $REF \\ aln.bam gatk HaplotypeCaller \\ -R $REF \\ -I aln.bam \\ -ploidy 2 \\ -O test.GATK.vcf Let's run it (make sure that you are in the tutorial/episode2 directory!): (first-steps) $ sbatch submit_job.sh And wait for the response which will tell you that your job was submitted and which job id number it was assigned. Note that sbatch only tells you that the job has started, but nothing about finishing. You won't get any response at the terminal when the job finishes. It will take approximately 20 minutes to finish the job.","title":"The sbatch Command"},{"location":"first-steps/episode-2/#monitoring-jobs","text":"You'll probably want to see how your job is doing. You can get a list of your jobs using: (first-steps) $ squeue --me Note that logins are also considered as jobs. Identify your job by the <JOBID> (1 st column) or the name of the script (3 rd column). The most likely states you will see (5 th column of the table): PD pending, waiting to be submitted R running disappeared, either because of an error or because it finished In the 8 th column you can see that your job is very likely running on a different machine than the one you are on! Get more information about your jobs by either passing the job id: (first-steps) $ sstat <JOBID> And of course, watch what the logs are telling you: (first-steps) $ tail -f logs/tutorial-<JOBID>.log There will be no notification when your job is done, so it is best to watch the squeue command. To watch the sbatch command there is a linux command watch that you give a command to execute every few seconds. This is useful for looking for changes in the output of a command. The seconds between two executions can be set with the -n option. It is best to use -n 60 to minimize unnecessary load on the file system: (first-steps) $ watch -n 60 squeue --me If for some reason your job is hanging, you can delete your job using qcancel with your job-ID: (first-steps) $ qcancel <job-ID>","title":"Monitoring Jobs"},{"location":"first-steps/episode-2/#job-queues","text":"The cluster has a special way of organizing itself and by telling the cluster how long and with which priority you want your jobs to run, you can help it in this. There is a system set up on the cluster where you can enqueue your jobs to so-called partitions . partitions have different prioritites and are allowed for different running times. To get to know what partitions are available, and how to use them properly, we highly encourage you to read the cluster queues wiki page .","title":"Job Queues"},{"location":"first-steps/episode-3/","text":"First Steps: Episode 3 \u00b6 Episode Topic 0 How can I install the tools? 1 How can I use the static data? 2 How can I distribute my jobs on the cluster (Slurm)? 3 How can I organize my jobs with Snakemake? 4 How can I combine Snakemake and Slurm? In this episode we will discuss how we can parallelize steps in a pipeline that are not dependent on each other. In the last episode we saw a case (the variant calling) that could have been potentially parallelized. We will take care of that today. Please note that we are not going to use the sbatch command we learned earlier. Thus, this tutorial will run on the same node where you execute the script. We will introduce you to Snakemake, a tool with which we can model dependencies and run things in parallel. In the next tutorial we will learn how to submit the jobs with sbatch and Snakemake combined. For those who know make already, Snakemake will be familiar. You can think of Snakemake being a bunch of dedicated bash scripts that you can make dependent on each other. Snakemake will start the next script when a previous one finishes, and potentially it will run things in parallel if the dependencies allow. Snakemake can get confusing, especially if the project gets big. This tutorial will only cover the very basics of this powerful tool. For more, we highly recommend digging into the Snakemake documentation: https://snakemake.readthedocs.io/en/stable/ http://slides.com/johanneskoester/deck-1#/ Every Snakemake run requires a Snakefile file. Create a new folder inside your tutorial folder and copy the skeleton: (first-steps) $ mkdir -p /fast/users/${USER}/work/tutorial/episode3 (first-steps) $ pushd /fast/users/${USER}/work/tutorial/episode3 (first-steps) $ cp /fast/projects/cubit/work/tutorial/skeletons/Snakefile . (first-steps) $ chmod u+w Snakefile Your Snakefile should look as follows: rule all : input : 'snps/test.vcf' , 'structural_variants/test.vcf' rule alignment : input : '/fast/projects/cubit/work/tutorial/input/test_R1.fq.gz' , '/fast/projects/cubit/work/tutorial/input/test_R2.fq.gz' , output : bam = 'alignment/test.bam' , bai = 'alignment/test.bam.bai' , shell : r \"\"\" export TMPDIR=/fast/users/${{USER}}/scratch/tmp mkdir -p ${{TMPDIR}} BWAREF=/fast/projects/cubit/current/static_data/precomputed/BWA/0.7.15/GRCh37/g1k_phase1/human_g1k_v37.fasta bwa mem -t 8 \\ -R \"@RG\\tID:FLOWCELL.LANE\\tPL:ILLUMINA\\tLB:test\\tSM:PA01\" \\ ${{BWAREF}} \\ {input} \\ | samtools view -b - \\ | samtools sort -O BAM -T ${{TMPDIR}} -o {output.bam} samtools index {output.bam} \"\"\" rule structural_variants : input : 'alignment/test.bam' output : 'structural_variants/test.vcf' shell : r \"\"\" REF=/fast/projects/cubit/current/static_data/reference/GRCh37/g1k_phase1/human_g1k_v37.fasta delly call -o {output} -g ${{REF}} {input} \"\"\" rule snps : input : 'alignment/test.bam' output : 'snps/test.vcf' shell : r \"\"\" REF=/fast/projects/cubit/current/static_data/reference/GRCh37/g1k_phase1/human_g1k_v37.fasta gatk HaplotypeCaller \\ -R ${{REF}} \\ -I {input} \\ -ploidy 2 \\ -O {output} \"\"\" Let me explain. The content resembles the same steps we took in the previous tutorials. Although every step has its own rule (alignment, snp calling, structural variant calling), we could instead have written everything in one rule. It is up to you to design your rules! Note that the rule names are arbitrary and not mentioned anywhere else in the file. But there is one primary rule: the rule all . This is the kickoff rule that makes everything run. As you might have noticed, every rule has three main parameters: input , output and shell . input defines the files that are going into the rule, output those that are produced when executing the rule, and shell is the bash script that processes input to produce output . Rule all does not have any output or shell , it uses input to start the chain of rules. Note that the input files of this rule are the output files of rule snps and structural_variants . The input of those rules is the output of rule alignment . This is how Snakemake processes the rules: It looks for rule all (or a rule that just has input files) and figures out how it can create the required input files with other rules by looking at their output files (the input files of one rule must be the output files of another rule). In our case it traces the workflow back to rule snps and structural_variants as they have the matching output files. They depend in return on the alignment, so the alignment rule must be executed, and this is the first thing that will be done by Snakemake. There are also some peculiarities about Snakemake: You can name files in input or output as is done in rule alignment with the output files. You can access the input and output files in the script by writing {input} or {output} . If they are not named, they will be concatenated, separated by white space If they are named, access them with their name, e.g., {output.bam} Curly braces must be escaped with curly braces, e.g., for bash variables: ${{VAR}} instead of ${VAR} but not Snakemake internal variables like {input} or {output} In the rule structural_variants we cheat a bit because delly does not produce output files if it can't find variants. We do this by touching (i.e., creating) the required output file. Snakemake has a function for doing so (call touch() on the filename). Intermediate folders in the path to output files are always created if they don't exist. Because Snakemake is Python based, you can write your own functions for it to use, e.g. for creating file names automatically. But Snakemake can do more. It is able to parse the paths of the output files and set wildcards if you want. For this your input (and output) file names have to follow a parsable scheme. In our case they do! Our FASTQ files, our only initial input files, start with test . The output of the alignment as well as the variant calling is also prefixed test . We now can modify the Snakemake file accordingly, by exchanging every occurrence of test in each input or output field with {id} (note that you could also give a different name for your variable). Only the input rule should not be touched, otherwise Snakemake would not know which value this variable should have. Your Snakefile should look now like this: rule all : input : 'snps/test.vcf' , 'structural_variants/test.vcf' rule alignment : input : l '/fast/projects/cubit/work/tutorial/input/ {id} _R1.fq.gz' , '/fast/projects/cubit/work/tutorial/input/ {id} _R2.fq.gz' , output : bam = 'alignment/ {id} .bam' , bai = 'alignment/ {id} .bam.bai' , shell : r \"\"\" export TMPDIR=/fast/users/${{USER}}/scratch/tmp mkdir -p ${{TMPDIR}} BWAREF=/fast/projects/cubit/current/static_data/precomputed/BWA/0.7.15/GRCh37/g1k_phase1/human_g1k_v37.fasta bwa mem -t 8 \\ -R \"@RG\\tID:FLOWCELL.LANE\\tPL:ILLUMINA\\tLB:test\\tSM:PA01\" \\ ${{BWAREF}} \\ {input} \\ | samtools view -b - \\ | samtools sort -O BAM -T ${{TMPDIR}} -o {output.bam} samtools index {output.bam} \"\"\" rule structural_variants : input : 'alignment/ {id} .bam' output : 'structural_variants/ {id} .vcf' shell : r \"\"\" REF=/fast/projects/cubit/current/static_data/reference/GRCh37/g1k_phase1/human_g1k_v37.fasta delly call -o {output} -g ${{REF}} {input} \"\"\" rule snps : input : 'alignment/ {id} .bam' output : 'snps/ {id} .vcf' shell : r \"\"\" REF=/fast/projects/cubit/current/static_data/reference/GRCh37/g1k_phase1/human_g1k_v37.fasta gatk HaplotypeCaller \\ -R ${{REF}} \\ -I {input} \\ -ploidy 2 \\ -O {output} \"\"\" Before we finally run this, we can make a dry run. Snakemake will show you what it would do: (first-steps) $ snakemake -n If everything looks green, you can run it for real. We provide it two cores to allow two single-threaded jobs to be run simultaneously: (first-steps) $ snakemake -j 2","title":"Episode 3"},{"location":"first-steps/episode-3/#first-steps-episode-3","text":"Episode Topic 0 How can I install the tools? 1 How can I use the static data? 2 How can I distribute my jobs on the cluster (Slurm)? 3 How can I organize my jobs with Snakemake? 4 How can I combine Snakemake and Slurm? In this episode we will discuss how we can parallelize steps in a pipeline that are not dependent on each other. In the last episode we saw a case (the variant calling) that could have been potentially parallelized. We will take care of that today. Please note that we are not going to use the sbatch command we learned earlier. Thus, this tutorial will run on the same node where you execute the script. We will introduce you to Snakemake, a tool with which we can model dependencies and run things in parallel. In the next tutorial we will learn how to submit the jobs with sbatch and Snakemake combined. For those who know make already, Snakemake will be familiar. You can think of Snakemake being a bunch of dedicated bash scripts that you can make dependent on each other. Snakemake will start the next script when a previous one finishes, and potentially it will run things in parallel if the dependencies allow. Snakemake can get confusing, especially if the project gets big. This tutorial will only cover the very basics of this powerful tool. For more, we highly recommend digging into the Snakemake documentation: https://snakemake.readthedocs.io/en/stable/ http://slides.com/johanneskoester/deck-1#/ Every Snakemake run requires a Snakefile file. Create a new folder inside your tutorial folder and copy the skeleton: (first-steps) $ mkdir -p /fast/users/${USER}/work/tutorial/episode3 (first-steps) $ pushd /fast/users/${USER}/work/tutorial/episode3 (first-steps) $ cp /fast/projects/cubit/work/tutorial/skeletons/Snakefile . (first-steps) $ chmod u+w Snakefile Your Snakefile should look as follows: rule all : input : 'snps/test.vcf' , 'structural_variants/test.vcf' rule alignment : input : '/fast/projects/cubit/work/tutorial/input/test_R1.fq.gz' , '/fast/projects/cubit/work/tutorial/input/test_R2.fq.gz' , output : bam = 'alignment/test.bam' , bai = 'alignment/test.bam.bai' , shell : r \"\"\" export TMPDIR=/fast/users/${{USER}}/scratch/tmp mkdir -p ${{TMPDIR}} BWAREF=/fast/projects/cubit/current/static_data/precomputed/BWA/0.7.15/GRCh37/g1k_phase1/human_g1k_v37.fasta bwa mem -t 8 \\ -R \"@RG\\tID:FLOWCELL.LANE\\tPL:ILLUMINA\\tLB:test\\tSM:PA01\" \\ ${{BWAREF}} \\ {input} \\ | samtools view -b - \\ | samtools sort -O BAM -T ${{TMPDIR}} -o {output.bam} samtools index {output.bam} \"\"\" rule structural_variants : input : 'alignment/test.bam' output : 'structural_variants/test.vcf' shell : r \"\"\" REF=/fast/projects/cubit/current/static_data/reference/GRCh37/g1k_phase1/human_g1k_v37.fasta delly call -o {output} -g ${{REF}} {input} \"\"\" rule snps : input : 'alignment/test.bam' output : 'snps/test.vcf' shell : r \"\"\" REF=/fast/projects/cubit/current/static_data/reference/GRCh37/g1k_phase1/human_g1k_v37.fasta gatk HaplotypeCaller \\ -R ${{REF}} \\ -I {input} \\ -ploidy 2 \\ -O {output} \"\"\" Let me explain. The content resembles the same steps we took in the previous tutorials. Although every step has its own rule (alignment, snp calling, structural variant calling), we could instead have written everything in one rule. It is up to you to design your rules! Note that the rule names are arbitrary and not mentioned anywhere else in the file. But there is one primary rule: the rule all . This is the kickoff rule that makes everything run. As you might have noticed, every rule has three main parameters: input , output and shell . input defines the files that are going into the rule, output those that are produced when executing the rule, and shell is the bash script that processes input to produce output . Rule all does not have any output or shell , it uses input to start the chain of rules. Note that the input files of this rule are the output files of rule snps and structural_variants . The input of those rules is the output of rule alignment . This is how Snakemake processes the rules: It looks for rule all (or a rule that just has input files) and figures out how it can create the required input files with other rules by looking at their output files (the input files of one rule must be the output files of another rule). In our case it traces the workflow back to rule snps and structural_variants as they have the matching output files. They depend in return on the alignment, so the alignment rule must be executed, and this is the first thing that will be done by Snakemake. There are also some peculiarities about Snakemake: You can name files in input or output as is done in rule alignment with the output files. You can access the input and output files in the script by writing {input} or {output} . If they are not named, they will be concatenated, separated by white space If they are named, access them with their name, e.g., {output.bam} Curly braces must be escaped with curly braces, e.g., for bash variables: ${{VAR}} instead of ${VAR} but not Snakemake internal variables like {input} or {output} In the rule structural_variants we cheat a bit because delly does not produce output files if it can't find variants. We do this by touching (i.e., creating) the required output file. Snakemake has a function for doing so (call touch() on the filename). Intermediate folders in the path to output files are always created if they don't exist. Because Snakemake is Python based, you can write your own functions for it to use, e.g. for creating file names automatically. But Snakemake can do more. It is able to parse the paths of the output files and set wildcards if you want. For this your input (and output) file names have to follow a parsable scheme. In our case they do! Our FASTQ files, our only initial input files, start with test . The output of the alignment as well as the variant calling is also prefixed test . We now can modify the Snakemake file accordingly, by exchanging every occurrence of test in each input or output field with {id} (note that you could also give a different name for your variable). Only the input rule should not be touched, otherwise Snakemake would not know which value this variable should have. Your Snakefile should look now like this: rule all : input : 'snps/test.vcf' , 'structural_variants/test.vcf' rule alignment : input : l '/fast/projects/cubit/work/tutorial/input/ {id} _R1.fq.gz' , '/fast/projects/cubit/work/tutorial/input/ {id} _R2.fq.gz' , output : bam = 'alignment/ {id} .bam' , bai = 'alignment/ {id} .bam.bai' , shell : r \"\"\" export TMPDIR=/fast/users/${{USER}}/scratch/tmp mkdir -p ${{TMPDIR}} BWAREF=/fast/projects/cubit/current/static_data/precomputed/BWA/0.7.15/GRCh37/g1k_phase1/human_g1k_v37.fasta bwa mem -t 8 \\ -R \"@RG\\tID:FLOWCELL.LANE\\tPL:ILLUMINA\\tLB:test\\tSM:PA01\" \\ ${{BWAREF}} \\ {input} \\ | samtools view -b - \\ | samtools sort -O BAM -T ${{TMPDIR}} -o {output.bam} samtools index {output.bam} \"\"\" rule structural_variants : input : 'alignment/ {id} .bam' output : 'structural_variants/ {id} .vcf' shell : r \"\"\" REF=/fast/projects/cubit/current/static_data/reference/GRCh37/g1k_phase1/human_g1k_v37.fasta delly call -o {output} -g ${{REF}} {input} \"\"\" rule snps : input : 'alignment/ {id} .bam' output : 'snps/ {id} .vcf' shell : r \"\"\" REF=/fast/projects/cubit/current/static_data/reference/GRCh37/g1k_phase1/human_g1k_v37.fasta gatk HaplotypeCaller \\ -R ${{REF}} \\ -I {input} \\ -ploidy 2 \\ -O {output} \"\"\" Before we finally run this, we can make a dry run. Snakemake will show you what it would do: (first-steps) $ snakemake -n If everything looks green, you can run it for real. We provide it two cores to allow two single-threaded jobs to be run simultaneously: (first-steps) $ snakemake -j 2","title":"First Steps: Episode 3"},{"location":"first-steps/episode-4/","text":"First Steps: Episode 4 \u00b6 Episode Topic 0 How can I install the tools? 1 How can I use the static data? 2 How can I distribute my jobs on the cluster (Slurm)? 3 How can I organize my jobs with Snakemake? 4 How can I combine Snakemake and Slurm? In the last episodes we learned about distributing a job among the cluster nodes using sbatch and how to automate and parallelize our pipeline with Snakemake. We are lucky that those two powerful commands can be combined. What is the result? You will have an automated pipeline with Snakemake that uses sbatch to distribute jobs among the cluster nodes instead of running only the same node. The best thing is that we can reuse our Snakefile as it is and just write a wrapper script to call Snakemake. We run the script and the magic will start. First, create a new folder for this episode: (first-steps) $ mkdir -p /fast/users/${USER}/work/tutorial/episode4/logs (first-steps) $ pushd /fast/users/${USER}/work/tutorial/episode4 And copy the wrapper script to this folder as well as the Snakefile (you can also reuse the one with the adjustments from the previous episode ): (first-steps) $ cp /fast/projects/cubit/work/tutorial/skeletons/submit_snakejob.sh . (first-steps) $ cp /fast/projects/cubit/work/tutorial/skeletons/Snakefile . (first-steps) $ chmod u+w submit_snakejob.sh Snakefile The Snakefile is already known to you but let me explain the wrapper script submit_snakejob.sh : #!/bin/bash # Set a name for the job (-J or --job-name). #SBATCH --job-name=tutorial # Set the file to write the stdout and stderr to (if -e is not set; -o or --output). #SBATCH --output=logs/%x-%j.log # Set the number of cores (-n or --ntasks). #SBATCH --ntasks=2 # Force allocation of the two cores on ONE node. #SBATCH --nodes=1 # Set the memory per CPU. Units can be given in T|G|M|K. #SBATCH --mem-per-cpu=100M # Set the partition to be used (-p or --partition). #SBATCH --partition=medium # Set the expected running time of your job (-t or --time). # Formats are MM:SS, HH:MM:SS, Days-HH, Days-HH:MM, Days-HH:MM:SS #SBATCH --time=30:00 export TMPDIR = /fast/users/ ${ USER } /scratch/tmp export LOGDIR = logs/ ${ SLURM_JOB_NAME } - ${ SLURM_JOB_ID } mkdir -p $LOGDIR unset DRMAA_LIBRARY_PATH eval \" $($( which conda ) shell.bash hook ) \" conda activate first-steps set -x # Note that Slurm DRMAA differs slightly from original Slurm syntax # --mem-per-cpu doesn't accept units and the default unit here is MB # -t only accepts HH:MM snakemake \\ --drmaa \" \\ -p medium \\ -t 01:00 \\ --nodes=1 \\ --mem-per-cpu=1000 \\ -n 8 \\ -o $LOGDIR /%x-%j.log\" \\ -j 2 \\ -k \\ -p In the beginning you see the #SBATCH that introduces the parameters when you provide this script to sbatch as described in the second episode . Please make sure that the logs folder exists before starting the run! We then set and export the TMPDIR and LOGDIR variables. Note that LOGDIR has a subfolder named $SLURM_JOB_NAME-$SLURM_JOB_ID that will be created for you. Snakemake will store its logfiles for this very Snakemake run in this folder. The next new thing is set -x . This simply prints to the terminal every command that is executed within the script. This is useful for debugging. Finally, the Snakemake call takes place. With the --drmaa option we define how the jobs inside Snakemake should be started on the nodes. Please note that the sbatch command does not appear here, but the argument string provided to the --drmaa option is the same as the parameters for sbatch , except for some minor differences as descrived above or here . The drmaa library provides more stable job distribution than using plain sbatch in this scenario. Note that we don't write the parameter string to a bash file (which we don't have in this case). There are three new parameters that are defining the hardware requirements for our run and the project: --mem-per-cpu=X : How much memory ONE job (=Snakemake rule) should get in Megabyte -t HH:MM : How long a job (=Snakemake rule) is allowed to run -n X : How many cores of a node a job (=Snakemake rule) should get --nodes=1 : Force allocation of all cores on a single node. -p medium : The partition name (this system was introduced in Episode 2 and is described here ) Note that the memory is not shared among the cores. This means the final memory on a node is defined by <mem-per-cpu> * <n> So in our example the total memory requested on one node would be 8GB. The other parameters provided to Snakemake are: -j : Use 2 cores for this Snakemake run. In this scenario the parameter determines how many jobs will be submitted to SLURM at a time. -k : Keep going if a job fails -p : Print out shell commands Finally, run the script: (first-steps) $ sbatch submit_snakejob.sh If you watch squeue --me now, you will see that the jobs are distributed to the system: (first-steps) $ watch -n 60 squeue --me Please refer to the Snakemake documentation for more details on using Snakemake, in particular how to use the cluster configuration on how to specify the resource requirements on a per-rule base.","title":"Episode 4"},{"location":"first-steps/episode-4/#first-steps-episode-4","text":"Episode Topic 0 How can I install the tools? 1 How can I use the static data? 2 How can I distribute my jobs on the cluster (Slurm)? 3 How can I organize my jobs with Snakemake? 4 How can I combine Snakemake and Slurm? In the last episodes we learned about distributing a job among the cluster nodes using sbatch and how to automate and parallelize our pipeline with Snakemake. We are lucky that those two powerful commands can be combined. What is the result? You will have an automated pipeline with Snakemake that uses sbatch to distribute jobs among the cluster nodes instead of running only the same node. The best thing is that we can reuse our Snakefile as it is and just write a wrapper script to call Snakemake. We run the script and the magic will start. First, create a new folder for this episode: (first-steps) $ mkdir -p /fast/users/${USER}/work/tutorial/episode4/logs (first-steps) $ pushd /fast/users/${USER}/work/tutorial/episode4 And copy the wrapper script to this folder as well as the Snakefile (you can also reuse the one with the adjustments from the previous episode ): (first-steps) $ cp /fast/projects/cubit/work/tutorial/skeletons/submit_snakejob.sh . (first-steps) $ cp /fast/projects/cubit/work/tutorial/skeletons/Snakefile . (first-steps) $ chmod u+w submit_snakejob.sh Snakefile The Snakefile is already known to you but let me explain the wrapper script submit_snakejob.sh : #!/bin/bash # Set a name for the job (-J or --job-name). #SBATCH --job-name=tutorial # Set the file to write the stdout and stderr to (if -e is not set; -o or --output). #SBATCH --output=logs/%x-%j.log # Set the number of cores (-n or --ntasks). #SBATCH --ntasks=2 # Force allocation of the two cores on ONE node. #SBATCH --nodes=1 # Set the memory per CPU. Units can be given in T|G|M|K. #SBATCH --mem-per-cpu=100M # Set the partition to be used (-p or --partition). #SBATCH --partition=medium # Set the expected running time of your job (-t or --time). # Formats are MM:SS, HH:MM:SS, Days-HH, Days-HH:MM, Days-HH:MM:SS #SBATCH --time=30:00 export TMPDIR = /fast/users/ ${ USER } /scratch/tmp export LOGDIR = logs/ ${ SLURM_JOB_NAME } - ${ SLURM_JOB_ID } mkdir -p $LOGDIR unset DRMAA_LIBRARY_PATH eval \" $($( which conda ) shell.bash hook ) \" conda activate first-steps set -x # Note that Slurm DRMAA differs slightly from original Slurm syntax # --mem-per-cpu doesn't accept units and the default unit here is MB # -t only accepts HH:MM snakemake \\ --drmaa \" \\ -p medium \\ -t 01:00 \\ --nodes=1 \\ --mem-per-cpu=1000 \\ -n 8 \\ -o $LOGDIR /%x-%j.log\" \\ -j 2 \\ -k \\ -p In the beginning you see the #SBATCH that introduces the parameters when you provide this script to sbatch as described in the second episode . Please make sure that the logs folder exists before starting the run! We then set and export the TMPDIR and LOGDIR variables. Note that LOGDIR has a subfolder named $SLURM_JOB_NAME-$SLURM_JOB_ID that will be created for you. Snakemake will store its logfiles for this very Snakemake run in this folder. The next new thing is set -x . This simply prints to the terminal every command that is executed within the script. This is useful for debugging. Finally, the Snakemake call takes place. With the --drmaa option we define how the jobs inside Snakemake should be started on the nodes. Please note that the sbatch command does not appear here, but the argument string provided to the --drmaa option is the same as the parameters for sbatch , except for some minor differences as descrived above or here . The drmaa library provides more stable job distribution than using plain sbatch in this scenario. Note that we don't write the parameter string to a bash file (which we don't have in this case). There are three new parameters that are defining the hardware requirements for our run and the project: --mem-per-cpu=X : How much memory ONE job (=Snakemake rule) should get in Megabyte -t HH:MM : How long a job (=Snakemake rule) is allowed to run -n X : How many cores of a node a job (=Snakemake rule) should get --nodes=1 : Force allocation of all cores on a single node. -p medium : The partition name (this system was introduced in Episode 2 and is described here ) Note that the memory is not shared among the cores. This means the final memory on a node is defined by <mem-per-cpu> * <n> So in our example the total memory requested on one node would be 8GB. The other parameters provided to Snakemake are: -j : Use 2 cores for this Snakemake run. In this scenario the parameter determines how many jobs will be submitted to SLURM at a time. -k : Keep going if a job fails -p : Print out shell commands Finally, run the script: (first-steps) $ sbatch submit_snakejob.sh If you watch squeue --me now, you will see that the jobs are distributed to the system: (first-steps) $ watch -n 60 squeue --me Please refer to the Snakemake documentation for more details on using Snakemake, in particular how to use the cluster configuration on how to specify the resource requirements on a per-rule base.","title":"First Steps: Episode 4"},{"location":"help/faq/","text":"Frequently Asked Questions \u00b6 What is this Website? \u00b6 This is the BIH cluster documentation that was created and is maintained by BIH Core Unit Bioinformatics (CUBI) and BIH HPC IT with contributions by BIH HPC Users. The aim is to gather the information for using the cluster efficiently and helping common issues andproblems. Where can I get help? \u00b6 First, contact bih-cluster@charite.de with your question. Here, administrators, CUBI, and other users alike are subscribed and can answer your question. For problems while connecting and logging in, please contact helpdesk@mdc-berlin.de or helpdesk@charite.de . For problems with BIH HPC please contact [ hpc-helpdesk@bihealth.de ]. I cannot connect to the cluster. What's wrong? \u00b6 Please see the section Connection Problems . I'd like to learn more about Slurm \u00b6 Some documentation is available on this website, e.g., start at Slurm Quickstart . What is the difference between MAX and BIH cluster? What is their relation? \u00b6 Administrativa The BIH cluster is the cluster of the Berlin Institute of Health (BIH) and is located in Buch and operated by MDC IT. MDC IT performs the administration for the BIH and the BIH cluster uses the authentication (user/password/groups) infrastructure of the MDC. Thus you have to get an MDC account for using the BIH cluster. Systems support can be requested from helpdesk@mdc-berlin.de as for the MAX cluster. The MAX cluster is the cluster of the Max Delbrueck Center (MDC) in Buch. This cluster is used by the researchers at MDC and integrates a lot of infrastructure of the MDC. Request for both systems are handled separately, depending on the user's affiliation with research/service groups. Hardware and Systems Both clusters consist of similar hardware for the compute nodes and both feature a DDN system at different number of nodes and different storage volume. Both clusters run CentOS but at potentially different version. BIH HPC uses the Slurm workload manager whereas MAX uses Univa Grid Engine. The BIH cluster has a significantly faster internal network (40GB/s optical). Bioinformatics Software On the BIH cluster, users can install their own bioinformatics software in their user directory. On the MAX cluster, users can also install their own software or use software provided by Altuna Akalin's group at MDC . My SSH sessions break with \" packet_write_wait: Connection to XXX : Broken pipe \". How can I fix this? \u00b6 Try to put the following line at the top of your ~/.ssh/config . ServerAliveInterval 30 This will make ssh send an empty network package to the server. This will prevent network hardware from thinking your connection is unused/broken and terminating it. If the problem persists, please report it to hpc-helpdesk@bihealth.de . My job terminated before being done. What happened? \u00b6 First of all, look into your job logs. In the case that the job was terminated by Slurm (e.g., because it ran too long), you will find a message at the bottom. Otherwise, you can use sacct -j JOBID to read the information that the job accounting system has recorded for your job. Use the --long flag to see all fields (and probably pipe it into less as: sacct -j JOBID --long | less -S ). Things to look out for: What is the exit code? Is the highest recorded memory usage too high/higher than expected (field MaxRSS )? Is the running time too long/longer than expected (field Elapsed )? How can I create a new project? \u00b6 You can create a project if you are either a group leader of an AG or a delegate of an AG. In this case, please send an email to hpc-admin@bihealth.de and request a new project. I have a problem with my SSH key, help! \u00b6 Please contact MDC Helpdesk for help. CUBI is also only a user on the BIH cluster and has no access to the user, password, or SSH keys registries. I have a problem with logging into the cluster, help! \u00b6 See \"I have a problem with my SSH key, help!\" I cannot create PNGs in R \u00b6 For using the png method, you need to have an X11 session running. This might be the case if you logged into a cluster node using srun --x11 if configured correctly but is not the case if you submitted a bash job. The solution is to use xvfb-run (xvfb = X11 virtual frame-buffer). Here is the content of an example script: $ cat img.R #!/usr/bin/env Rscript png('cars.png') cars <- c(1, 3, 6, 4, 9) plot(cars) dev.off() Here, it fails without X11: $ ./img.R Error in .External2(C_X11, paste(\"png::\", filename, sep = \"\"), g$width, : unable to start device PNG Calls: png In addition: Warning message: In png(\"cars.png\") : unable to open connection to X11 display '' Execution halted Here, it works with xvfb-run : $ xvfb-run ./img.R null device 1 $ ls cars.png foo.png img.R Rplots.pdf My jobs don't get scheduled \u00b6 You can use scontrol show job JOBID to get the details displayed about your jobs. In the example below, we can see that the job is in the PENDING state. The Reason field tells us that the job did not scheduled because the specified dependency was neverfulfilled. You can find a list of all job reason codes in the Slurm squeue documentation . JobId = 863089 JobName = pipeline_job.sh UserId = holtgrem_c ( 100131 ) GroupId = hpc-ag-cubi ( 5272 ) MCS_label = N/A Priority = 1 Nice = 0 Account =( null ) QOS = normal JobState = PENDING Reason = DependencyNeverSatisfied Dependency = afterok:863087 ( failed ) Requeue = 1 Restarts = 0 BatchFlag = 1 Reboot = 0 ExitCode = 0 :0 RunTime = 00 :00:00 TimeLimit = 08 :00:00 TimeMin = N/A SubmitTime = 2020 -05-03T18:57:34 EligibleTime = Unknown AccrueTime = Unknown StartTime = Unknown EndTime = Unknown Deadline = N/A SuspendTime = None SecsPreSuspend = 0 LastSchedEval = 2020 -05-03T18:57:34 Partition = debug AllocNode:Sid = med-login1:28797 ReqNodeList =( null ) ExcNodeList =( null ) NodeList =( null ) NumNodes = 1 NumCPUs = 1 NumTasks = 1 CPUs/Task = 1 ReqB:S:C:T = 0 :0:*:* TRES = cpu = 1 ,node = 1 ,billing = 1 Socks/Node = * NtasksPerN:B:S:C = 0 :0:*:* CoreSpec = * MinCPUsNode = 1 MinMemoryNode = 0 MinTmpDiskNode = 0 Features =( null ) DelayBoot = 00 :00:00 OverSubscribe = OK Contiguous = 0 Licenses =( null ) Network =( null ) Command = /fast/work/projects/medgen_genomes/2019-06-05_genomes_reboot/GRCh37/wgs_cnv_export/pipeline_job.sh WorkDir = /fast/work/projects/medgen_genomes/2019-06-05_genomes_reboot/GRCh37/wgs_cnv_export StdErr = /fast/work/projects/medgen_genomes/2019-06-05_genomes_reboot/GRCh37/wgs_cnv_export/slurm-863089.out StdIn = /dev/null StdOut = /fast/work/projects/medgen_genomes/2019-06-05_genomes_reboot/GRCh37/wgs_cnv_export/slurm-863089.out Power = MailUser =( null ) MailType = NONE My jobs don't run in the partition I expect \u00b6 You can see the partition that your job runs in with squeue -j JOBID : med-login1:~$ squeue -j 877092 JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 877092 medium snakejob holtgrem R 0 :05 1 med0626 See Job Scheduler for information about the partition's properties. To get your job to run in the medium partition, for example use the --partition=medium or -p medium arguments to your srun or sbatch commands. My jobs get killed after four hours \u00b6 This is probably answered by the answer to My jobs don't run in the partition I expect . How can I mount a network volume from elsewhere on the cluster? \u00b6 You cannot. Also see the For the Impatient section of the manual. Why can I not mount a network volume from elsewhere on the cluster? \u00b6 For performance and stability reasons. Network volumes are notorious for degrading performance, depending on the used protocol, even stability. How can I then access the files from my workstation/server? \u00b6 You can transfer files to the cluster through Rsync over SSH or through SFTP to the med-transfer1 or med-transfer2 node. Do not transfer files through the login nodes. Large file transfers through the login nodes can cause performance degradation for the users with interactive SSH connections. How can I circumvent \"invalid instruction\" (signal 4) errors? \u00b6 Make sure that software is compiled with \"sandy bridge\" optimizations and no later one. E.g., use the -march=sandybridge argument to the GCC/LLVM compiler executables. If you absolutely need it, there are some boxes with more recent processors in the cluster (e.g., Haswell architecture). Look at the /proc/cpuinfo files for details. Where should my (Mini)conda install go? \u00b6 As conda installations are big and contain many files, they should go into your work directory. E.g., /fast/users/$USER/work/miniconda is appropriate. I have problems connecting to the GPU node! What's wrong? \u00b6 Please check whether there might be other jobs waiting in front of you! The following squeue call will show the allocated GPUs of jobs in the gpu queue. This is done by specifying a format string and using the %b field. squeue -o \"%.10i %9P %20j %10u %.2t %.10M %.6D %10R %b\" -p gpu JOBID PARTITION NAME USER ST TIME NODES NODELIST ( R TRES_PER_NODE 872571 gpu bash user1 R 15 :53:25 1 med0303 gpu:tesla:1 862261 gpu bash user2 R 2 -16:26:59 1 med0304 gpu:tesla:4 860771 gpu kidney.job user3 R 2 -16:27:12 1 med0302 gpu:tesla:1 860772 gpu kidney.job user3 R 2 -16:27:12 1 med0302 gpu:tesla:1 860773 gpu kidney.job user3 R 2 -16:27:12 1 med0302 gpu:tesla:1 860770 gpu kidney.job user3 R 4 -03:23:08 1 med0301 gpu:tesla:1 860766 gpu kidney.job user3 R 4 -03:23:11 1 med0303 gpu:tesla:1 860767 gpu kidney.job user3 R 4 -03:23:11 1 med0301 gpu:tesla:1 860768 gpu kidney.job user3 R 4 -03:23:11 1 med0301 gpu:tesla:1 In the example above, user1 has one job with one GPU running on med0303, user2 has one job running with 4 GPUs on med0304 and user3 has 7 jobs in total running of different machines with one GPU each. How can I access graphical user interfaces (such as for Matlab) on the cluster? \u00b6 First of all, you will need an X(11) server on your local machine (see Wikipedia: X Window System . This server offers a \"graphical surface\" that the programs on the cluster can then paint on. You need to make sure that the programs running on the cluster can access this graphical surface. Generally, you need to connect to the login nodes with X forwarding. Refer to the manual of your SSH client on how to do this ( -X for Linux/Mac ssh As you should not run compute-intensive programs on the login node, connect to a cluster node with X forwarding. With Slurm, this is done using srun --pty --x11 bash -i (instead of srun --pty --x11 bash -i ). Also see: Running graphical(X11) applications on Windows Running graphical(X11) applications on Linux How can I log into a node outside of the scheduler? \u00b6 This is sometimes useful, e.g., for monitoring the CPU/GPU usage of your job interactively. No Computation Outside of Slurm Do not perform any computation outside of the scheduler as (1) this breaks the purpose of the scheduling system and (2) administration is not aware and might kill you jobs. The answer is simple, just SSH into this node. med-login1:~$ ssh med0XXX Snakemake DRMAA doesn't accept my Slurm parameters!? \u00b6 Yes. Unfortunately, Slurm DRMAA differs slightly from the original Slurm syntax. Why am I getting multiple nodes to my job? \u00b6 Classically, jobs on HPC systems are written in a way that they can run on multiple nodes at once, using the network to communicate. Slurm comes from this world and when allocating more than one CPU/core, it might allocate them on different nodes. Please use --nodes=1 to force Slurm to allocate them on a single node. How can I select a certain CPU architecture? \u00b6 You can select the CPU architecture by using the -C / --constraint flag to sbatch and srun . The following are available (as detected by the Linux kernel): ivybridge (96 nodes, plus 4 high-memory nodes) haswell (16 nodes) broadwell (112 nodes) skylake (16 nodes, plus 4 GPU nodes) You can specify contraints with OR such as --constraint=haswell|broadwell|skylake . You can see the assignment of architectures to nodes using the sinfo -o \"%8P %.5a %.10l %.6D %.6t %10f %N\" command. This will also display node partition, availability etc.","title":"Frequently Asked Questions"},{"location":"help/faq/#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"help/faq/#what-is-this-website","text":"This is the BIH cluster documentation that was created and is maintained by BIH Core Unit Bioinformatics (CUBI) and BIH HPC IT with contributions by BIH HPC Users. The aim is to gather the information for using the cluster efficiently and helping common issues andproblems.","title":"What is this Website?"},{"location":"help/faq/#where-can-i-get-help","text":"First, contact bih-cluster@charite.de with your question. Here, administrators, CUBI, and other users alike are subscribed and can answer your question. For problems while connecting and logging in, please contact helpdesk@mdc-berlin.de or helpdesk@charite.de . For problems with BIH HPC please contact [ hpc-helpdesk@bihealth.de ].","title":"Where can I get help?"},{"location":"help/faq/#i-cannot-connect-to-the-cluster-whats-wrong","text":"Please see the section Connection Problems .","title":"I cannot connect to the cluster. What's wrong?"},{"location":"help/faq/#id-like-to-learn-more-about-slurm","text":"Some documentation is available on this website, e.g., start at Slurm Quickstart .","title":"I'd like to learn more about Slurm"},{"location":"help/faq/#what-is-the-difference-between-max-and-bih-cluster-what-is-their-relation","text":"Administrativa The BIH cluster is the cluster of the Berlin Institute of Health (BIH) and is located in Buch and operated by MDC IT. MDC IT performs the administration for the BIH and the BIH cluster uses the authentication (user/password/groups) infrastructure of the MDC. Thus you have to get an MDC account for using the BIH cluster. Systems support can be requested from helpdesk@mdc-berlin.de as for the MAX cluster. The MAX cluster is the cluster of the Max Delbrueck Center (MDC) in Buch. This cluster is used by the researchers at MDC and integrates a lot of infrastructure of the MDC. Request for both systems are handled separately, depending on the user's affiliation with research/service groups. Hardware and Systems Both clusters consist of similar hardware for the compute nodes and both feature a DDN system at different number of nodes and different storage volume. Both clusters run CentOS but at potentially different version. BIH HPC uses the Slurm workload manager whereas MAX uses Univa Grid Engine. The BIH cluster has a significantly faster internal network (40GB/s optical). Bioinformatics Software On the BIH cluster, users can install their own bioinformatics software in their user directory. On the MAX cluster, users can also install their own software or use software provided by Altuna Akalin's group at MDC .","title":"What is the difference between MAX and BIH cluster? What is their relation?"},{"location":"help/faq/#my-ssh-sessions-break-with-packet_write_wait-connection-to-xxx-broken-pipe-how-can-i-fix-this","text":"Try to put the following line at the top of your ~/.ssh/config . ServerAliveInterval 30 This will make ssh send an empty network package to the server. This will prevent network hardware from thinking your connection is unused/broken and terminating it. If the problem persists, please report it to hpc-helpdesk@bihealth.de .","title":"My SSH sessions break with \"packet_write_wait: Connection to XXX : Broken pipe\". How can I fix this?"},{"location":"help/faq/#my-job-terminated-before-being-done-what-happened","text":"First of all, look into your job logs. In the case that the job was terminated by Slurm (e.g., because it ran too long), you will find a message at the bottom. Otherwise, you can use sacct -j JOBID to read the information that the job accounting system has recorded for your job. Use the --long flag to see all fields (and probably pipe it into less as: sacct -j JOBID --long | less -S ). Things to look out for: What is the exit code? Is the highest recorded memory usage too high/higher than expected (field MaxRSS )? Is the running time too long/longer than expected (field Elapsed )?","title":"My job terminated before being done. What happened?"},{"location":"help/faq/#how-can-i-create-a-new-project","text":"You can create a project if you are either a group leader of an AG or a delegate of an AG. In this case, please send an email to hpc-admin@bihealth.de and request a new project.","title":"How can I create a new project?"},{"location":"help/faq/#i-have-a-problem-with-my-ssh-key-help","text":"Please contact MDC Helpdesk for help. CUBI is also only a user on the BIH cluster and has no access to the user, password, or SSH keys registries.","title":"I have a problem with my SSH key, help!"},{"location":"help/faq/#i-have-a-problem-with-logging-into-the-cluster-help","text":"See \"I have a problem with my SSH key, help!\"","title":"I have a problem with logging into the cluster, help!"},{"location":"help/faq/#i-cannot-create-pngs-in-r","text":"For using the png method, you need to have an X11 session running. This might be the case if you logged into a cluster node using srun --x11 if configured correctly but is not the case if you submitted a bash job. The solution is to use xvfb-run (xvfb = X11 virtual frame-buffer). Here is the content of an example script: $ cat img.R #!/usr/bin/env Rscript png('cars.png') cars <- c(1, 3, 6, 4, 9) plot(cars) dev.off() Here, it fails without X11: $ ./img.R Error in .External2(C_X11, paste(\"png::\", filename, sep = \"\"), g$width, : unable to start device PNG Calls: png In addition: Warning message: In png(\"cars.png\") : unable to open connection to X11 display '' Execution halted Here, it works with xvfb-run : $ xvfb-run ./img.R null device 1 $ ls cars.png foo.png img.R Rplots.pdf","title":"I cannot create PNGs in R"},{"location":"help/faq/#my-jobs-dont-get-scheduled","text":"You can use scontrol show job JOBID to get the details displayed about your jobs. In the example below, we can see that the job is in the PENDING state. The Reason field tells us that the job did not scheduled because the specified dependency was neverfulfilled. You can find a list of all job reason codes in the Slurm squeue documentation . JobId = 863089 JobName = pipeline_job.sh UserId = holtgrem_c ( 100131 ) GroupId = hpc-ag-cubi ( 5272 ) MCS_label = N/A Priority = 1 Nice = 0 Account =( null ) QOS = normal JobState = PENDING Reason = DependencyNeverSatisfied Dependency = afterok:863087 ( failed ) Requeue = 1 Restarts = 0 BatchFlag = 1 Reboot = 0 ExitCode = 0 :0 RunTime = 00 :00:00 TimeLimit = 08 :00:00 TimeMin = N/A SubmitTime = 2020 -05-03T18:57:34 EligibleTime = Unknown AccrueTime = Unknown StartTime = Unknown EndTime = Unknown Deadline = N/A SuspendTime = None SecsPreSuspend = 0 LastSchedEval = 2020 -05-03T18:57:34 Partition = debug AllocNode:Sid = med-login1:28797 ReqNodeList =( null ) ExcNodeList =( null ) NodeList =( null ) NumNodes = 1 NumCPUs = 1 NumTasks = 1 CPUs/Task = 1 ReqB:S:C:T = 0 :0:*:* TRES = cpu = 1 ,node = 1 ,billing = 1 Socks/Node = * NtasksPerN:B:S:C = 0 :0:*:* CoreSpec = * MinCPUsNode = 1 MinMemoryNode = 0 MinTmpDiskNode = 0 Features =( null ) DelayBoot = 00 :00:00 OverSubscribe = OK Contiguous = 0 Licenses =( null ) Network =( null ) Command = /fast/work/projects/medgen_genomes/2019-06-05_genomes_reboot/GRCh37/wgs_cnv_export/pipeline_job.sh WorkDir = /fast/work/projects/medgen_genomes/2019-06-05_genomes_reboot/GRCh37/wgs_cnv_export StdErr = /fast/work/projects/medgen_genomes/2019-06-05_genomes_reboot/GRCh37/wgs_cnv_export/slurm-863089.out StdIn = /dev/null StdOut = /fast/work/projects/medgen_genomes/2019-06-05_genomes_reboot/GRCh37/wgs_cnv_export/slurm-863089.out Power = MailUser =( null ) MailType = NONE","title":"My jobs don't get scheduled"},{"location":"help/faq/#my-jobs-dont-run-in-the-partition-i-expect","text":"You can see the partition that your job runs in with squeue -j JOBID : med-login1:~$ squeue -j 877092 JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 877092 medium snakejob holtgrem R 0 :05 1 med0626 See Job Scheduler for information about the partition's properties. To get your job to run in the medium partition, for example use the --partition=medium or -p medium arguments to your srun or sbatch commands.","title":"My jobs don't run in the partition I expect"},{"location":"help/faq/#my-jobs-get-killed-after-four-hours","text":"This is probably answered by the answer to My jobs don't run in the partition I expect .","title":"My jobs get killed after four hours"},{"location":"help/faq/#how-can-i-mount-a-network-volume-from-elsewhere-on-the-cluster","text":"You cannot. Also see the For the Impatient section of the manual.","title":"How can I mount a network volume from elsewhere on the cluster?"},{"location":"help/faq/#why-can-i-not-mount-a-network-volume-from-elsewhere-on-the-cluster","text":"For performance and stability reasons. Network volumes are notorious for degrading performance, depending on the used protocol, even stability.","title":"Why can I not mount a network volume from elsewhere on the cluster?"},{"location":"help/faq/#how-can-i-then-access-the-files-from-my-workstationserver","text":"You can transfer files to the cluster through Rsync over SSH or through SFTP to the med-transfer1 or med-transfer2 node. Do not transfer files through the login nodes. Large file transfers through the login nodes can cause performance degradation for the users with interactive SSH connections.","title":"How can I then access the files from my workstation/server?"},{"location":"help/faq/#how-can-i-circumvent-invalid-instruction-signal-4-errors","text":"Make sure that software is compiled with \"sandy bridge\" optimizations and no later one. E.g., use the -march=sandybridge argument to the GCC/LLVM compiler executables. If you absolutely need it, there are some boxes with more recent processors in the cluster (e.g., Haswell architecture). Look at the /proc/cpuinfo files for details.","title":"How can I circumvent \"invalid instruction\" (signal 4) errors?"},{"location":"help/faq/#where-should-my-miniconda-install-go","text":"As conda installations are big and contain many files, they should go into your work directory. E.g., /fast/users/$USER/work/miniconda is appropriate.","title":"Where should my (Mini)conda install go?"},{"location":"help/faq/#i-have-problems-connecting-to-the-gpu-node-whats-wrong","text":"Please check whether there might be other jobs waiting in front of you! The following squeue call will show the allocated GPUs of jobs in the gpu queue. This is done by specifying a format string and using the %b field. squeue -o \"%.10i %9P %20j %10u %.2t %.10M %.6D %10R %b\" -p gpu JOBID PARTITION NAME USER ST TIME NODES NODELIST ( R TRES_PER_NODE 872571 gpu bash user1 R 15 :53:25 1 med0303 gpu:tesla:1 862261 gpu bash user2 R 2 -16:26:59 1 med0304 gpu:tesla:4 860771 gpu kidney.job user3 R 2 -16:27:12 1 med0302 gpu:tesla:1 860772 gpu kidney.job user3 R 2 -16:27:12 1 med0302 gpu:tesla:1 860773 gpu kidney.job user3 R 2 -16:27:12 1 med0302 gpu:tesla:1 860770 gpu kidney.job user3 R 4 -03:23:08 1 med0301 gpu:tesla:1 860766 gpu kidney.job user3 R 4 -03:23:11 1 med0303 gpu:tesla:1 860767 gpu kidney.job user3 R 4 -03:23:11 1 med0301 gpu:tesla:1 860768 gpu kidney.job user3 R 4 -03:23:11 1 med0301 gpu:tesla:1 In the example above, user1 has one job with one GPU running on med0303, user2 has one job running with 4 GPUs on med0304 and user3 has 7 jobs in total running of different machines with one GPU each.","title":"I have problems connecting to the GPU node! What's wrong?"},{"location":"help/faq/#how-can-i-access-graphical-user-interfaces-such-as-for-matlab-on-the-cluster","text":"First of all, you will need an X(11) server on your local machine (see Wikipedia: X Window System . This server offers a \"graphical surface\" that the programs on the cluster can then paint on. You need to make sure that the programs running on the cluster can access this graphical surface. Generally, you need to connect to the login nodes with X forwarding. Refer to the manual of your SSH client on how to do this ( -X for Linux/Mac ssh As you should not run compute-intensive programs on the login node, connect to a cluster node with X forwarding. With Slurm, this is done using srun --pty --x11 bash -i (instead of srun --pty --x11 bash -i ). Also see: Running graphical(X11) applications on Windows Running graphical(X11) applications on Linux","title":"How can I access graphical user interfaces (such as for Matlab) on the cluster?"},{"location":"help/faq/#how-can-i-log-into-a-node-outside-of-the-scheduler","text":"This is sometimes useful, e.g., for monitoring the CPU/GPU usage of your job interactively. No Computation Outside of Slurm Do not perform any computation outside of the scheduler as (1) this breaks the purpose of the scheduling system and (2) administration is not aware and might kill you jobs. The answer is simple, just SSH into this node. med-login1:~$ ssh med0XXX","title":"How can I log into a node outside of the scheduler?"},{"location":"help/faq/#snakemake-drmaa-doesnt-accept-my-slurm-parameters","text":"Yes. Unfortunately, Slurm DRMAA differs slightly from the original Slurm syntax.","title":"Snakemake DRMAA doesn't accept my Slurm parameters!?"},{"location":"help/faq/#why-am-i-getting-multiple-nodes-to-my-job","text":"Classically, jobs on HPC systems are written in a way that they can run on multiple nodes at once, using the network to communicate. Slurm comes from this world and when allocating more than one CPU/core, it might allocate them on different nodes. Please use --nodes=1 to force Slurm to allocate them on a single node.","title":"Why am I getting multiple nodes to my job?"},{"location":"help/faq/#how-can-i-select-a-certain-cpu-architecture","text":"You can select the CPU architecture by using the -C / --constraint flag to sbatch and srun . The following are available (as detected by the Linux kernel): ivybridge (96 nodes, plus 4 high-memory nodes) haswell (16 nodes) broadwell (112 nodes) skylake (16 nodes, plus 4 GPU nodes) You can specify contraints with OR such as --constraint=haswell|broadwell|skylake . You can see the assignment of architectures to nodes using the sinfo -o \"%8P %.5a %.10l %.6D %.6t %10f %N\" command. This will also display node partition, availability etc.","title":"How can I select a certain CPU architecture?"},{"location":"help/good-tickets/","text":"How-To: Write a Good Ticket \u00b6 This page describes how to write a good help request ticket. Write a descriptive summary. Put in a short summary into the Subject. Expand on this in a first paragraph. Try to answer the following questions: What are you trying to achieve? When did the problem start? Did it work before? Which steps did you attempt to achieve this? Give us your basic information. Please give us your user name on the cluster. Put enough details in the details section. Please give us the exact commands you type into your console. What are the symptoms/is the error message Never put your password into the ticket. In the case that you handle person-related data of patients/study participants, never write any of this information into the ticket or sequent email. Please do not send us screenshot images of what you did but copy and paste the text instead. There is more specific questions for common issues given below. Problems Connecting to the Cluster \u00b6 From which machine/IP do you try to connect ( ifconfig on Linux/Mac, ipconfig on Windows)? Did it work before? What is your user name? Please send us the output of ssh-add -l and add -vvv to the SSH command that fails for you. What is the response of the server? Problems Submitting Jobs \u00b6 Please give us the directory that you run things in. Please send us the submission script that you have problems with. If the job was submitted, Slurm will give you a job ID. We will need this ID. Please send us the output of scontrol show job <jobid> or sacct --long -j <jobid> of your job.","title":"Good Tickets"},{"location":"help/good-tickets/#how-to-write-a-good-ticket","text":"This page describes how to write a good help request ticket. Write a descriptive summary. Put in a short summary into the Subject. Expand on this in a first paragraph. Try to answer the following questions: What are you trying to achieve? When did the problem start? Did it work before? Which steps did you attempt to achieve this? Give us your basic information. Please give us your user name on the cluster. Put enough details in the details section. Please give us the exact commands you type into your console. What are the symptoms/is the error message Never put your password into the ticket. In the case that you handle person-related data of patients/study participants, never write any of this information into the ticket or sequent email. Please do not send us screenshot images of what you did but copy and paste the text instead. There is more specific questions for common issues given below.","title":"How-To: Write a Good Ticket"},{"location":"help/good-tickets/#problems-connecting-to-the-cluster","text":"From which machine/IP do you try to connect ( ifconfig on Linux/Mac, ipconfig on Windows)? Did it work before? What is your user name? Please send us the output of ssh-add -l and add -vvv to the SSH command that fails for you. What is the response of the server?","title":"Problems Connecting to the Cluster"},{"location":"help/good-tickets/#problems-submitting-jobs","text":"Please give us the directory that you run things in. Please send us the submission script that you have problems with. If the job was submitted, Slurm will give you a job ID. We will need this ID. Please send us the output of scontrol show job <jobid> or sacct --long -j <jobid> of your job.","title":"Problems Submitting Jobs"},{"location":"help/helpdesk/","text":"HPC IT Helpdesk \u00b6 Getting Help Our helpdesk can be reached via email to hpc-helpdesk@bihealth.de . Please read our guide on how to write good tickets first. Note that there also is the bih-cluster@charite.de email address that you can user for user discussions and getting support from other users (in particular for things out of scope for the helpdesk). Helpdesk Scope \u00b6 Our helpdesk can support you in the following areas: Problems/questions with connecting to the clusters. Problems/questions with using the cluster scheduler or operating system. Requests for the installation of common software. Problems with running your software that works in other environments. We will try our best to resolve these issues. Please note that all other questions can only be answered in a \"best effort way\". Helpdesk Non-Scope \u00b6 The following topics are out of scope for the BIH HPC Helpdesk: Generic Linux or programming questions (try stackoverflow.com ). Managing users, groups, and projects on the clusters (use hpc-gatekeeper@bihealth.de ). Generic help with Snakemake or other workflow engines (See Stackoverflow for getting help with Snakemake). Help with bioinformatics or other scientific software. Please contact the authors/communities of these software for help (also known as \"upstream\"). We're happy to see if we can help when there is a concrete problem with the software, e.g., something that breaks from one week to another without you changing anything and you assume a change on the cluster, or you need a generic dependency that you cannot install via conda or on your own.","title":"HPC Helpdesk"},{"location":"help/helpdesk/#hpc-it-helpdesk","text":"Getting Help Our helpdesk can be reached via email to hpc-helpdesk@bihealth.de . Please read our guide on how to write good tickets first. Note that there also is the bih-cluster@charite.de email address that you can user for user discussions and getting support from other users (in particular for things out of scope for the helpdesk).","title":"HPC IT Helpdesk"},{"location":"help/helpdesk/#helpdesk-scope","text":"Our helpdesk can support you in the following areas: Problems/questions with connecting to the clusters. Problems/questions with using the cluster scheduler or operating system. Requests for the installation of common software. Problems with running your software that works in other environments. We will try our best to resolve these issues. Please note that all other questions can only be answered in a \"best effort way\".","title":"Helpdesk Scope"},{"location":"help/helpdesk/#helpdesk-non-scope","text":"The following topics are out of scope for the BIH HPC Helpdesk: Generic Linux or programming questions (try stackoverflow.com ). Managing users, groups, and projects on the clusters (use hpc-gatekeeper@bihealth.de ). Generic help with Snakemake or other workflow engines (See Stackoverflow for getting help with Snakemake). Help with bioinformatics or other scientific software. Please contact the authors/communities of these software for help (also known as \"upstream\"). We're happy to see if we can help when there is a concrete problem with the software, e.g., something that breaks from one week to another without you changing anything and you assume a change on the cluster, or you need a generic dependency that you cannot install via conda or on your own.","title":"Helpdesk Non-Scope"},{"location":"how-to/connect/gpu-nodes/","text":"How-To: Connect to GPU Nodes \u00b6 The cluster has four nodes with four Tesla V100 GPUs each: med030[1-4] . Connecting to a node with GPUs is easy. You simply request a GPU using the -p gpu --gres=gpu:tesla:COUNT argument to srun and batch . This will place your job in the gpu partition (which is where the GPU nodes live) and allocate a number of COUNT GPUs to your job. Hint Make sure to read the FAQ entry \" I have problems connecting to the GPU node! What's wrong? \". Please Limit Interactive GPU Usage While interactive computation on the GPU nodes is convenient, it makes it very easy to forget a job after your computation is complete and let it run idle. While your job is allocated, it blocks the allocated GPUs and other users cannot use them although you might not be actually using them. Please prefer batch jobs for your GPU jobs over interactive jobs. Administration monitors the ratio of idling jobs on the and will restrict interactive GPU usage in the future if idling interactive jobs become a problem. We might limit interactive GPU usage to very short time spans for testing (say 10 minutes or less) or block interactive GPU usage alltogether. Please do not give a reason to do so. Prequisites \u00b6 You have to register with hpc-gatekeeper@bihealth.de for requesting access. Afterwards, you can connect to the GPU nodes as shown below. Preparation \u00b6 We will setup a miniconda installation with pytorch testing the GPU. If you already have this setup then you can skip this step med-login1:~$ srun --pty bash med0703:~$ wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh med0703:~$ bash Miniconda3-latest-Linux-x86_64.sh -b -p ~/miniconda3 med0703:~$ source ~/miniconda3/bin/activate med0703:~$ conda create -y -n gpu-test python = 3 pytorch med0703:~$ conda activate gpu-test med0703:~$ python -c 'import torch; print(torch.cuda.is_available())' False med0703:~$ exit med-login1:~$ The False shows that CUDA is not available on the node but that is to be expected. We're only warming up! Allocating GPUs \u00b6 Let us now allocate a GPU. The Slurm schedule will properly allocate GPUs for you and setup the environment variable that tell CUDA which devices are available. The following dry run shows these environment variables (and that they are not available on the login node). med-login1:~$ export | grep CUDA_VISIBLE_DEVICES med-login1:~$ srun -p gpu --gres = gpu:tesla:1 --pty bash med0303:~$ export | grep CUDA_VISIBLE_DEVICES declare -x CUDA_VISIBLE_DEVICES = \"0\" med0303:~$ exit med-login1:~$ srun -p gpu --gres = gpu:tesla:2 --pty bash med0303:~$ export | grep CUDA_VISIBLE_DEVICES declare -x CUDA_VISIBLE_DEVICES = \"0,1\" You can see that you can also reserve multiple GPUs. If we were to open two concurrent connections (e.g., in a screen ) to the same node when allocating one GPU each, the allocated GPUs would be non-overlapping. Note that any two jobs are isolated using Linux cgroups (\"container\" technology \ud83d\ude80) so you cannot accidentally use a GPU of another job. Now to the somewhat boring part where we show that CUDA actually works. med-login1:~$ srun -p gpu --gres = gpu:tesla:1 --pty bash med0301:~$ nvcc --version nvcc: NVIDIA ( R ) Cuda compiler driver Copyright ( c ) 2005 -2019 NVIDIA Corporation Built on Wed_Oct_23_19:24:38_PDT_2019 Cuda compilation tools, release 10 .2, V10.2.89 med0301:~$ source ~/miniconda3/bin/activate med0301:~$ conda activate gpu-test med0301:~$ python -c 'import torch; print(torch.cuda.is_available())' True Bonus #1 : Who is using the GPUs? \u00b6 Use squeue to find out about currently queued jobs (the egrep only keeps the header and entries in the gpu partition). med-login1:~$ squeue | egrep -iw 'JOBID|gpu' JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 33 gpu bash holtgrem R 2 :26 1 med0301 Bonus #2 : Is the GPU running? \u00b6 To find out how active the GPU nodes actually are, you can connect to the nodes (without allocating a GPU you can do this even if the node is full) and then use nvidia-smi . med-login1:~$ ssh med0301 bash med0301:~$ nvidia-smi Fri Mar 6 11 :10:08 2020 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 440 .33.01 Driver Version: 440 .33.01 CUDA Version: 10 .2 | | -------------------------------+----------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | =============================== + ====================== + ====================== | | 0 Tesla V100-SXM2... Off | 00000000 :18:00.0 Off | 0 | | N/A 62C P0 246W / 300W | 16604MiB / 32510MiB | 99 % Default | +-------------------------------+----------------------+----------------------+ | 1 Tesla V100-SXM2... Off | 00000000 :3B:00.0 Off | 0 | | N/A 61C P0 270W / 300W | 16604MiB / 32510MiB | 100 % Default | +-------------------------------+----------------------+----------------------+ | 2 Tesla V100-SXM2... Off | 00000000 :86:00.0 Off | 0 | | N/A 39C P0 55W / 300W | 0MiB / 32510MiB | 0 % Default | +-------------------------------+----------------------+----------------------+ | 3 Tesla V100-SXM2... Off | 00000000 :AF:00.0 Off | 0 | | N/A 44C P0 60W / 300W | 0MiB / 32510MiB | 4 % Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | | ============================================================================= | | 0 43461 C python 16593MiB | | 1 43373 C python 16593MiB | +-----------------------------------------------------------------------------+ Fair Share / Fair Use \u00b6 Note that allocating a GPU makes it unavailble for everyone else. There is currently no restriction in place on the GPU nodes, so please behave nicely and cooperatively. If you see someone blocking the GPU nodes for long time, then first find out who it is. You can type getent passwd USER_NAME on any cluster node to see the email address (and work phone number if added) of the user. Send a friendly email to the other user, most likely they blocked the node accidentally. If you cannot resolve the issue (e.g., the user is not reachable) then please contact hpc-helpdesk@bihealth.de with your issue.","title":"GPU Nodes"},{"location":"how-to/connect/gpu-nodes/#how-to-connect-to-gpu-nodes","text":"The cluster has four nodes with four Tesla V100 GPUs each: med030[1-4] . Connecting to a node with GPUs is easy. You simply request a GPU using the -p gpu --gres=gpu:tesla:COUNT argument to srun and batch . This will place your job in the gpu partition (which is where the GPU nodes live) and allocate a number of COUNT GPUs to your job. Hint Make sure to read the FAQ entry \" I have problems connecting to the GPU node! What's wrong? \". Please Limit Interactive GPU Usage While interactive computation on the GPU nodes is convenient, it makes it very easy to forget a job after your computation is complete and let it run idle. While your job is allocated, it blocks the allocated GPUs and other users cannot use them although you might not be actually using them. Please prefer batch jobs for your GPU jobs over interactive jobs. Administration monitors the ratio of idling jobs on the and will restrict interactive GPU usage in the future if idling interactive jobs become a problem. We might limit interactive GPU usage to very short time spans for testing (say 10 minutes or less) or block interactive GPU usage alltogether. Please do not give a reason to do so.","title":"How-To: Connect to GPU Nodes"},{"location":"how-to/connect/gpu-nodes/#prequisites","text":"You have to register with hpc-gatekeeper@bihealth.de for requesting access. Afterwards, you can connect to the GPU nodes as shown below.","title":"Prequisites"},{"location":"how-to/connect/gpu-nodes/#preparation","text":"We will setup a miniconda installation with pytorch testing the GPU. If you already have this setup then you can skip this step med-login1:~$ srun --pty bash med0703:~$ wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh med0703:~$ bash Miniconda3-latest-Linux-x86_64.sh -b -p ~/miniconda3 med0703:~$ source ~/miniconda3/bin/activate med0703:~$ conda create -y -n gpu-test python = 3 pytorch med0703:~$ conda activate gpu-test med0703:~$ python -c 'import torch; print(torch.cuda.is_available())' False med0703:~$ exit med-login1:~$ The False shows that CUDA is not available on the node but that is to be expected. We're only warming up!","title":"Preparation"},{"location":"how-to/connect/gpu-nodes/#allocating-gpus","text":"Let us now allocate a GPU. The Slurm schedule will properly allocate GPUs for you and setup the environment variable that tell CUDA which devices are available. The following dry run shows these environment variables (and that they are not available on the login node). med-login1:~$ export | grep CUDA_VISIBLE_DEVICES med-login1:~$ srun -p gpu --gres = gpu:tesla:1 --pty bash med0303:~$ export | grep CUDA_VISIBLE_DEVICES declare -x CUDA_VISIBLE_DEVICES = \"0\" med0303:~$ exit med-login1:~$ srun -p gpu --gres = gpu:tesla:2 --pty bash med0303:~$ export | grep CUDA_VISIBLE_DEVICES declare -x CUDA_VISIBLE_DEVICES = \"0,1\" You can see that you can also reserve multiple GPUs. If we were to open two concurrent connections (e.g., in a screen ) to the same node when allocating one GPU each, the allocated GPUs would be non-overlapping. Note that any two jobs are isolated using Linux cgroups (\"container\" technology \ud83d\ude80) so you cannot accidentally use a GPU of another job. Now to the somewhat boring part where we show that CUDA actually works. med-login1:~$ srun -p gpu --gres = gpu:tesla:1 --pty bash med0301:~$ nvcc --version nvcc: NVIDIA ( R ) Cuda compiler driver Copyright ( c ) 2005 -2019 NVIDIA Corporation Built on Wed_Oct_23_19:24:38_PDT_2019 Cuda compilation tools, release 10 .2, V10.2.89 med0301:~$ source ~/miniconda3/bin/activate med0301:~$ conda activate gpu-test med0301:~$ python -c 'import torch; print(torch.cuda.is_available())' True","title":"Allocating GPUs"},{"location":"how-to/connect/gpu-nodes/#bonus-1-who-is-using-the-gpus","text":"Use squeue to find out about currently queued jobs (the egrep only keeps the header and entries in the gpu partition). med-login1:~$ squeue | egrep -iw 'JOBID|gpu' JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 33 gpu bash holtgrem R 2 :26 1 med0301","title":"Bonus #1: Who is using the GPUs?"},{"location":"how-to/connect/gpu-nodes/#bonus-2-is-the-gpu-running","text":"To find out how active the GPU nodes actually are, you can connect to the nodes (without allocating a GPU you can do this even if the node is full) and then use nvidia-smi . med-login1:~$ ssh med0301 bash med0301:~$ nvidia-smi Fri Mar 6 11 :10:08 2020 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 440 .33.01 Driver Version: 440 .33.01 CUDA Version: 10 .2 | | -------------------------------+----------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | =============================== + ====================== + ====================== | | 0 Tesla V100-SXM2... Off | 00000000 :18:00.0 Off | 0 | | N/A 62C P0 246W / 300W | 16604MiB / 32510MiB | 99 % Default | +-------------------------------+----------------------+----------------------+ | 1 Tesla V100-SXM2... Off | 00000000 :3B:00.0 Off | 0 | | N/A 61C P0 270W / 300W | 16604MiB / 32510MiB | 100 % Default | +-------------------------------+----------------------+----------------------+ | 2 Tesla V100-SXM2... Off | 00000000 :86:00.0 Off | 0 | | N/A 39C P0 55W / 300W | 0MiB / 32510MiB | 0 % Default | +-------------------------------+----------------------+----------------------+ | 3 Tesla V100-SXM2... Off | 00000000 :AF:00.0 Off | 0 | | N/A 44C P0 60W / 300W | 0MiB / 32510MiB | 4 % Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | | ============================================================================= | | 0 43461 C python 16593MiB | | 1 43373 C python 16593MiB | +-----------------------------------------------------------------------------+","title":"Bonus #2: Is the GPU running?"},{"location":"how-to/connect/gpu-nodes/#fair-share-fair-use","text":"Note that allocating a GPU makes it unavailble for everyone else. There is currently no restriction in place on the GPU nodes, so please behave nicely and cooperatively. If you see someone blocking the GPU nodes for long time, then first find out who it is. You can type getent passwd USER_NAME on any cluster node to see the email address (and work phone number if added) of the user. Send a friendly email to the other user, most likely they blocked the node accidentally. If you cannot resolve the issue (e.g., the user is not reachable) then please contact hpc-helpdesk@bihealth.de with your issue.","title":"Fair Share / Fair Use"},{"location":"how-to/connect/high-memory/","text":"How-To: Connect to High-Memory Nodes \u00b6 Prequisites \u00b6 You have to register with hpc-gatekeeper@bihealth.de for requesting access. Afterwards, you can connect to the High-Memory using the highmem SLURM partition (see below). How-To \u00b6 In the cluster there are four High-memory used which can be used: med-login1:~$ sinfo -p highmem PARTITION AVAIL TIMELIMIT NODES STATE NODELIST highmem up 14-00:00:0 3 idle med040[1-4] To connect to one of them, use the below command from login node: med-login1:~$ srun --pty -p highmem bash -i med0401:~$ You can also pick one of the hostnames: med-login1:~$ ssh med0403 med0403:~$ After successfull login, you can see that you are in \"highmem\" queue: med0403:~$ squeue JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) [...] 270 highmem bash holtgrem R 1:25 1 med0403","title":"High-Memory Nodes"},{"location":"how-to/connect/high-memory/#how-to-connect-to-high-memory-nodes","text":"","title":"How-To: Connect to High-Memory Nodes"},{"location":"how-to/connect/high-memory/#prequisites","text":"You have to register with hpc-gatekeeper@bihealth.de for requesting access. Afterwards, you can connect to the High-Memory using the highmem SLURM partition (see below).","title":"Prequisites"},{"location":"how-to/connect/high-memory/#how-to","text":"In the cluster there are four High-memory used which can be used: med-login1:~$ sinfo -p highmem PARTITION AVAIL TIMELIMIT NODES STATE NODELIST highmem up 14-00:00:0 3 idle med040[1-4] To connect to one of them, use the below command from login node: med-login1:~$ srun --pty -p highmem bash -i med0401:~$ You can also pick one of the hostnames: med-login1:~$ ssh med0403 med0403:~$ After successfull login, you can see that you are in \"highmem\" queue: med0403:~$ squeue JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) [...] 270 highmem bash holtgrem R 1:25 1 med0403","title":"How-To"},{"location":"how-to/misc/contribute/","text":"How-To: Contribute to this Document \u00b6 Click on the edit link at the top of each page as shown below. More details in How-To: Contribute to Docs .","title":"Contribute to Docs"},{"location":"how-to/misc/contribute/#how-to-contribute-to-this-document","text":"Click on the edit link at the top of each page as shown below. More details in How-To: Contribute to Docs .","title":"How-To: Contribute to this Document"},{"location":"how-to/service/file-exchange/","text":"How-To: Use File Exchange \u00b6 BIH HPC IT provides a file exchange server to be used by the BIH core facilities and their users. The server is located in the BIH DMZ in Buch. Users authenticate using their Charite/BIH ( user@CHARITE ) or MDC accounts ( user@MDC-BERLIN ). File exchange is organized using \"file boxes\", directories created on the server to which selected users are granted access. Access control list maintenance is done with audit-trails (\"Revisionssicherheit\") and the file access itself is also logged to comply with data protection standards. Jump to \"From Windows\" Jump to \"From Linux\" Jump to \"From Mac\" Access from Charite Network Access from the Charite network (IP ranges 141.x.x.x and 10.x.x.x ) must connect through the Charite proxy ( http://proxy.charite.de:8080 ). Depending on the client software that you are using, you might have to configure the proxy. File Box Management \u00b6 File boxes are created by the core facilities (e.g., the genomics facilities at Charite and MDC). The facility members also organize the access control. Please talk to your core facility contact on file exchange. External users must obtain a Charite or MDC account first. Account creation is handled by the core facilities that the external user is a customer of. File Access \u00b6 Generally, you will be given a URL to your file box similar to https://file-exchange.bihealth.org/<file-box-id> . The files are served over an encrypted connection using WebDAV (which uses HTTPS). The following describes how to access the files in the box from different platforms. From Linux \u00b6 We describe how to access the files on the command line using the lftp program. The program is preinstalled on the BIH (and the MDC cluster) and you should be able to just install it with yum install lftp on CentOS/Red Hat or apt-get install lftp on Ubuntu/Debian. When using lftp , you have to add some configuration first: # cat >>~/.lftprc <<\"EOF\" set ssl:verify-certificate no set ftp:ssl-force yes EOF In case that you want to access the files using a graphical user interface, search Google for \"WebDAV\" and your operating system or desktop environment. File browsers such as Nautilus and Thunar have built-in WebDAV support. Connecting \u00b6 First, log into the machine that has lftp installed. The login nodes of the BIH cluster do not have it installed but all compute and file transfer nodes have it. Go to the data download location. host:~$ mkdir -p ~/scratch/download_dir host:~$ cd ~/scratch/download_dir Next, start lftp . You can open the connection using open -u <user>@<DOMAIN> https://file-exchange.bihealth.org/<file-box-id> where <user> is your user name, e.g., holtgrem , <domain> is either MDC or CHARITE , and <file-box-id> the file box ID from the URL provided to you. When prompted, use your normal Charite/MDC password to login. host:download_dir$ lftp lftp :~> open -u holtgrem@CHARITE https://file-exchange.bihealth.org/c62910b3-c1ba-49a5-81a6-a68f1f15aef6 Password: cd ok, cwd = /c62910b3-c1ba-49a5-81a6-a68f1f15aef6 lftp holtgrem@CHARITE@file-exchange.bihealth.org:/c62910b3-c1ba-49a5-81a6-a68f1f15aef6> Browsing Data \u00b6 You can find a full reference of lftp on the lftp man page . You could also use help COMMAND on the lftp prompt. For example, to look at the files of the server for a bit... lftp holtgrem@CHARITE@file-exchange.bihealth.org:/c62910b3-c1ba-49a5-81a6-a68f1f15aef6> ls drwxr-xr-x -- / drwxr-xr-x -- dir -rw-r--r-- -- file1 lftp holtgrem@CHARITE@file-exchange.bihealth.org:/c62910b3-c1ba-49a5-81a6-a68f1f15aef6> find ./ ./dir/ ./dir/file2 ./file1 Downloading Data \u00b6 To download all data use mirror , e.g. with -P 4 to use four download threads. lftp holtgrem@CHARITE@file-exchange.bihealth.org:/c62910b3-c1ba-49a5-81a6-a68f1f15aef6> mirror . Total: 2 directories, 3 files, 0 symlinks New: 3 files, 0 symlinks lftp holtgrem@CHARITE@file-exchange.bihealth.org:/c62910b3-c1ba-49a5-81a6-a68f1f15aef6> exit host:download_dir$ tree . \u251c\u2500\u2500 dir \u2502 \u2514\u2500\u2500 file2 \u251c\u2500\u2500 file1 \u2514\u2500\u2500 file.txt 1 directory, 3 files Ignoring gnutls_record_recv errors. A common error to see is mirror: Fatal error: gnutls_record_recv: The TLS connection was non-properly terminated. . You can just ignore this. Uploading Data \u00b6 To upload data, you can use mirror -R . which is essentially the \"reverse\" of the mirror command. lftp holtgrem@CHARITE@file-exchange.bihealth.org:/c62910b3-c1ba-49a5-81a6-a68f1f15aef6> mirror -R mirror: Fatal error: gnutls_record_recv: The TLS connection was non-properly terminated. mirror: Fatal error: gnutls_record_recv: The TLS connection was non-properly terminated. mirror: Fatal error: gnutls_record_recv: The TLS connection was non-properly terminated. Total: 2 directories, 3 files, 0 symlinks Modified: 3 files, 0 symlinks 4 errors detected From Windows \u00b6 We recommend to use WinSCP for file transfer. Pre-packaged WinSCP on Charite Workstations. Charite IT has packaged WinSCP and you can install it using Matrix24 Empirum on Windows 10 using these instructions in the Charite intranet . Installing WinSCP yourself. You can obtain it from the WinSCP Download Page . A \"portable\" version is available that comes as a ZIP archive that you just have to extract without an installer. Connecting \u00b6 After starting WinSCP, you will see a window titled Login . Just paste the URL (e.g., https://file-exchange.bihealth.org/c62910b3-c1ba-49a5-81a6-a68f1f15aef6/ ) of the file box into the Host name entry field. In this case, the fields File protocol etc. will be filled automatically. Next, enter your user name as user@CHARITE or user@MDC-BERLIN (the capitalization of the part behind the @ is important). The window should now look similar to the one below. Proxy Configuration on Charite Network If you are on the Charite network then you have to configure the proxy. Otherwise, you have to skip this step. Click Advanced and a window titled Advanced Site Settings will pop up. Here, select Connection / Proxy in the left side. Select HTTP for the Proxy type . Then, enter proxy.charite.de as the Proxy host name and set the Port number to 8080 . The window should nwo look as below. Then, click OK to apply the proxy settings. Finally, click Login . You can now transfer files between the file exchange server and your local computer using drag and drop between WinSCP and your local Windows File Explorer. Alternatively, you can use the two-panel view of WinSCP to transfer files as described here . From Mac \u00b6 For Mac, we you can also use lftp as described above in From Linux . You can find install instructions here online . Proxy Configuration on Charite Network If you are on the Charite network then you must have configured the proxy appropriately. Otherwise, you have to skip this step. You can find them in your System Preference in the Network section, in the Advanced tab of your network (e.g., WiFi ). If you want to use a graphical interface then we recommend the usage of Cyberduck . After starting the program, click Open Connection on the top left, then select WebDAV (HTTPS) and fill out the form as in the following way. Paste the file box URL into the server field and use your login name ( user@CHARITE or user@MDC-BERLIN ) with your usual password. If you need to perform access through a graphical user interface on your Mac, please contact hpc-helpdesk@bihealth.org for support. Security \u00b6 The file exchange server has the fail2ban software installed and configured (Charite, MDC, and BIH IPs are excluded from this). If you are entering your user/password incorrectly for more than 5 times in 10 minutes then your machine will be banned for one hour. This means someone else that has the same IP address from the side of the file exchange server can get you blocked. This can happen if you are in the same home or university network with NAT or if you are behind a proxy. In this case you get a \"connection refused\" error. In this case, try again in one hour.","title":"File Exchange"},{"location":"how-to/service/file-exchange/#how-to-use-file-exchange","text":"BIH HPC IT provides a file exchange server to be used by the BIH core facilities and their users. The server is located in the BIH DMZ in Buch. Users authenticate using their Charite/BIH ( user@CHARITE ) or MDC accounts ( user@MDC-BERLIN ). File exchange is organized using \"file boxes\", directories created on the server to which selected users are granted access. Access control list maintenance is done with audit-trails (\"Revisionssicherheit\") and the file access itself is also logged to comply with data protection standards. Jump to \"From Windows\" Jump to \"From Linux\" Jump to \"From Mac\" Access from Charite Network Access from the Charite network (IP ranges 141.x.x.x and 10.x.x.x ) must connect through the Charite proxy ( http://proxy.charite.de:8080 ). Depending on the client software that you are using, you might have to configure the proxy.","title":"How-To: Use File Exchange"},{"location":"how-to/service/file-exchange/#file-box-management","text":"File boxes are created by the core facilities (e.g., the genomics facilities at Charite and MDC). The facility members also organize the access control. Please talk to your core facility contact on file exchange. External users must obtain a Charite or MDC account first. Account creation is handled by the core facilities that the external user is a customer of.","title":"File Box Management"},{"location":"how-to/service/file-exchange/#file-access","text":"Generally, you will be given a URL to your file box similar to https://file-exchange.bihealth.org/<file-box-id> . The files are served over an encrypted connection using WebDAV (which uses HTTPS). The following describes how to access the files in the box from different platforms.","title":"File Access"},{"location":"how-to/service/file-exchange/#from-linux","text":"We describe how to access the files on the command line using the lftp program. The program is preinstalled on the BIH (and the MDC cluster) and you should be able to just install it with yum install lftp on CentOS/Red Hat or apt-get install lftp on Ubuntu/Debian. When using lftp , you have to add some configuration first: # cat >>~/.lftprc <<\"EOF\" set ssl:verify-certificate no set ftp:ssl-force yes EOF In case that you want to access the files using a graphical user interface, search Google for \"WebDAV\" and your operating system or desktop environment. File browsers such as Nautilus and Thunar have built-in WebDAV support.","title":"From Linux"},{"location":"how-to/service/file-exchange/#connecting","text":"First, log into the machine that has lftp installed. The login nodes of the BIH cluster do not have it installed but all compute and file transfer nodes have it. Go to the data download location. host:~$ mkdir -p ~/scratch/download_dir host:~$ cd ~/scratch/download_dir Next, start lftp . You can open the connection using open -u <user>@<DOMAIN> https://file-exchange.bihealth.org/<file-box-id> where <user> is your user name, e.g., holtgrem , <domain> is either MDC or CHARITE , and <file-box-id> the file box ID from the URL provided to you. When prompted, use your normal Charite/MDC password to login. host:download_dir$ lftp lftp :~> open -u holtgrem@CHARITE https://file-exchange.bihealth.org/c62910b3-c1ba-49a5-81a6-a68f1f15aef6 Password: cd ok, cwd = /c62910b3-c1ba-49a5-81a6-a68f1f15aef6 lftp holtgrem@CHARITE@file-exchange.bihealth.org:/c62910b3-c1ba-49a5-81a6-a68f1f15aef6>","title":"Connecting"},{"location":"how-to/service/file-exchange/#browsing-data","text":"You can find a full reference of lftp on the lftp man page . You could also use help COMMAND on the lftp prompt. For example, to look at the files of the server for a bit... lftp holtgrem@CHARITE@file-exchange.bihealth.org:/c62910b3-c1ba-49a5-81a6-a68f1f15aef6> ls drwxr-xr-x -- / drwxr-xr-x -- dir -rw-r--r-- -- file1 lftp holtgrem@CHARITE@file-exchange.bihealth.org:/c62910b3-c1ba-49a5-81a6-a68f1f15aef6> find ./ ./dir/ ./dir/file2 ./file1","title":"Browsing Data"},{"location":"how-to/service/file-exchange/#downloading-data","text":"To download all data use mirror , e.g. with -P 4 to use four download threads. lftp holtgrem@CHARITE@file-exchange.bihealth.org:/c62910b3-c1ba-49a5-81a6-a68f1f15aef6> mirror . Total: 2 directories, 3 files, 0 symlinks New: 3 files, 0 symlinks lftp holtgrem@CHARITE@file-exchange.bihealth.org:/c62910b3-c1ba-49a5-81a6-a68f1f15aef6> exit host:download_dir$ tree . \u251c\u2500\u2500 dir \u2502 \u2514\u2500\u2500 file2 \u251c\u2500\u2500 file1 \u2514\u2500\u2500 file.txt 1 directory, 3 files Ignoring gnutls_record_recv errors. A common error to see is mirror: Fatal error: gnutls_record_recv: The TLS connection was non-properly terminated. . You can just ignore this.","title":"Downloading Data"},{"location":"how-to/service/file-exchange/#uploading-data","text":"To upload data, you can use mirror -R . which is essentially the \"reverse\" of the mirror command. lftp holtgrem@CHARITE@file-exchange.bihealth.org:/c62910b3-c1ba-49a5-81a6-a68f1f15aef6> mirror -R mirror: Fatal error: gnutls_record_recv: The TLS connection was non-properly terminated. mirror: Fatal error: gnutls_record_recv: The TLS connection was non-properly terminated. mirror: Fatal error: gnutls_record_recv: The TLS connection was non-properly terminated. Total: 2 directories, 3 files, 0 symlinks Modified: 3 files, 0 symlinks 4 errors detected","title":"Uploading Data"},{"location":"how-to/service/file-exchange/#from-windows","text":"We recommend to use WinSCP for file transfer. Pre-packaged WinSCP on Charite Workstations. Charite IT has packaged WinSCP and you can install it using Matrix24 Empirum on Windows 10 using these instructions in the Charite intranet . Installing WinSCP yourself. You can obtain it from the WinSCP Download Page . A \"portable\" version is available that comes as a ZIP archive that you just have to extract without an installer.","title":"From Windows"},{"location":"how-to/service/file-exchange/#connecting_1","text":"After starting WinSCP, you will see a window titled Login . Just paste the URL (e.g., https://file-exchange.bihealth.org/c62910b3-c1ba-49a5-81a6-a68f1f15aef6/ ) of the file box into the Host name entry field. In this case, the fields File protocol etc. will be filled automatically. Next, enter your user name as user@CHARITE or user@MDC-BERLIN (the capitalization of the part behind the @ is important). The window should now look similar to the one below. Proxy Configuration on Charite Network If you are on the Charite network then you have to configure the proxy. Otherwise, you have to skip this step. Click Advanced and a window titled Advanced Site Settings will pop up. Here, select Connection / Proxy in the left side. Select HTTP for the Proxy type . Then, enter proxy.charite.de as the Proxy host name and set the Port number to 8080 . The window should nwo look as below. Then, click OK to apply the proxy settings. Finally, click Login . You can now transfer files between the file exchange server and your local computer using drag and drop between WinSCP and your local Windows File Explorer. Alternatively, you can use the two-panel view of WinSCP to transfer files as described here .","title":"Connecting"},{"location":"how-to/service/file-exchange/#from-mac","text":"For Mac, we you can also use lftp as described above in From Linux . You can find install instructions here online . Proxy Configuration on Charite Network If you are on the Charite network then you must have configured the proxy appropriately. Otherwise, you have to skip this step. You can find them in your System Preference in the Network section, in the Advanced tab of your network (e.g., WiFi ). If you want to use a graphical interface then we recommend the usage of Cyberduck . After starting the program, click Open Connection on the top left, then select WebDAV (HTTPS) and fill out the form as in the following way. Paste the file box URL into the server field and use your login name ( user@CHARITE or user@MDC-BERLIN ) with your usual password. If you need to perform access through a graphical user interface on your Mac, please contact hpc-helpdesk@bihealth.org for support.","title":"From Mac"},{"location":"how-to/service/file-exchange/#security","text":"The file exchange server has the fail2ban software installed and configured (Charite, MDC, and BIH IPs are excluded from this). If you are entering your user/password incorrectly for more than 5 times in 10 minutes then your machine will be banned for one hour. This means someone else that has the same IP address from the side of the file exchange server can get you blocked. This can happen if you are in the same home or university network with NAT or if you are behind a proxy. In this case you get a \"connection refused\" error. In this case, try again in one hour.","title":"Security"},{"location":"how-to/software/cell-ranger/","text":"How-To: Run CellRanger \u00b6 what is Cell Ranger? \u00b6 from the official website : \"Cell Ranger is a set of analysis pipelines that process Chromium single-cell RNA-seq output to align reads, generate feature-barcode matrices and perform clustering and gene expression analysis\" installation \u00b6 requires registration before download from here to unpack Cell Ranger, its dependencies and the cellranger script: cd /fast/users/$USER/scratch mv /path/to/cellranger-3.0.2.tar.gz . tar -xzvf cellranger-3.0.2.tar.gz reference data \u00b6 will be provided in /fast/projects/cubit/current/static_data/app_support/cellranger cluster support SLURM \u00b6 add a file slurm.template to /fast/users/$USER/scratch/cellranger-3.0.2/martian-cs/v3.2.0/jobmanagers/sge.template with the following contents: #!/usr/bin/env bash # # Copyright (c) 2016 10x Genomics, Inc. All rights reserved. # # ============================================================================= # Setup Instructions # ============================================================================= # # 1. Add any other necessary Slurm arguments such as partition (-p) or account # (-A). If your system requires a walltime (-t), 24 hours (24:00:00) is # sufficient. We recommend you do not remove any arguments below or Martian # may not run properly. # # 2. Change filename of slurm.template.example to slurm.template. # # ============================================================================= # Template # ============================================================================= # #SBATCH -J __MRO_JOB_NAME__ #SBATCH --export=ALL #SBATCH --nodes=1 --ntasks-per-node=__MRO_THREADS__ #SBATCH --signal=2 #SBATCH --no-requeue #SBATCH --partition=medium #SBATCH --time=24:00:00 ### Alternatively: --ntasks=1 --cpus-per-task=__MRO_THREADS__ ### Consult with your cluster administrators to find the combination that ### works best for single-node, multi-threaded applications on your system. #SBATCH --mem=__MRO_MEM_GB__G #SBATCH -o __MRO_STDOUT__ #SBATCH -e __MRO_STDERR__ __MRO_CMD__ demultiplexing \u00b6 if that hasn't been done yet, you can use cellranger mkfastq (details to be added) run the pipeline ( count ) \u00b6 create a script run_cellranger.sh with these contents (consult the documentation for help: #!/bin/bash /fast/users/$USER/scratch/cellranger-3.0.2/cellranger count \\ --id=sample_id \\ --transcriptome=/fast/projects/cubit/current/static_data/app_support/cellranger/refdata-cellranger-${species}-3.0.0\\ --fastqs=/path/to/fastqs \\ --sample=sample_name \\ --expect-cells=n_cells \\ --jobmode=slurm \\ --maxjobs=100 \\ --jobinterval=1000 and then submit the job via sbatch --ntasks=1 --mem-per-cpu=4G --time=8:00:00 -p medium -o cellranger.log run_cellranger.sh cluster support SGE (outdated) \u00b6 add a file sge.template to /fast/users/$USER/scratch/cellranger-3.0.2/martian-cs/v3.2.0/jobmanagers/sge.template with the following contents: # ============================================================================= # Template # ============================================================================= # #$ -N __MRO_JOB_NAME__ #$ -V #$ -pe smp __MRO_THREADS__ #$ -cwd #$ -P medium #$ -o __MRO_STDOUT__ #$ -e __MRO_STDERR__ #$ -l h_vmem=__MRO_MEM_GB_PER_THREAD__G #$ -l h_rt=08:00:00 #$ -m a #$ -M user@email.com __MRO_CMD__ and submit the job via qsub -cwd -V -pe smp 1 -l h_vmem=8G -l h_rt=24:00:00 -P medium -m a -j y run_cellranger.sh","title":"Cell Ranger"},{"location":"how-to/software/cell-ranger/#how-to-run-cellranger","text":"","title":"How-To: Run CellRanger"},{"location":"how-to/software/cell-ranger/#what-is-cell-ranger","text":"from the official website : \"Cell Ranger is a set of analysis pipelines that process Chromium single-cell RNA-seq output to align reads, generate feature-barcode matrices and perform clustering and gene expression analysis\"","title":"what is Cell Ranger?"},{"location":"how-to/software/cell-ranger/#installation","text":"requires registration before download from here to unpack Cell Ranger, its dependencies and the cellranger script: cd /fast/users/$USER/scratch mv /path/to/cellranger-3.0.2.tar.gz . tar -xzvf cellranger-3.0.2.tar.gz","title":"installation"},{"location":"how-to/software/cell-ranger/#reference-data","text":"will be provided in /fast/projects/cubit/current/static_data/app_support/cellranger","title":"reference data"},{"location":"how-to/software/cell-ranger/#cluster-support-slurm","text":"add a file slurm.template to /fast/users/$USER/scratch/cellranger-3.0.2/martian-cs/v3.2.0/jobmanagers/sge.template with the following contents: #!/usr/bin/env bash # # Copyright (c) 2016 10x Genomics, Inc. All rights reserved. # # ============================================================================= # Setup Instructions # ============================================================================= # # 1. Add any other necessary Slurm arguments such as partition (-p) or account # (-A). If your system requires a walltime (-t), 24 hours (24:00:00) is # sufficient. We recommend you do not remove any arguments below or Martian # may not run properly. # # 2. Change filename of slurm.template.example to slurm.template. # # ============================================================================= # Template # ============================================================================= # #SBATCH -J __MRO_JOB_NAME__ #SBATCH --export=ALL #SBATCH --nodes=1 --ntasks-per-node=__MRO_THREADS__ #SBATCH --signal=2 #SBATCH --no-requeue #SBATCH --partition=medium #SBATCH --time=24:00:00 ### Alternatively: --ntasks=1 --cpus-per-task=__MRO_THREADS__ ### Consult with your cluster administrators to find the combination that ### works best for single-node, multi-threaded applications on your system. #SBATCH --mem=__MRO_MEM_GB__G #SBATCH -o __MRO_STDOUT__ #SBATCH -e __MRO_STDERR__ __MRO_CMD__","title":"cluster support SLURM"},{"location":"how-to/software/cell-ranger/#demultiplexing","text":"if that hasn't been done yet, you can use cellranger mkfastq (details to be added)","title":"demultiplexing"},{"location":"how-to/software/cell-ranger/#run-the-pipeline-count","text":"create a script run_cellranger.sh with these contents (consult the documentation for help: #!/bin/bash /fast/users/$USER/scratch/cellranger-3.0.2/cellranger count \\ --id=sample_id \\ --transcriptome=/fast/projects/cubit/current/static_data/app_support/cellranger/refdata-cellranger-${species}-3.0.0\\ --fastqs=/path/to/fastqs \\ --sample=sample_name \\ --expect-cells=n_cells \\ --jobmode=slurm \\ --maxjobs=100 \\ --jobinterval=1000 and then submit the job via sbatch --ntasks=1 --mem-per-cpu=4G --time=8:00:00 -p medium -o cellranger.log run_cellranger.sh","title":"run the pipeline (count)"},{"location":"how-to/software/cell-ranger/#cluster-support-sge-outdated","text":"add a file sge.template to /fast/users/$USER/scratch/cellranger-3.0.2/martian-cs/v3.2.0/jobmanagers/sge.template with the following contents: # ============================================================================= # Template # ============================================================================= # #$ -N __MRO_JOB_NAME__ #$ -V #$ -pe smp __MRO_THREADS__ #$ -cwd #$ -P medium #$ -o __MRO_STDOUT__ #$ -e __MRO_STDERR__ #$ -l h_vmem=__MRO_MEM_GB_PER_THREAD__G #$ -l h_rt=08:00:00 #$ -m a #$ -M user@email.com __MRO_CMD__ and submit the job via qsub -cwd -V -pe smp 1 -l h_vmem=8G -l h_rt=24:00:00 -P medium -m a -j y run_cellranger.sh","title":"cluster support SGE (outdated)"},{"location":"how-to/software/jupyter/","text":"How-To: Run Jupyter \u00b6 What is Jupyter \u00b6 Project Jupyter is a networking protocol for interactive computing that allows the user to write and execute code for a high number of different programming languages. The most used client is Jupyter Notebook that can be encountered in various form all over the web. Its basic principle is a document consisting of different cells, each of which contains either code (executed in place) or documentation (written in markdown). This allows one to handily describe the processed workflow. Setup and running Jupyter on the cluster \u00b6 Install Jupyter on the cluster (via conda, by creating a custom environment) med0xxx:~$ conda create -n jupyter jupyter med0xxx:~$ conda activate jupyter (If you want to work in a language other than python, you can install more Jupyter language kernel, see the kernel list ) Now you can start the Jupyter server session (you may want to do this in a screen & srun --pty bash -i session as jupyter keeps running while you are doing computations) med0xxx:~$ jupyter notebook --no-browser Check the port number (usually 8888 ) in the on output and remember it for later: [ I 23 :39:40.860 NotebookApp ] The Jupyter Notebook is running at: [ I 23 :39:40.860 NotebookApp ] http://localhost:8888/ By default, Jupyter will create an access token (a link stated in the output) to protect your notebook against unauthorized access which you have to save and enter in the accessing browser. You can change this to password base authorization via jupyter notebook password . If you are running multiple server on one or more nodes, one can separate them by changing the port number by adding --port=$PORT . Connecting to the Running Session \u00b6 This is slightly trickier as we have to create a SSH connection/tunnel via multiple hubs in between. The easiest is probably to configure your .ssh/config to automatically route your connection via the login-node (and MDC-jail if connecting from outside of the MDC network). You have to add (replace with your $CLUSTER_USER and $MDC_USER ) Host med0* user $CLUSTER_USER ProxyCommand ssh $CLUSTER_USER@med-login2.bihealth.org -W %h:%p (and if you are outside of the MDC network): Host med-login2 ProxyCommand ssh $MDC_USER@ssh1.mdc-berlin.de -W %h:%p to your ~/.ssh/config (If you have a newer version (7.2+) of SSH installed, you can also use ProxyJump $CLUSTER_USER@med-login2.bihealth.org instead of ProxyCommand ... ) See whether this works via i.e. ssh med0110 Now you setup a tunnel workstation:~$ ssh -N -f -L 127 .0.0.1:8888:localhost: ${ PORT } med0 ${ NODE } with the port of your server (usually 8888 ) and the cluster node srun has send you to. You should now be able to connect to the Jupyter server via localhost:8888 in your webbrowser (see the note about token and password above) and start using Jupyter. Losing connection \u00b6 It can and will happen that will lose connection, either due to network problems or due to shut-down of your computer. This is not a problem at all and you will not lose data, just reconnect to your session. If your notebooks are also losing connection (you will see a colorful remark in the top right corner), reconnect and click the colorful button. If this does not work, your work is still not lost as all cells that have been executed are automatically saved anyways. Copy all unexecuted cells (those are only saved periodically) and reload the browser page (after reconnecting) with F5 . (you can also open a copy of the notebook in another tab, just be aware that there may be synchronisation problems) Ending a Session \u00b6 There are two independent steps in ending a session: Canceling the SSH tunnel Identify the running SSH process med0xxx:~$ ps aux | grep \" $PORT \" This will give you something like this: user 54 0.0 0.0 43104 784 ? Ss 15:06 0:00 ssh -N -f -L 127.0.0.1:8888:localhost:8888 med0213 user 58 0.0 0.0 41116 1024 tty1 S 15:42 0:00 grep --color=auto 8888 from which you need the process ID (here 54 ) Terminate it the process med0213:~$ kill -9 $PID Shutdown the Jupyter server Open the Jupyter session, cancel the process with {Ctrl} + {C} and confirm {y}. Make sure you saved your notebooks beforehand (though auto-save catches most things). Advanced \u00b6 List of available Jupyter Kernels for different programming languages Jupyterlab is a further development in the Jupyter ecosystem that creates a display similar to RStudio with panels for the current file system and different notebooks in different tabs. One can install Jupyter kernels or python packages while running a server or notebook without restrictions If anyone has figured out, the following might also be interesting (please add): create a Jupyter-Hub multi-user support","title":"Jupyter"},{"location":"how-to/software/jupyter/#how-to-run-jupyter","text":"","title":"How-To: Run Jupyter"},{"location":"how-to/software/jupyter/#what-is-jupyter","text":"Project Jupyter is a networking protocol for interactive computing that allows the user to write and execute code for a high number of different programming languages. The most used client is Jupyter Notebook that can be encountered in various form all over the web. Its basic principle is a document consisting of different cells, each of which contains either code (executed in place) or documentation (written in markdown). This allows one to handily describe the processed workflow.","title":"What is Jupyter"},{"location":"how-to/software/jupyter/#setup-and-running-jupyter-on-the-cluster","text":"Install Jupyter on the cluster (via conda, by creating a custom environment) med0xxx:~$ conda create -n jupyter jupyter med0xxx:~$ conda activate jupyter (If you want to work in a language other than python, you can install more Jupyter language kernel, see the kernel list ) Now you can start the Jupyter server session (you may want to do this in a screen & srun --pty bash -i session as jupyter keeps running while you are doing computations) med0xxx:~$ jupyter notebook --no-browser Check the port number (usually 8888 ) in the on output and remember it for later: [ I 23 :39:40.860 NotebookApp ] The Jupyter Notebook is running at: [ I 23 :39:40.860 NotebookApp ] http://localhost:8888/ By default, Jupyter will create an access token (a link stated in the output) to protect your notebook against unauthorized access which you have to save and enter in the accessing browser. You can change this to password base authorization via jupyter notebook password . If you are running multiple server on one or more nodes, one can separate them by changing the port number by adding --port=$PORT .","title":"Setup and running Jupyter on the cluster"},{"location":"how-to/software/jupyter/#connecting-to-the-running-session","text":"This is slightly trickier as we have to create a SSH connection/tunnel via multiple hubs in between. The easiest is probably to configure your .ssh/config to automatically route your connection via the login-node (and MDC-jail if connecting from outside of the MDC network). You have to add (replace with your $CLUSTER_USER and $MDC_USER ) Host med0* user $CLUSTER_USER ProxyCommand ssh $CLUSTER_USER@med-login2.bihealth.org -W %h:%p (and if you are outside of the MDC network): Host med-login2 ProxyCommand ssh $MDC_USER@ssh1.mdc-berlin.de -W %h:%p to your ~/.ssh/config (If you have a newer version (7.2+) of SSH installed, you can also use ProxyJump $CLUSTER_USER@med-login2.bihealth.org instead of ProxyCommand ... ) See whether this works via i.e. ssh med0110 Now you setup a tunnel workstation:~$ ssh -N -f -L 127 .0.0.1:8888:localhost: ${ PORT } med0 ${ NODE } with the port of your server (usually 8888 ) and the cluster node srun has send you to. You should now be able to connect to the Jupyter server via localhost:8888 in your webbrowser (see the note about token and password above) and start using Jupyter.","title":"Connecting to the Running Session"},{"location":"how-to/software/jupyter/#losing-connection","text":"It can and will happen that will lose connection, either due to network problems or due to shut-down of your computer. This is not a problem at all and you will not lose data, just reconnect to your session. If your notebooks are also losing connection (you will see a colorful remark in the top right corner), reconnect and click the colorful button. If this does not work, your work is still not lost as all cells that have been executed are automatically saved anyways. Copy all unexecuted cells (those are only saved periodically) and reload the browser page (after reconnecting) with F5 . (you can also open a copy of the notebook in another tab, just be aware that there may be synchronisation problems)","title":"Losing connection"},{"location":"how-to/software/jupyter/#ending-a-session","text":"There are two independent steps in ending a session: Canceling the SSH tunnel Identify the running SSH process med0xxx:~$ ps aux | grep \" $PORT \" This will give you something like this: user 54 0.0 0.0 43104 784 ? Ss 15:06 0:00 ssh -N -f -L 127.0.0.1:8888:localhost:8888 med0213 user 58 0.0 0.0 41116 1024 tty1 S 15:42 0:00 grep --color=auto 8888 from which you need the process ID (here 54 ) Terminate it the process med0213:~$ kill -9 $PID Shutdown the Jupyter server Open the Jupyter session, cancel the process with {Ctrl} + {C} and confirm {y}. Make sure you saved your notebooks beforehand (though auto-save catches most things).","title":"Ending a Session"},{"location":"how-to/software/jupyter/#advanced","text":"List of available Jupyter Kernels for different programming languages Jupyterlab is a further development in the Jupyter ecosystem that creates a display similar to RStudio with panels for the current file system and different notebooks in different tabs. One can install Jupyter kernels or python packages while running a server or notebook without restrictions If anyone has figured out, the following might also be interesting (please add): create a Jupyter-Hub multi-user support","title":"Advanced"},{"location":"how-to/software/keras/","text":"How-To: Run Keras (Multi-GPU) \u00b6 Because the GPU nodes med030[1-4] has four GPU units we can train a model by using multiple GPUs in parallel. This How-To gives an example with Keras 2.2.4 together and tensorflow. Finally soem hints how you can submit a job on the cluster. Hint With tensorflow > 2.0 and newer keras version the multi_gpu_model is deprecated and you have to use the MirroredStrategy . Keras code \u00b6 we need to import the multi_gpu_model model from keras.utils and have to pass our actual model (maybe sequential Keras model) into it. In general Keras automatically configures the number of available nodes ( gpus=None ). This seems not to work on our system. So we have to specify the numer of GPUs, e.g. two with gpus=2 . We put this in a try catch environment that it will also work on CPUs. from keras.utils import multi_gpu_model try : model = multi_gpu_model ( model , gpus = 2 ) except : pass That's it! Please read here on how to submit jobs to the GPU nodes. Conda environment \u00b6 All this was tested with the following conda environment: name: cuda channels: - conda-forge - bioconda - defaults dependencies: - keras=2.2.4 - python=3.6.7 - tensorboard=1.12.0 - tensorflow=1.12.0 - tensorflow-base=1.12.0 - tensorflow-gpu=1.12.0","title":"Keras"},{"location":"how-to/software/keras/#how-to-run-keras-multi-gpu","text":"Because the GPU nodes med030[1-4] has four GPU units we can train a model by using multiple GPUs in parallel. This How-To gives an example with Keras 2.2.4 together and tensorflow. Finally soem hints how you can submit a job on the cluster. Hint With tensorflow > 2.0 and newer keras version the multi_gpu_model is deprecated and you have to use the MirroredStrategy .","title":"How-To: Run Keras (Multi-GPU)"},{"location":"how-to/software/keras/#keras-code","text":"we need to import the multi_gpu_model model from keras.utils and have to pass our actual model (maybe sequential Keras model) into it. In general Keras automatically configures the number of available nodes ( gpus=None ). This seems not to work on our system. So we have to specify the numer of GPUs, e.g. two with gpus=2 . We put this in a try catch environment that it will also work on CPUs. from keras.utils import multi_gpu_model try : model = multi_gpu_model ( model , gpus = 2 ) except : pass That's it! Please read here on how to submit jobs to the GPU nodes.","title":"Keras code"},{"location":"how-to/software/keras/#conda-environment","text":"All this was tested with the following conda environment: name: cuda channels: - conda-forge - bioconda - defaults dependencies: - keras=2.2.4 - python=3.6.7 - tensorboard=1.12.0 - tensorflow=1.12.0 - tensorflow-base=1.12.0 - tensorflow-gpu=1.12.0","title":"Conda environment"},{"location":"how-to/software/matlab/","text":"How-To: Use Matlab \u00b6 Matlab is not fully integrated yet We also need to finalize installation and then document the compiler version GNU Octave as Matlab alternative Note that GNU Octave is an Open Source alternative to Matlab. While both packages are not 100% compatible, Octave is an alternative that does not require any license management. Further, you can easily install it yourself using Conda . Want to use the Matlab GUI? Make sure you understand X forwarding as outline in this FAQ entry . Pre-requisites \u00b6 You have to register with hpc-gatekeeper@bihealth.de for requesting access to the Latlab licenses. Register on User Self Organisation MATLAB after registering with hpc-gatekeeper. Afterwards, you can connect to the High-Memory using the license_matlab_r2016b resource (see below). How-To Use \u00b6 BIH has a license of Matlab R2016b for 16 seats and various licensed packages (see below). To display the available licenses: login-1:~$ scontrol show lic LicenseName = matlab_r2016b Total = 16 Used = 0 Free = 16 Remote = no Matlab is installed on all of the compute nodes: # The following is VITAL so the scheduler allocates a license to your session. login-1:~$ srun -L matlab_r2016b:1 --pty bash -i med0127:~$ scontrol show lic LicenseName=matlab_r2016b Total=16 Used=1 Free=15 Remote=no med0127:~$ module avail ----------------- /usr/share/Modules/modulefiles ----------------- dot module-info null module-git modules use.own ----------------------- /opt/local/modules ----------------------- cmake/3.11.0-0 llvm/6.0.0-0 openmpi/3.1.0-0 gcc/7.2.0-0 matlab/r2016b-0 med0127:~$ module load matlab/r2016b-0 Start matlab without GUI: matlab -nosplash -nodisplay -nojvm Start matlab with GUI (requires X forwarding (ssh -X)): matlab med0127:~$ matlab -nosplash -nodisplay -nojvm < M A T L A B (R) > Copyright 1984-2016 The MathWorks, Inc. R2016b (9.1.0.441655) 64-bit (glnxa64) September 7, 2016 For online documentation, see http://www.mathworks.com/support For product information, visit www.mathworks.com. Non-Degree Granting Education License -- for use at non-degree granting, nonprofit, educational organizations only. Not for government, commercial, or other organizational use. > > ver -------------------------------------------------------------------------------------------- MATLAB Version: 9.1.0.441655 (R2016b) MATLAB License Number: 1108905 Operating System: Linux 3.10.0-862.3.2.el7.x86_64 #1 SMP Mon May 21 23:36:36 UTC 2018 x86_64 Java Version: Java is not enabled -------------------------------------------------------------------------------------------- MATLAB Version 9.1 (R2016b) Bioinformatics Toolbox Version 4.7 (R2016b) Global Optimization Toolbox Version 3.4.1 (R2016b) Image Processing Toolbox Version 9.5 (R2016b) Optimization Toolbox Version 7.5 (R2016b) Parallel Computing Toolbox Version 6.9 (R2016b) Partial Differential Equation Toolbox Version 2.3 (R2016b) Signal Processing Toolbox Version 7.3 (R2016b) SimBiology Version 5.5 (R2016b) Statistics and Machine Learning Toolbox Version 11.0 (R2016b) Wavelet Toolbox Version 4.17 (R2016b) > > exit Running MATLAB UI \u00b6 For starting the Matlab with GUI, make sure that your client is running a X11 server and you connect with X11 forwarding enabled (e.g., ssh -X med-login1 from the Linux command line). Then, make sure to use srun -L matlab_r2016b:1 --pty --x11 for connecting to a node with X11 forwarding enabled. client:~$ ssh -X med-login1 [ ... ] med-login1:~ $ srun -L matlab_r2016b:1 --pty --x11 [ ... ] med0203:~$ module load matlab/r2016b-0 Start matlab without GUI: matlab -nosplash -nodisplay -nojvm Start matlab with GUI ( requires X forwarding ( ssh -X )) : matlab med0203:~$ matlab [ UI will start ] For forcing starting in text mode can be done (as said after module load ): matlab -nosplash -nodisplay -nojvm . Also see this FAQ entry . See Available Matlab Licenses \u00b6 You can use scontrol show lic to see the currently available MATLAB license. E.g., here I am running an interactive shell in which I have requested 1 of the 16 MATLAB licenses, so 15 more remain. $ scontrol show lic LicenseName=matlab_r2016b Total=16 Used=1 Free=15 Remote=no A Working Example \u00b6 Get a checkout of our MATLAB example. Then, look around at the contents of this repository. med-login1:~$ git clone https://github.com/bihealth/bih-cluster-matlab-example.git med-login1:~$ cd bih-cluster-matlab-example med-login1:~$ cat job_script.sh # !/bin/bash # Logging goes to directory sge_log # SBATCH -o slurm_log/%x-%J.log # Keep current environment variables # SBATCH --export = ALL # Name of the script # SBATCH --job-name MATLAB-example # Allocate 4GB of RAM per core # SBATCH --mem 4G # Maximal running time of 2 hours # SBATCH --time 02 :00:00 # Allocate one Matlab license # SBATCH -L matlab_r2016b:1 module load matlab/r2016b-0 matlab -r example $ cat example.m % Example Hello World script for Matlab. disp('Hello world!') disp('Thinking...') pause(10) disp(sprintf('The square root of 2 is = %f', sqrt(2))) exit For submitting the script, you can do the following med-login1:~$ sbatch job_script.sh This will submit a job with one Matlab license requested. If you were to submit 17 of these jobs, then at least one of them would have to wait until all licenses are free. Matlab License Server Note that there is a Matlab license server running on the server that will check whether 16 or less Matlab sessions are currently running. If a Matlab session is running but this is not made known to the scheduler via -L matlab_r2016b then this can lead to scripts crashing as not enough licenses are available. If this happens to you, double-check that you have specified the license requirements correctly and notify hpc-helpdesk@bihealth.de in case of any problems. We will try to sort out the situation then.","title":"Matlab"},{"location":"how-to/software/matlab/#how-to-use-matlab","text":"Matlab is not fully integrated yet We also need to finalize installation and then document the compiler version GNU Octave as Matlab alternative Note that GNU Octave is an Open Source alternative to Matlab. While both packages are not 100% compatible, Octave is an alternative that does not require any license management. Further, you can easily install it yourself using Conda . Want to use the Matlab GUI? Make sure you understand X forwarding as outline in this FAQ entry .","title":"How-To: Use Matlab"},{"location":"how-to/software/matlab/#pre-requisites","text":"You have to register with hpc-gatekeeper@bihealth.de for requesting access to the Latlab licenses. Register on User Self Organisation MATLAB after registering with hpc-gatekeeper. Afterwards, you can connect to the High-Memory using the license_matlab_r2016b resource (see below).","title":"Pre-requisites"},{"location":"how-to/software/matlab/#how-to-use","text":"BIH has a license of Matlab R2016b for 16 seats and various licensed packages (see below). To display the available licenses: login-1:~$ scontrol show lic LicenseName = matlab_r2016b Total = 16 Used = 0 Free = 16 Remote = no Matlab is installed on all of the compute nodes: # The following is VITAL so the scheduler allocates a license to your session. login-1:~$ srun -L matlab_r2016b:1 --pty bash -i med0127:~$ scontrol show lic LicenseName=matlab_r2016b Total=16 Used=1 Free=15 Remote=no med0127:~$ module avail ----------------- /usr/share/Modules/modulefiles ----------------- dot module-info null module-git modules use.own ----------------------- /opt/local/modules ----------------------- cmake/3.11.0-0 llvm/6.0.0-0 openmpi/3.1.0-0 gcc/7.2.0-0 matlab/r2016b-0 med0127:~$ module load matlab/r2016b-0 Start matlab without GUI: matlab -nosplash -nodisplay -nojvm Start matlab with GUI (requires X forwarding (ssh -X)): matlab med0127:~$ matlab -nosplash -nodisplay -nojvm < M A T L A B (R) > Copyright 1984-2016 The MathWorks, Inc. R2016b (9.1.0.441655) 64-bit (glnxa64) September 7, 2016 For online documentation, see http://www.mathworks.com/support For product information, visit www.mathworks.com. Non-Degree Granting Education License -- for use at non-degree granting, nonprofit, educational organizations only. Not for government, commercial, or other organizational use. > > ver -------------------------------------------------------------------------------------------- MATLAB Version: 9.1.0.441655 (R2016b) MATLAB License Number: 1108905 Operating System: Linux 3.10.0-862.3.2.el7.x86_64 #1 SMP Mon May 21 23:36:36 UTC 2018 x86_64 Java Version: Java is not enabled -------------------------------------------------------------------------------------------- MATLAB Version 9.1 (R2016b) Bioinformatics Toolbox Version 4.7 (R2016b) Global Optimization Toolbox Version 3.4.1 (R2016b) Image Processing Toolbox Version 9.5 (R2016b) Optimization Toolbox Version 7.5 (R2016b) Parallel Computing Toolbox Version 6.9 (R2016b) Partial Differential Equation Toolbox Version 2.3 (R2016b) Signal Processing Toolbox Version 7.3 (R2016b) SimBiology Version 5.5 (R2016b) Statistics and Machine Learning Toolbox Version 11.0 (R2016b) Wavelet Toolbox Version 4.17 (R2016b) > > exit","title":"How-To Use"},{"location":"how-to/software/matlab/#running-matlab-ui","text":"For starting the Matlab with GUI, make sure that your client is running a X11 server and you connect with X11 forwarding enabled (e.g., ssh -X med-login1 from the Linux command line). Then, make sure to use srun -L matlab_r2016b:1 --pty --x11 for connecting to a node with X11 forwarding enabled. client:~$ ssh -X med-login1 [ ... ] med-login1:~ $ srun -L matlab_r2016b:1 --pty --x11 [ ... ] med0203:~$ module load matlab/r2016b-0 Start matlab without GUI: matlab -nosplash -nodisplay -nojvm Start matlab with GUI ( requires X forwarding ( ssh -X )) : matlab med0203:~$ matlab [ UI will start ] For forcing starting in text mode can be done (as said after module load ): matlab -nosplash -nodisplay -nojvm . Also see this FAQ entry .","title":"Running MATLAB UI"},{"location":"how-to/software/matlab/#see-available-matlab-licenses","text":"You can use scontrol show lic to see the currently available MATLAB license. E.g., here I am running an interactive shell in which I have requested 1 of the 16 MATLAB licenses, so 15 more remain. $ scontrol show lic LicenseName=matlab_r2016b Total=16 Used=1 Free=15 Remote=no","title":"See Available Matlab Licenses"},{"location":"how-to/software/matlab/#a-working-example","text":"Get a checkout of our MATLAB example. Then, look around at the contents of this repository. med-login1:~$ git clone https://github.com/bihealth/bih-cluster-matlab-example.git med-login1:~$ cd bih-cluster-matlab-example med-login1:~$ cat job_script.sh # !/bin/bash # Logging goes to directory sge_log # SBATCH -o slurm_log/%x-%J.log # Keep current environment variables # SBATCH --export = ALL # Name of the script # SBATCH --job-name MATLAB-example # Allocate 4GB of RAM per core # SBATCH --mem 4G # Maximal running time of 2 hours # SBATCH --time 02 :00:00 # Allocate one Matlab license # SBATCH -L matlab_r2016b:1 module load matlab/r2016b-0 matlab -r example $ cat example.m % Example Hello World script for Matlab. disp('Hello world!') disp('Thinking...') pause(10) disp(sprintf('The square root of 2 is = %f', sqrt(2))) exit For submitting the script, you can do the following med-login1:~$ sbatch job_script.sh This will submit a job with one Matlab license requested. If you were to submit 17 of these jobs, then at least one of them would have to wait until all licenses are free. Matlab License Server Note that there is a Matlab license server running on the server that will check whether 16 or less Matlab sessions are currently running. If a Matlab session is running but this is not made known to the scheduler via -L matlab_r2016b then this can lead to scripts crashing as not enough licenses are available. If this happens to you, double-check that you have specified the license requirements correctly and notify hpc-helpdesk@bihealth.de in case of any problems. We will try to sort out the situation then.","title":"A Working Example"},{"location":"how-to/software/openmpi/","text":"How-To: Build and Run OpenMPI Program \u00b6 This article describes how to build an run an OpenMPI program. We will build a simple C program that uses the OpenMPI message passing interface and run it in parallel. You should be able to go from here with other languages and more complex programs. We will use a simple Makefile for building the software. Loading OpenMPI Environment \u00b6 First, load the OpenMPI package. med-login1:~$ srun --pty bash -i med0127:~$ module load openmpi/4.3.0-0 Then, check that the installation works med0127:~$ ompi_info | head Package: Open MPI root@med0127 Distribution Open MPI: 4 .0.3 Open MPI repo revision: v4.0.3 Open MPI release date: Mar 03 , 2020 Open RTE: 4 .0.3 Open RTE repo revision: v4.0.3 Open RTE release date: Mar 03 , 2020 OPAL: 4 .0.3 OPAL repo revision: v4.0.3 OPAL release date: Mar 03 , 2020 Building the example \u00b6 Next, clone the OpenMPI example project from Gitlab. med0127:~$ git clone git@github.com:bihealth/bih-cluster-openmpi-example.git med0127:~$ cd bih-cluster-openmpi-example/src Makefile .PHONY : default clean # configure compilers CC = mpicc CXX = mpicxx # configure flags CCFLAGS += $( shell mpicc --showme:compile ) LDFLAGS += $( shell mpicc --showme:link ) default : openmpi_example openmpi_example : openmpi_example . o clean : rm -f openmpi_example.o openmpi_example openmpi_example.c #include <stdio.h> #include <mpi.h> int main ( int argc , char ** argv ) { // Initialize the MPI environment MPI_Init ( NULL , NULL ); // Get the number of processes int world_size ; MPI_Comm_size ( MPI_COMM_WORLD , & world_size ); // Get the rank of the process int world_rank ; MPI_Comm_rank ( MPI_COMM_WORLD , & world_rank ); // Get the name of the processor char processor_name [ MPI_MAX_PROCESSOR_NAME ]; int name_len ; MPI_Get_processor_name ( processor_name , & name_len ); // Print off a hello world message printf ( \"Hello world from processor %s, rank %d\" \" out of %d processors \\n \" , processor_name , world_rank , world_size ); // Finalize the MPI environment. MPI_Finalize (); return 0 ; } run_mpi.sh #!/bin/bash # Example job script for (single-threaded) MPI programs. # Generic arguments # Job name #SBATCH --job-name openmpi_example # Maximal running time of 10 min #SBATCH --time 00:10:00 # Allocate 1GB of memory per node #SBATCH --mem 1G # Write logs to directory \"slurm_log\" #SBATCH -o slurm_log/slurm-%x-%J.log # MPI-specific parameters # Run 64 tasks (threads/on virtual cores) #SBATCH --nodes 64 # Make sure to source the profile.d file (not available on head nodes). /etc/profile.d/modules.sh # Load the OpenMPI environment module to get the runtime environment. module load openmpi/3.1.0-0 # Launch the program. mpirun -np 64 ./openmpi_example The next step is building the software med0127:~$ make mpicc -c -o openmpi_example.o openmpi_example.c mpicc -pthread -Wl,-rpath -Wl,/opt/local/openmpi-4.0.3-0/lib -Wl,--enable-new-dtags -L/opt/local/openmpi-4.0.3-0/lib -lmpi openmpi_example.o -o openmpi_example med0127:~$ ls -lh total 259K -rw-rw---- 1 holtgrem_c hpc-ag-cubi 287 Apr 7 23 :29 Makefile -rwxrwx--- 1 holtgrem_c hpc-ag-cubi 8 .5K Apr 8 00 :15 openmpi_example -rw-rw---- 1 holtgrem_c hpc-ag-cubi 760 Apr 7 23 :29 openmpi_example.c -rw-rw---- 1 holtgrem_c hpc-ag-cubi 2 .1K Apr 8 00 :15 openmpi_example.o -rwxrwx--- 1 holtgrem_c hpc-ag-cubi 1 .3K Apr 7 23 :29 run_hybrid.sh -rwxrwx--- 1 holtgrem_c hpc-ag-cubi 663 Apr 7 23 :35 run_mpi.sh drwxrwx--- 2 holtgrem_c hpc-ag-cubi 4 .0K Apr 7 23 :29 sge_log The software will run outside of the MPI environment -- but in a single process only, of course. med0127:~$ ./openmpi_example Hello world from processor med0127, rank 0 out of 1 processors Running OpenMPI Software \u00b6 All of the arguments are already in the run_mpi.sh script. med01247:~# sbatch run_mpi.sh Explanation of the OpenMPI-specific arguments --ntasks 64 : run 64 processes in the MPI environment. Let's look at the slurm log file, e.g., in slurm_log/slurm-openmpi_example-3181.log . med0124:~$ cat slurm_log/slurm-openmpi_example-*.log Hello world from processor med0133, rank 6 out of 64 processors Hello world from processor med0133, rank 25 out of 64 processors Hello world from processor med0133, rank 1 out of 64 processors Hello world from processor med0133, rank 2 out of 64 processors Hello world from processor med0133, rank 3 out of 64 processors Hello world from processor med0133, rank 7 out of 64 processors Hello world from processor med0133, rank 9 out of 64 processors Hello world from processor med0133, rank 12 out of 64 processors Hello world from processor med0133, rank 13 out of 64 processors Hello world from processor med0133, rank 15 out of 64 processors Hello world from processor med0133, rank 16 out of 64 processors Hello world from processor med0133, rank 17 out of 64 processors Hello world from processor med0133, rank 18 out of 64 processors Hello world from processor med0133, rank 23 out of 64 processors Hello world from processor med0133, rank 24 out of 64 processors Hello world from processor med0133, rank 26 out of 64 processors Hello world from processor med0133, rank 27 out of 64 processors Hello world from processor med0133, rank 31 out of 64 processors Hello world from processor med0133, rank 0 out of 64 processors Hello world from processor med0133, rank 4 out of 64 processors Hello world from processor med0133, rank 5 out of 64 processors Hello world from processor med0133, rank 8 out of 64 processors Hello world from processor med0133, rank 10 out of 64 processors Hello world from processor med0133, rank 11 out of 64 processors Hello world from processor med0133, rank 14 out of 64 processors Hello world from processor med0133, rank 19 out of 64 processors Hello world from processor med0133, rank 20 out of 64 processors Hello world from processor med0133, rank 21 out of 64 processors Hello world from processor med0133, rank 22 out of 64 processors Hello world from processor med0133, rank 28 out of 64 processors Hello world from processor med0133, rank 29 out of 64 processors Hello world from processor med0133, rank 30 out of 64 processors Hello world from processor med0134, rank 32 out of 64 processors Hello world from processor med0134, rank 33 out of 64 processors Hello world from processor med0134, rank 34 out of 64 processors Hello world from processor med0134, rank 38 out of 64 processors Hello world from processor med0134, rank 39 out of 64 processors Hello world from processor med0134, rank 42 out of 64 processors Hello world from processor med0134, rank 44 out of 64 processors Hello world from processor med0134, rank 45 out of 64 processors Hello world from processor med0134, rank 46 out of 64 processors Hello world from processor med0134, rank 53 out of 64 processors Hello world from processor med0134, rank 54 out of 64 processors Hello world from processor med0134, rank 55 out of 64 processors Hello world from processor med0134, rank 60 out of 64 processors Hello world from processor med0134, rank 62 out of 64 processors Hello world from processor med0134, rank 35 out of 64 processors Hello world from processor med0134, rank 36 out of 64 processors Hello world from processor med0134, rank 37 out of 64 processors Hello world from processor med0134, rank 40 out of 64 processors Hello world from processor med0134, rank 41 out of 64 processors Hello world from processor med0134, rank 43 out of 64 processors Hello world from processor med0134, rank 47 out of 64 processors Hello world from processor med0134, rank 48 out of 64 processors Hello world from processor med0134, rank 49 out of 64 processors Hello world from processor med0134, rank 50 out of 64 processors Hello world from processor med0134, rank 51 out of 64 processors Hello world from processor med0134, rank 52 out of 64 processors Hello world from processor med0134, rank 56 out of 64 processors Hello world from processor med0134, rank 57 out of 64 processors Hello world from processor med0134, rank 59 out of 64 processors Hello world from processor med0134, rank 61 out of 64 processors Hello world from processor med0134, rank 63 out of 64 processors Hello world from processor med0134, rank 58 out of 64 processors Running Hybrid Software (MPI+Multithreading) \u00b6 In some cases, you want to mix multithreading (e.g., via OpenMP) with MPI to run one process with multiple threads that then can communicate via shared memory. Note that OpenMPI will let processes on the same node communicate via shared memory anyway, so this might not be necessary in all cases. The file run_hybrid.sh shows how to run an MPI job with 8 threads each. Note well that memory is allocated on a per-slot (thus per-thread) base! run_hybrid.sh #!/bin/bash # Example job script for multi-threaded MPI programs, sometimes # called \"hybrid\" MPI computing. # Generic arguments # Job name #SBATCH --job-name openmpi_example # Maximal running time of 10 min #SBATCH --time 00:10:00 # Allocate 1GB of memory per node #SBATCH --mem 1G # Write logs to directory \"slurm_log\" #SBATCH -o slurm_log/slurm-%x-%J.log # MPI-specific parameters # Run 8 tasks (threads/on virtual cores) #SBATCH --ntasks 8 # Allocate 4 CPUs per task (cores/threads) #SBATCH --cpus-per-task 4 # Make sure to source the profile.d file (not available on head nodes). source /etc/profile.d/modules.sh # Load the OpenMPI environment module to get the runtime environment. module load openmpi/4.0.3-0 # Launch the program. mpirun -n 8 ./openmpi_example We changed the following run 8 tasks (\"processes\") allocate 4 threads each Let's look at the log output: # cat slurm_log/slurm-openmpi_example-3193.log Hello world from processor med0133, rank 1 out of 8 processors Hello world from processor med0133, rank 3 out of 8 processors Hello world from processor med0133, rank 2 out of 8 processors Hello world from processor med0133, rank 6 out of 8 processors Hello world from processor med0133, rank 0 out of 8 processors Hello world from processor med0133, rank 4 out of 8 processors Hello world from processor med0133, rank 5 out of 8 processors Hello world from processor med0133, rank 7 out of 8 processors Each process can now launch 4 threads (e.g., by defining export OMP_NUM_THREADS=4 before the program call).","title":"OpenMPI"},{"location":"how-to/software/openmpi/#how-to-build-and-run-openmpi-program","text":"This article describes how to build an run an OpenMPI program. We will build a simple C program that uses the OpenMPI message passing interface and run it in parallel. You should be able to go from here with other languages and more complex programs. We will use a simple Makefile for building the software.","title":"How-To: Build and Run OpenMPI Program"},{"location":"how-to/software/openmpi/#loading-openmpi-environment","text":"First, load the OpenMPI package. med-login1:~$ srun --pty bash -i med0127:~$ module load openmpi/4.3.0-0 Then, check that the installation works med0127:~$ ompi_info | head Package: Open MPI root@med0127 Distribution Open MPI: 4 .0.3 Open MPI repo revision: v4.0.3 Open MPI release date: Mar 03 , 2020 Open RTE: 4 .0.3 Open RTE repo revision: v4.0.3 Open RTE release date: Mar 03 , 2020 OPAL: 4 .0.3 OPAL repo revision: v4.0.3 OPAL release date: Mar 03 , 2020","title":"Loading OpenMPI Environment"},{"location":"how-to/software/openmpi/#building-the-example","text":"Next, clone the OpenMPI example project from Gitlab. med0127:~$ git clone git@github.com:bihealth/bih-cluster-openmpi-example.git med0127:~$ cd bih-cluster-openmpi-example/src Makefile .PHONY : default clean # configure compilers CC = mpicc CXX = mpicxx # configure flags CCFLAGS += $( shell mpicc --showme:compile ) LDFLAGS += $( shell mpicc --showme:link ) default : openmpi_example openmpi_example : openmpi_example . o clean : rm -f openmpi_example.o openmpi_example openmpi_example.c #include <stdio.h> #include <mpi.h> int main ( int argc , char ** argv ) { // Initialize the MPI environment MPI_Init ( NULL , NULL ); // Get the number of processes int world_size ; MPI_Comm_size ( MPI_COMM_WORLD , & world_size ); // Get the rank of the process int world_rank ; MPI_Comm_rank ( MPI_COMM_WORLD , & world_rank ); // Get the name of the processor char processor_name [ MPI_MAX_PROCESSOR_NAME ]; int name_len ; MPI_Get_processor_name ( processor_name , & name_len ); // Print off a hello world message printf ( \"Hello world from processor %s, rank %d\" \" out of %d processors \\n \" , processor_name , world_rank , world_size ); // Finalize the MPI environment. MPI_Finalize (); return 0 ; } run_mpi.sh #!/bin/bash # Example job script for (single-threaded) MPI programs. # Generic arguments # Job name #SBATCH --job-name openmpi_example # Maximal running time of 10 min #SBATCH --time 00:10:00 # Allocate 1GB of memory per node #SBATCH --mem 1G # Write logs to directory \"slurm_log\" #SBATCH -o slurm_log/slurm-%x-%J.log # MPI-specific parameters # Run 64 tasks (threads/on virtual cores) #SBATCH --nodes 64 # Make sure to source the profile.d file (not available on head nodes). /etc/profile.d/modules.sh # Load the OpenMPI environment module to get the runtime environment. module load openmpi/3.1.0-0 # Launch the program. mpirun -np 64 ./openmpi_example The next step is building the software med0127:~$ make mpicc -c -o openmpi_example.o openmpi_example.c mpicc -pthread -Wl,-rpath -Wl,/opt/local/openmpi-4.0.3-0/lib -Wl,--enable-new-dtags -L/opt/local/openmpi-4.0.3-0/lib -lmpi openmpi_example.o -o openmpi_example med0127:~$ ls -lh total 259K -rw-rw---- 1 holtgrem_c hpc-ag-cubi 287 Apr 7 23 :29 Makefile -rwxrwx--- 1 holtgrem_c hpc-ag-cubi 8 .5K Apr 8 00 :15 openmpi_example -rw-rw---- 1 holtgrem_c hpc-ag-cubi 760 Apr 7 23 :29 openmpi_example.c -rw-rw---- 1 holtgrem_c hpc-ag-cubi 2 .1K Apr 8 00 :15 openmpi_example.o -rwxrwx--- 1 holtgrem_c hpc-ag-cubi 1 .3K Apr 7 23 :29 run_hybrid.sh -rwxrwx--- 1 holtgrem_c hpc-ag-cubi 663 Apr 7 23 :35 run_mpi.sh drwxrwx--- 2 holtgrem_c hpc-ag-cubi 4 .0K Apr 7 23 :29 sge_log The software will run outside of the MPI environment -- but in a single process only, of course. med0127:~$ ./openmpi_example Hello world from processor med0127, rank 0 out of 1 processors","title":"Building the example"},{"location":"how-to/software/openmpi/#running-openmpi-software","text":"All of the arguments are already in the run_mpi.sh script. med01247:~# sbatch run_mpi.sh Explanation of the OpenMPI-specific arguments --ntasks 64 : run 64 processes in the MPI environment. Let's look at the slurm log file, e.g., in slurm_log/slurm-openmpi_example-3181.log . med0124:~$ cat slurm_log/slurm-openmpi_example-*.log Hello world from processor med0133, rank 6 out of 64 processors Hello world from processor med0133, rank 25 out of 64 processors Hello world from processor med0133, rank 1 out of 64 processors Hello world from processor med0133, rank 2 out of 64 processors Hello world from processor med0133, rank 3 out of 64 processors Hello world from processor med0133, rank 7 out of 64 processors Hello world from processor med0133, rank 9 out of 64 processors Hello world from processor med0133, rank 12 out of 64 processors Hello world from processor med0133, rank 13 out of 64 processors Hello world from processor med0133, rank 15 out of 64 processors Hello world from processor med0133, rank 16 out of 64 processors Hello world from processor med0133, rank 17 out of 64 processors Hello world from processor med0133, rank 18 out of 64 processors Hello world from processor med0133, rank 23 out of 64 processors Hello world from processor med0133, rank 24 out of 64 processors Hello world from processor med0133, rank 26 out of 64 processors Hello world from processor med0133, rank 27 out of 64 processors Hello world from processor med0133, rank 31 out of 64 processors Hello world from processor med0133, rank 0 out of 64 processors Hello world from processor med0133, rank 4 out of 64 processors Hello world from processor med0133, rank 5 out of 64 processors Hello world from processor med0133, rank 8 out of 64 processors Hello world from processor med0133, rank 10 out of 64 processors Hello world from processor med0133, rank 11 out of 64 processors Hello world from processor med0133, rank 14 out of 64 processors Hello world from processor med0133, rank 19 out of 64 processors Hello world from processor med0133, rank 20 out of 64 processors Hello world from processor med0133, rank 21 out of 64 processors Hello world from processor med0133, rank 22 out of 64 processors Hello world from processor med0133, rank 28 out of 64 processors Hello world from processor med0133, rank 29 out of 64 processors Hello world from processor med0133, rank 30 out of 64 processors Hello world from processor med0134, rank 32 out of 64 processors Hello world from processor med0134, rank 33 out of 64 processors Hello world from processor med0134, rank 34 out of 64 processors Hello world from processor med0134, rank 38 out of 64 processors Hello world from processor med0134, rank 39 out of 64 processors Hello world from processor med0134, rank 42 out of 64 processors Hello world from processor med0134, rank 44 out of 64 processors Hello world from processor med0134, rank 45 out of 64 processors Hello world from processor med0134, rank 46 out of 64 processors Hello world from processor med0134, rank 53 out of 64 processors Hello world from processor med0134, rank 54 out of 64 processors Hello world from processor med0134, rank 55 out of 64 processors Hello world from processor med0134, rank 60 out of 64 processors Hello world from processor med0134, rank 62 out of 64 processors Hello world from processor med0134, rank 35 out of 64 processors Hello world from processor med0134, rank 36 out of 64 processors Hello world from processor med0134, rank 37 out of 64 processors Hello world from processor med0134, rank 40 out of 64 processors Hello world from processor med0134, rank 41 out of 64 processors Hello world from processor med0134, rank 43 out of 64 processors Hello world from processor med0134, rank 47 out of 64 processors Hello world from processor med0134, rank 48 out of 64 processors Hello world from processor med0134, rank 49 out of 64 processors Hello world from processor med0134, rank 50 out of 64 processors Hello world from processor med0134, rank 51 out of 64 processors Hello world from processor med0134, rank 52 out of 64 processors Hello world from processor med0134, rank 56 out of 64 processors Hello world from processor med0134, rank 57 out of 64 processors Hello world from processor med0134, rank 59 out of 64 processors Hello world from processor med0134, rank 61 out of 64 processors Hello world from processor med0134, rank 63 out of 64 processors Hello world from processor med0134, rank 58 out of 64 processors","title":"Running OpenMPI Software"},{"location":"how-to/software/openmpi/#running-hybrid-software-mpimultithreading","text":"In some cases, you want to mix multithreading (e.g., via OpenMP) with MPI to run one process with multiple threads that then can communicate via shared memory. Note that OpenMPI will let processes on the same node communicate via shared memory anyway, so this might not be necessary in all cases. The file run_hybrid.sh shows how to run an MPI job with 8 threads each. Note well that memory is allocated on a per-slot (thus per-thread) base! run_hybrid.sh #!/bin/bash # Example job script for multi-threaded MPI programs, sometimes # called \"hybrid\" MPI computing. # Generic arguments # Job name #SBATCH --job-name openmpi_example # Maximal running time of 10 min #SBATCH --time 00:10:00 # Allocate 1GB of memory per node #SBATCH --mem 1G # Write logs to directory \"slurm_log\" #SBATCH -o slurm_log/slurm-%x-%J.log # MPI-specific parameters # Run 8 tasks (threads/on virtual cores) #SBATCH --ntasks 8 # Allocate 4 CPUs per task (cores/threads) #SBATCH --cpus-per-task 4 # Make sure to source the profile.d file (not available on head nodes). source /etc/profile.d/modules.sh # Load the OpenMPI environment module to get the runtime environment. module load openmpi/4.0.3-0 # Launch the program. mpirun -n 8 ./openmpi_example We changed the following run 8 tasks (\"processes\") allocate 4 threads each Let's look at the log output: # cat slurm_log/slurm-openmpi_example-3193.log Hello world from processor med0133, rank 1 out of 8 processors Hello world from processor med0133, rank 3 out of 8 processors Hello world from processor med0133, rank 2 out of 8 processors Hello world from processor med0133, rank 6 out of 8 processors Hello world from processor med0133, rank 0 out of 8 processors Hello world from processor med0133, rank 4 out of 8 processors Hello world from processor med0133, rank 5 out of 8 processors Hello world from processor med0133, rank 7 out of 8 processors Each process can now launch 4 threads (e.g., by defining export OMP_NUM_THREADS=4 before the program call).","title":"Running Hybrid Software (MPI+Multithreading)"},{"location":"how-to/software/scientific-software/","text":"How-To: Install Custom Scientific Software \u00b6 This page gives an end-to-end example how to build and install Gromacs as an example for managing complex scientific software installs in user land. You don't have to learn or understand the specifics of Gromacs. We use it as an example as there are some actual users on the BIH cluster. However, installing it is out of scope of BIH HPC administration. Gromacs is a good example as it is a sufficiently complex piece of software. Quite some configuration is done on the command line and there is no current software package of it in the common RPM repositories. However, it is quite well-documented and easy to install for scientific software so there is a lot to be learned. Related Documents \u00b6 How-To: Build and Run OpenMPI Programs Steps for Installing Scientific Software \u00b6 We will perform the following step: Download and extract the source of the software Configure the software (i.e., create the actual build system Makefile s) Compile the software Install the software Create environment module files so the software is easy to use Many scientific software packages will have more dependencies. If the dependencies are available as CentOS Core or EPEL packages (such as zlib), HPC IT administration can install them. However, otherwise you will have to install them on their own. Warning Do not perform the compilation on the login nodes but go to a compute node instead. Downloading and Extracting Software \u00b6 This is best done in your scratch directory as we don't have to keep these files around for long. Note that the files in your scratch directory will automatically be removed after 4 weeks. You can also use your work directory here. med-login1:~$ srun --pty bash -i med0127:~$ mkdir $HOME /scratch/gromacs-install med0127:~$ cd $HOME /scratch/gromacs-install med0127:~$ wget http://ftp.gromacs.org/pub/gromacs/gromacs-2018.3.tar.gz med0127:~$ tar xf gromacs-2018.3.tar.gz med0127:~$ ls gromacs-2018.3 admin cmake COPYING CTestConfig.cmake INSTALL scripts src AUTHORS CMakeLists.txt CPackInit.cmake docs README share tests So far so good! Perform the Configure Step \u00b6 This is the most critical step. Most scientific C/C++ software has a build step and allows for, e.g., disabling and enabling features or setting installation paths. Here, you can configure the software depending on your needs and environment. Also, it is the easiest step to mess up. Gromac's documentation is actually quite good but the author had problems to follow it to the letter. Gromacs recommends to create an MPI and a non-MPI build but the precise way did not work. This installation creates two flavours for Gromacs 2018.3, but in a different way than the Gromacs documentation proposes. First, here is how to configure the non-MPI flavour Gromacs wants a modern compiler, so we load gcc . We will need to note down the precise version we used so later we can load it for running Gromacs with the appropriate libraries. We will install gromacs into $HOME/work/software , which is appropriate for user-installed software, but it could also go into a group or project directory. Note that we install the software into your work directory as software installations are quite large and might go above your home quota. Also, software installations are usually not precious enough to waste resources on snapshots and backups. Also that we force Gromacs to use AVX_256 for SIMD support (Intel sandy bridge architecture) to not get unsupported CPU instruction errors. med0127:~$ module load gcc/7.2.0-0 cmake/3.11.0-0 med0127:~$ module list Currently Loaded Modulefiles: 1 ) gcc/7.2.0-0 2 ) cmake/3.11.0-0 med0127:~$ mkdir gromacs-2018.3-build-nompi med0127:~$ cd gromacs-2018.3-build-nompi med0127:~$ cmake ../gromacs-2018.3 \\ -DGMX_BUILD_OWN_FFTW = ON \\ -DGMX_MPI = OFF \\ -DGMX_SIMD = AVX_256 \\ -DCMAKE_INSTALL_PREFIX = $HOME /work/software/gromacs/2018.3 Second, here is how to configure the MPI flavour. Note that we are also enabling the openmpi module. We will also need the precise version here so we can later load the correct libraries. Note that we install the software into the directory gromacs-mpi but switch off shared library building as recommended by the Gromacs documentation. med0127:~$ module load openmpi/3.1.0-0 med0127:~$ module list Currently Loaded Modulefiles: 1 ) gcc/7.2.0-0 2 ) cmake/3.11.0-0 3 ) openmpi/4.0.3-0 med0127:~$ mkdir gromacs-2018.3-build-mpi med0127:~$ cd gromacs-2018.3-build-mpi med0127:~$ cmake ../gromacs-2018.3 \\ -DGMX_BUILD_OWN_FFTW = ON \\ -DGMX_MPI = ON \\ -DGMX_SIMD = AVX_256 \\ -DCMAKE_INSTALL_PREFIX = $HOME /work/software/gromacs-mpi/2018.3 \\ -DCMAKE_C_COMPILER = $( which mpicc ) \\ -DCMAKE_CXX_COMPILER = $( which mpicxx ) \\ -DBUILD_SHARED_LIBS = off Perform the Build and Install Steps \u00b6 This is simple, using -j 32 allows us to build with 32 threads. If something goes wrong: meh, the \"joys\" of compilling C software. Getting Support for Building Software BIH HPC IT cannot provide support for compiling scientific software. Please contact the appropriate mailing lists or forums for your scientific software. You should only contact the BIH HPC IT helpdesk only if you are sure that the problem is with the BIH HPC cluster. You should try to resolve the issue on your own and with the developers of the software that you are trying to build/use. For the no-MPI version: med0127:~$ cd ../cd gromacs-2018.3-build-nompi med0127:~$ make -j 32 [ ... ] med0127:~$ make install For the MPI version: med0127:~$ cd ../cd gromacs-2018.3-build-mpi med0127:~$ make -j 32 [ ... ] med0127:~$ make install Create Environment Modules Files \u00b6 For Gromacs 2018.3, the following is appropriate. You should be able to use this as a template for your environment module files: med0127:~$ mkdir -p $HOME /local/modules/gromacs med0127:~$ cat > $HOME /local/modules/gromacs/2018.3 << \"EOF\" #%Module proc ModulesHelp { } { puts stderr { Gromacs molecular simulation toolkit ( non-MPI version ) - http://www.gromacs.org } } module-whatis { Gromacs molecular simulation toolkit ( non-MPI )} set root /fast/users/YOURUSER/work/software/gromacs-mpi/2018.3 prereq gcc/7.2.0-0 conflict gromacs conflict gromacs-mpi prepend-path LD_LIBRARY_PATH $root /lib64 prepend-path LIBRARY_PATH $root /lib64 prepend-path MANPATH $root /share/man prepend-path PATH $root /bin setenv GMXRC $root /bin/GMXRC EOF med0127:~$ mkdir -p $HOME /local/modules/gromacs-mpi med0127:~$ cat > $HOME /local/modules/gromacs-mpi/2018.3 << \"EOF\" #%Module proc ModulesHelp { } { puts stderr { Gromacs molecular simulation toolkit ( MPI version ) - http://www.gromacs.org } } module-whatis { Gromacs molecular simulation toolkit ( MPI )} set root /fast/users/YOURUSER/work/software/gromacs-mpi/2018.3 prereq openmpi/4.0.3-0 prereq gcc/7.2.0-0 conflict gromacs conflict gromacs-mpi prepend-path LD_LIBRARY_PATH $root /lib64 prepend-path LIBRARY_PATH $root /lib64 prepend-path MANPATH $root /share/man prepend-path PATH $root /bin setenv GMXRC $root /bin/GMXRC EOF With the next command, make your local modules files path known to the environemtn modules system. med0127:~$ module use $HOME /local/modules You can verify the result: med0127:~$ module avail ------------------ /fast/users/YOURUSER/local/modules ------------------ gromacs/2018.3 gromacs-mpi/2018.3 -------------------- /usr/share/Modules/modulefiles -------------------- dot module-info null module-git modules use.own -------------------------- /opt/local/modules -------------------------- cmake/3.11.0-0 llvm/6.0.0-0 openmpi/3.1.0-0 gcc/7.2.0-0 matlab/r2016b-0 openmpi/4.0.3-0 Interlude: Convenient module use \u00b6 You can add this to your ~/.bashrc file to always execute the module use after login. Note that module is not available on the login or transfer nodes, the following should work fine: med0127:~$ cat >>~/.bashrc << \"EOF\" case \" ${ HOSTNAME } \" in med-login* | med-transfer* ) ;; * ) module use $HOME /local/modules ;; esac EOF Note that the paths chosen above are sensible but arbitrary. You can install any software anywhere you have permission to -- somewhere in your user and group home, maybe a project home makes most sense on the BIH HPC, no root permissions required. You can also place the module files anywhere, as long as the module use line is appropriate. As a best practice, you could use the following location: User-specific installation: $HOME/work/software as a root to install software to $HOME/work/software/$PKG/$VERSION for installing a given software package in a given version $HOME/work/software/modules as the root for modules to install $HOME/work/software/$PKG/$VERSION for the module file to load the software in a given version $HOME/work/software/modules.sh as a Bash script to contain the line module use $HOME/work/software/modules Group/project specific installation for a shared setup. Don't forget to give the group and yourself read permission only so you don't accidentally damage files after instalation ( chmod ug=rX,o= $GROUP/work/software , the upper case X is essential to only set +x on directories and not files): $GROUP/work/software as a root to install software to $GROUP/work/software/$PKG/$VERSION for installing a given software package in a given version $GROUP/work/software/modules as the root for modules to install $GROUP/work/software/$PKG/$VERSION for the module file to load the software in a given version $GROUP/work/software/modules.sh as a Bash script to contain the case Bash snippet from above but with module use $GROUP/work/software/modules This setup allows multiple users to provide software installations and share it with others. Going on with Gromacs \u00b6 Every time you want to use Gromacs, you can now do med0127:~$ module load gcc/7.2.0-0 gromacs/2018.3 or, if you want to have the MPI version: med0127:~$ module load gcc/7.2.0-0 openmpi/4.0.3-0 gromacs-mpi/2018.3 Launching Gromacs \u00b6 Something along the lines of the following job script should be appropriate. See How-To: Build Run OpenMPI Programs for more information. #!/bin/bash # Example job script for (single-threaded) MPI programs. # Generic arguments # Job name #SBATCH --job-name gromacs # Maximal running time of 10 min #SBATCH --time 00:10:00 # Allocate 1GB of memory per CPU #SBATCH --mem 1G # Write logs to directory \"slurm_log/<name>-<job id>.log\" (dir must exist) #SBATCH --output slurm_log/%x-%J.log # MPI-specific parameters # Launch on 8 nodes (== 8 tasks) #SBATCH --ntasks 8 # Allocate 4 CPUs per task (== per node) #SBATCH --cpus-per-task 4 # Load the OpenMPI and GCC environment module to get the runtime environment. module load gcc/4.7.0-0 module load openmpi/4.0.3-0 # Make custom environment modules known. Alternative, you can \"module use\" # them in the session you use for submitting the job. module use $HOME /local/modules module load gromacs-mpi/2018.3 # Launch the program on 8 nodes and tell Gromacs to use 4 threads for each # invocation. export OMP_NUM_THREADS = 4 mpirun -n 8 gmx_mpi mdrun -deffnm npt_1000 med0127:~$ mkdir slurm_log med0127:~$ sbatch job_script.sh Submitted batch job 3229","title":"Scientific Software"},{"location":"how-to/software/scientific-software/#how-to-install-custom-scientific-software","text":"This page gives an end-to-end example how to build and install Gromacs as an example for managing complex scientific software installs in user land. You don't have to learn or understand the specifics of Gromacs. We use it as an example as there are some actual users on the BIH cluster. However, installing it is out of scope of BIH HPC administration. Gromacs is a good example as it is a sufficiently complex piece of software. Quite some configuration is done on the command line and there is no current software package of it in the common RPM repositories. However, it is quite well-documented and easy to install for scientific software so there is a lot to be learned.","title":"How-To: Install Custom Scientific Software"},{"location":"how-to/software/scientific-software/#related-documents","text":"How-To: Build and Run OpenMPI Programs","title":"Related Documents"},{"location":"how-to/software/scientific-software/#steps-for-installing-scientific-software","text":"We will perform the following step: Download and extract the source of the software Configure the software (i.e., create the actual build system Makefile s) Compile the software Install the software Create environment module files so the software is easy to use Many scientific software packages will have more dependencies. If the dependencies are available as CentOS Core or EPEL packages (such as zlib), HPC IT administration can install them. However, otherwise you will have to install them on their own. Warning Do not perform the compilation on the login nodes but go to a compute node instead.","title":"Steps for Installing Scientific Software"},{"location":"how-to/software/scientific-software/#downloading-and-extracting-software","text":"This is best done in your scratch directory as we don't have to keep these files around for long. Note that the files in your scratch directory will automatically be removed after 4 weeks. You can also use your work directory here. med-login1:~$ srun --pty bash -i med0127:~$ mkdir $HOME /scratch/gromacs-install med0127:~$ cd $HOME /scratch/gromacs-install med0127:~$ wget http://ftp.gromacs.org/pub/gromacs/gromacs-2018.3.tar.gz med0127:~$ tar xf gromacs-2018.3.tar.gz med0127:~$ ls gromacs-2018.3 admin cmake COPYING CTestConfig.cmake INSTALL scripts src AUTHORS CMakeLists.txt CPackInit.cmake docs README share tests So far so good!","title":"Downloading and Extracting Software"},{"location":"how-to/software/scientific-software/#perform-the-configure-step","text":"This is the most critical step. Most scientific C/C++ software has a build step and allows for, e.g., disabling and enabling features or setting installation paths. Here, you can configure the software depending on your needs and environment. Also, it is the easiest step to mess up. Gromac's documentation is actually quite good but the author had problems to follow it to the letter. Gromacs recommends to create an MPI and a non-MPI build but the precise way did not work. This installation creates two flavours for Gromacs 2018.3, but in a different way than the Gromacs documentation proposes. First, here is how to configure the non-MPI flavour Gromacs wants a modern compiler, so we load gcc . We will need to note down the precise version we used so later we can load it for running Gromacs with the appropriate libraries. We will install gromacs into $HOME/work/software , which is appropriate for user-installed software, but it could also go into a group or project directory. Note that we install the software into your work directory as software installations are quite large and might go above your home quota. Also, software installations are usually not precious enough to waste resources on snapshots and backups. Also that we force Gromacs to use AVX_256 for SIMD support (Intel sandy bridge architecture) to not get unsupported CPU instruction errors. med0127:~$ module load gcc/7.2.0-0 cmake/3.11.0-0 med0127:~$ module list Currently Loaded Modulefiles: 1 ) gcc/7.2.0-0 2 ) cmake/3.11.0-0 med0127:~$ mkdir gromacs-2018.3-build-nompi med0127:~$ cd gromacs-2018.3-build-nompi med0127:~$ cmake ../gromacs-2018.3 \\ -DGMX_BUILD_OWN_FFTW = ON \\ -DGMX_MPI = OFF \\ -DGMX_SIMD = AVX_256 \\ -DCMAKE_INSTALL_PREFIX = $HOME /work/software/gromacs/2018.3 Second, here is how to configure the MPI flavour. Note that we are also enabling the openmpi module. We will also need the precise version here so we can later load the correct libraries. Note that we install the software into the directory gromacs-mpi but switch off shared library building as recommended by the Gromacs documentation. med0127:~$ module load openmpi/3.1.0-0 med0127:~$ module list Currently Loaded Modulefiles: 1 ) gcc/7.2.0-0 2 ) cmake/3.11.0-0 3 ) openmpi/4.0.3-0 med0127:~$ mkdir gromacs-2018.3-build-mpi med0127:~$ cd gromacs-2018.3-build-mpi med0127:~$ cmake ../gromacs-2018.3 \\ -DGMX_BUILD_OWN_FFTW = ON \\ -DGMX_MPI = ON \\ -DGMX_SIMD = AVX_256 \\ -DCMAKE_INSTALL_PREFIX = $HOME /work/software/gromacs-mpi/2018.3 \\ -DCMAKE_C_COMPILER = $( which mpicc ) \\ -DCMAKE_CXX_COMPILER = $( which mpicxx ) \\ -DBUILD_SHARED_LIBS = off","title":"Perform the Configure Step"},{"location":"how-to/software/scientific-software/#perform-the-build-and-install-steps","text":"This is simple, using -j 32 allows us to build with 32 threads. If something goes wrong: meh, the \"joys\" of compilling C software. Getting Support for Building Software BIH HPC IT cannot provide support for compiling scientific software. Please contact the appropriate mailing lists or forums for your scientific software. You should only contact the BIH HPC IT helpdesk only if you are sure that the problem is with the BIH HPC cluster. You should try to resolve the issue on your own and with the developers of the software that you are trying to build/use. For the no-MPI version: med0127:~$ cd ../cd gromacs-2018.3-build-nompi med0127:~$ make -j 32 [ ... ] med0127:~$ make install For the MPI version: med0127:~$ cd ../cd gromacs-2018.3-build-mpi med0127:~$ make -j 32 [ ... ] med0127:~$ make install","title":"Perform the Build and Install Steps"},{"location":"how-to/software/scientific-software/#create-environment-modules-files","text":"For Gromacs 2018.3, the following is appropriate. You should be able to use this as a template for your environment module files: med0127:~$ mkdir -p $HOME /local/modules/gromacs med0127:~$ cat > $HOME /local/modules/gromacs/2018.3 << \"EOF\" #%Module proc ModulesHelp { } { puts stderr { Gromacs molecular simulation toolkit ( non-MPI version ) - http://www.gromacs.org } } module-whatis { Gromacs molecular simulation toolkit ( non-MPI )} set root /fast/users/YOURUSER/work/software/gromacs-mpi/2018.3 prereq gcc/7.2.0-0 conflict gromacs conflict gromacs-mpi prepend-path LD_LIBRARY_PATH $root /lib64 prepend-path LIBRARY_PATH $root /lib64 prepend-path MANPATH $root /share/man prepend-path PATH $root /bin setenv GMXRC $root /bin/GMXRC EOF med0127:~$ mkdir -p $HOME /local/modules/gromacs-mpi med0127:~$ cat > $HOME /local/modules/gromacs-mpi/2018.3 << \"EOF\" #%Module proc ModulesHelp { } { puts stderr { Gromacs molecular simulation toolkit ( MPI version ) - http://www.gromacs.org } } module-whatis { Gromacs molecular simulation toolkit ( MPI )} set root /fast/users/YOURUSER/work/software/gromacs-mpi/2018.3 prereq openmpi/4.0.3-0 prereq gcc/7.2.0-0 conflict gromacs conflict gromacs-mpi prepend-path LD_LIBRARY_PATH $root /lib64 prepend-path LIBRARY_PATH $root /lib64 prepend-path MANPATH $root /share/man prepend-path PATH $root /bin setenv GMXRC $root /bin/GMXRC EOF With the next command, make your local modules files path known to the environemtn modules system. med0127:~$ module use $HOME /local/modules You can verify the result: med0127:~$ module avail ------------------ /fast/users/YOURUSER/local/modules ------------------ gromacs/2018.3 gromacs-mpi/2018.3 -------------------- /usr/share/Modules/modulefiles -------------------- dot module-info null module-git modules use.own -------------------------- /opt/local/modules -------------------------- cmake/3.11.0-0 llvm/6.0.0-0 openmpi/3.1.0-0 gcc/7.2.0-0 matlab/r2016b-0 openmpi/4.0.3-0","title":"Create Environment Modules Files"},{"location":"how-to/software/scientific-software/#interlude-convenient-module-use","text":"You can add this to your ~/.bashrc file to always execute the module use after login. Note that module is not available on the login or transfer nodes, the following should work fine: med0127:~$ cat >>~/.bashrc << \"EOF\" case \" ${ HOSTNAME } \" in med-login* | med-transfer* ) ;; * ) module use $HOME /local/modules ;; esac EOF Note that the paths chosen above are sensible but arbitrary. You can install any software anywhere you have permission to -- somewhere in your user and group home, maybe a project home makes most sense on the BIH HPC, no root permissions required. You can also place the module files anywhere, as long as the module use line is appropriate. As a best practice, you could use the following location: User-specific installation: $HOME/work/software as a root to install software to $HOME/work/software/$PKG/$VERSION for installing a given software package in a given version $HOME/work/software/modules as the root for modules to install $HOME/work/software/$PKG/$VERSION for the module file to load the software in a given version $HOME/work/software/modules.sh as a Bash script to contain the line module use $HOME/work/software/modules Group/project specific installation for a shared setup. Don't forget to give the group and yourself read permission only so you don't accidentally damage files after instalation ( chmod ug=rX,o= $GROUP/work/software , the upper case X is essential to only set +x on directories and not files): $GROUP/work/software as a root to install software to $GROUP/work/software/$PKG/$VERSION for installing a given software package in a given version $GROUP/work/software/modules as the root for modules to install $GROUP/work/software/$PKG/$VERSION for the module file to load the software in a given version $GROUP/work/software/modules.sh as a Bash script to contain the case Bash snippet from above but with module use $GROUP/work/software/modules This setup allows multiple users to provide software installations and share it with others.","title":"Interlude: Convenient module use"},{"location":"how-to/software/scientific-software/#going-on-with-gromacs","text":"Every time you want to use Gromacs, you can now do med0127:~$ module load gcc/7.2.0-0 gromacs/2018.3 or, if you want to have the MPI version: med0127:~$ module load gcc/7.2.0-0 openmpi/4.0.3-0 gromacs-mpi/2018.3","title":"Going on with Gromacs"},{"location":"how-to/software/scientific-software/#launching-gromacs","text":"Something along the lines of the following job script should be appropriate. See How-To: Build Run OpenMPI Programs for more information. #!/bin/bash # Example job script for (single-threaded) MPI programs. # Generic arguments # Job name #SBATCH --job-name gromacs # Maximal running time of 10 min #SBATCH --time 00:10:00 # Allocate 1GB of memory per CPU #SBATCH --mem 1G # Write logs to directory \"slurm_log/<name>-<job id>.log\" (dir must exist) #SBATCH --output slurm_log/%x-%J.log # MPI-specific parameters # Launch on 8 nodes (== 8 tasks) #SBATCH --ntasks 8 # Allocate 4 CPUs per task (== per node) #SBATCH --cpus-per-task 4 # Load the OpenMPI and GCC environment module to get the runtime environment. module load gcc/4.7.0-0 module load openmpi/4.0.3-0 # Make custom environment modules known. Alternative, you can \"module use\" # them in the session you use for submitting the job. module use $HOME /local/modules module load gromacs-mpi/2018.3 # Launch the program on 8 nodes and tell Gromacs to use 4 threads for each # invocation. export OMP_NUM_THREADS = 4 mpirun -n 8 gmx_mpi mdrun -deffnm npt_1000 med0127:~$ mkdir slurm_log med0127:~$ sbatch job_script.sh Submitted batch job 3229","title":"Launching Gromacs"},{"location":"how-to/software/singularity/","text":"Using Singularity (with Docker Images) \u00b6 Singularity ( https://sylabs.io/docs/ ) is a popular alternative to docker, because it does not require to run as a privileged user. Singualrity can run Docker images out-of-the-box by converting them to the singularity image format. The following guide gives a quick dive into using docker images with singularity. Quickstart \u00b6 Link ~/.singularity to ~/work/.singularity Because you only have a quota of 1 GB in your home directory, you should symlink ~/.singularity to ~/work/.singularity . host:~$ mkdir -p ~/work/.singularity && ln -sr ~/work/.singularity ~/.singularity In case you already have a singularity directory: host:~$ mv ~/.singularity ~/work/.singularity && ln -sr ~/work/.singularity ~/.singularity Run a bash in a docker image: host:~$ singularity bash docker://godlovedc/lolcow Run a command in a docker image: host:~$ singularity exec docker://godlovedc/lolcow echo \"hello, hello!\" Run a bash in a docker image, enable access to the cuda driver (--nv) and mount a path (--bind or -B): host:~$ singularity bash --nv --bind /path_on_host/:/path_inside_container/ docker://godlovedc/lolcow Some Caveats and Notes \u00b6 Caveats The default singularity images format (.sif) is read-only. By default singularity mounts /home/$USER, /tmp, and $PWD in the container. Notes Environment variables can be provided by setting them in the bash and adding the prefix SINGULARITYENV_ : host:~$ SINGULARITYENV_HELLO = 123 singularity bash docker://godlovedc/lolcow echo $HELLO Calling singularity shell or singularity exec uses as cwd the host callers cwd not the one set in the Dockerfile. One can change this by setting --pwd . Referencing/Providing Docker Images \u00b6 Option 1: Use Docker Images via Docker Hub \u00b6 The easiest variant to run a docker image available via a docker hub is by specifying its url. This causes singularity to download the image and convert it to a singularity image: host:~$ singularity run docker://godlovedc/lolcow or to open a shell inside the image host:~$ singularity bash docker://godlovedc/lolcow Furthermore, similar to docker, one can pull (and convert) remote image with the following call: host:~$ singularity pull docker://godlovedc/lolcow In case your registry requires authentication you can provide it via a prompt by adding the option --docker-login : host:~$ singularity pull --docker-login docker://ilumb/mylolcow or by setting the following environment variables: host:~$ export SINGULARITY_DOCKER_USERNAME = ilumb host:~$ export SINGULARITY_DOCKER_PASSWORD = <redacted> host:~$ singularity pull docker://ilumb/mylolcow More details can be found in the Singularity documentation . Option 2: Converting Docker Images \u00b6 Another option is to convert your docker image into the Singularity image format. This can be easily done using the docker images provided by docker2singularity . To convert the docker image docker_image_name to the singularity image singularity_image_name one can use the following command line. The output image will be located in output_directory_for_images . host:~$ docker run -v /var/run/docker.sock:/var/run/docker.sock --privileged -t --rm quay.io/singularity/docker2singularity -v /output_directory_for_images/:/output --name singularity_image_name docker_image_name The resulting image can then directly be used as image: host:~$ singularity bash singularity_image_name Conversion Compatibility \u00b6 Here are some tips for making Docker images compatible with Singularity taken from docker2singulrity : Define all environmental variables using the ENV instruction set. Do not rely on ~/.bashrc , ~/.profile , etc. Define an ENTRYPOINT instruction set pointing to the command line interface to your pipeline. Do not define CMD - rely only on ENTRYPOINT . You can interactively test the software inside the container by overriding the ENTRYPOINT docker run -i -t --entrypoint /bin/bash bids/example . Do not rely on being able to write anywhere other than the home folder and /scratch. Make sure your container runs with the --read-only --tmpfs /run --tmpfs /tmp parameters (this emulates the read-only behavior of Singularity). Don't rely on having elevated user permissions. Don't use the USER instruction set.","title":"Singularity"},{"location":"how-to/software/singularity/#using-singularity-with-docker-images","text":"Singularity ( https://sylabs.io/docs/ ) is a popular alternative to docker, because it does not require to run as a privileged user. Singualrity can run Docker images out-of-the-box by converting them to the singularity image format. The following guide gives a quick dive into using docker images with singularity.","title":"Using Singularity (with Docker Images)"},{"location":"how-to/software/singularity/#quickstart","text":"Link ~/.singularity to ~/work/.singularity Because you only have a quota of 1 GB in your home directory, you should symlink ~/.singularity to ~/work/.singularity . host:~$ mkdir -p ~/work/.singularity && ln -sr ~/work/.singularity ~/.singularity In case you already have a singularity directory: host:~$ mv ~/.singularity ~/work/.singularity && ln -sr ~/work/.singularity ~/.singularity Run a bash in a docker image: host:~$ singularity bash docker://godlovedc/lolcow Run a command in a docker image: host:~$ singularity exec docker://godlovedc/lolcow echo \"hello, hello!\" Run a bash in a docker image, enable access to the cuda driver (--nv) and mount a path (--bind or -B): host:~$ singularity bash --nv --bind /path_on_host/:/path_inside_container/ docker://godlovedc/lolcow","title":"Quickstart"},{"location":"how-to/software/singularity/#some-caveats-and-notes","text":"Caveats The default singularity images format (.sif) is read-only. By default singularity mounts /home/$USER, /tmp, and $PWD in the container. Notes Environment variables can be provided by setting them in the bash and adding the prefix SINGULARITYENV_ : host:~$ SINGULARITYENV_HELLO = 123 singularity bash docker://godlovedc/lolcow echo $HELLO Calling singularity shell or singularity exec uses as cwd the host callers cwd not the one set in the Dockerfile. One can change this by setting --pwd .","title":"Some Caveats and Notes"},{"location":"how-to/software/singularity/#referencingproviding-docker-images","text":"","title":"Referencing/Providing Docker Images"},{"location":"how-to/software/singularity/#option-1-use-docker-images-via-docker-hub","text":"The easiest variant to run a docker image available via a docker hub is by specifying its url. This causes singularity to download the image and convert it to a singularity image: host:~$ singularity run docker://godlovedc/lolcow or to open a shell inside the image host:~$ singularity bash docker://godlovedc/lolcow Furthermore, similar to docker, one can pull (and convert) remote image with the following call: host:~$ singularity pull docker://godlovedc/lolcow In case your registry requires authentication you can provide it via a prompt by adding the option --docker-login : host:~$ singularity pull --docker-login docker://ilumb/mylolcow or by setting the following environment variables: host:~$ export SINGULARITY_DOCKER_USERNAME = ilumb host:~$ export SINGULARITY_DOCKER_PASSWORD = <redacted> host:~$ singularity pull docker://ilumb/mylolcow More details can be found in the Singularity documentation .","title":"Option 1: Use Docker Images via Docker Hub"},{"location":"how-to/software/singularity/#option-2-converting-docker-images","text":"Another option is to convert your docker image into the Singularity image format. This can be easily done using the docker images provided by docker2singularity . To convert the docker image docker_image_name to the singularity image singularity_image_name one can use the following command line. The output image will be located in output_directory_for_images . host:~$ docker run -v /var/run/docker.sock:/var/run/docker.sock --privileged -t --rm quay.io/singularity/docker2singularity -v /output_directory_for_images/:/output --name singularity_image_name docker_image_name The resulting image can then directly be used as image: host:~$ singularity bash singularity_image_name","title":"Option 2: Converting Docker Images"},{"location":"how-to/software/singularity/#conversion-compatibility","text":"Here are some tips for making Docker images compatible with Singularity taken from docker2singulrity : Define all environmental variables using the ENV instruction set. Do not rely on ~/.bashrc , ~/.profile , etc. Define an ENTRYPOINT instruction set pointing to the command line interface to your pipeline. Do not define CMD - rely only on ENTRYPOINT . You can interactively test the software inside the container by overriding the ENTRYPOINT docker run -i -t --entrypoint /bin/bash bids/example . Do not rely on being able to write anywhere other than the home folder and /scratch. Make sure your container runs with the --read-only --tmpfs /run --tmpfs /tmp parameters (this emulates the read-only behavior of Singularity). Don't rely on having elevated user permissions. Don't use the USER instruction set.","title":"Conversion Compatibility"},{"location":"how-to/software/tensorflow/","text":"How-To: Setup TensorFlow \u00b6 Not yet updated to Slurm TODO: This still needs to be updated to Slurm. TensorFlow is a package for deep learning with optional support for GPUs. You can find the original TensorFlow installation instructions here . This article describes how to set up TensorFlow with GPU support using Conda. This how-to assumes that you have just connected to a GPU node via qrsh -P gpu -l gpu=1 (with -P gpu , the gpu SGE project gives you access to the GPU node and -l gpu=1 reserves the GPU resource) . At the time of writing, Tensorflow was available with GPU support from conda in version 1.2. You're welcome to update this (and any other) wiki page with new information. This is the only way to keep the Wiki up to date. This tutorial assumes, that conda has been set up as described in [Software Management]((../../best-practice/software-installation-with-conda). Create conda environment \u00b6 $ conda create -n cuda python=3.6 $ source activate cuda $ conda install tensorflow-gpu==1.12.0 Run TensorFlow example \u00b6 We now trying to run the example from the TensorFlow documentation. Note that the two export lines are important. ## these two lines are important $ export CUDA_HOME=$(dirname $(dirname $(which python))) $ export LD_LIBRARY_PATH=$CUDA_HOME/lib:$LD_LIBRARY_PATH $ python >>> import tensorflow as tf >>> hello = tf.constant('Hello, TensorFlow!') >>> sess = tf.Session() 2017-09-19 18:07:40.571996: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations. 2017-09-19 18:07:40.572090: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations. 2017-09-19 18:07:40.572129: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations. 2017-09-19 18:07:43.742453: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: name: Tesla K20Xm major: 3 minor: 5 memoryClockRate (GHz) 0.732 pciBusID 0000:04:00.0 Total memory: 5.57GiB Free memory: 5.50GiB 2017-09-19 18:07:43.983863: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x3b11f70 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that. 2017-09-19 18:07:43.985238: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 1 with properties: name: Tesla K20Xm major: 3 minor: 5 memoryClockRate (GHz) 0.732 pciBusID 0000:42:00.0 Total memory: 5.57GiB Free memory: 5.50GiB 2017-09-19 18:07:43.985403: I tensorflow/core/common_runtime/gpu/gpu_device.cc:832] Peer access not supported between device ordinals 0 and 1 2017-09-19 18:07:43.985440: I tensorflow/core/common_runtime/gpu/gpu_device.cc:832] Peer access not supported between device ordinals 1 and 0 2017-09-19 18:07:43.985498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 1 2017-09-19 18:07:43.985517: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0: Y N 2017-09-19 18:07:43.985533: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 1: N Y 2017-09-19 18:07:43.985657: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K20Xm, pci bus id: 0000:04:00.0) 2017-09-19 18:07:43.985683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla K20Xm, pci bus id: 0000:42:00.0) >>> print(sess.run(hello)) b'Hello, TensorFlow!' >>> a = tf.constant(10) >>> b = tf.constant(32) >>> print(sess.run(a + b)) 42 >>> exit() Check that you get the lines metioning Tesla K20Xm and the two lines mentioning gpu:0 and gpu:1 .","title":"Tensorflow"},{"location":"how-to/software/tensorflow/#how-to-setup-tensorflow","text":"Not yet updated to Slurm TODO: This still needs to be updated to Slurm. TensorFlow is a package for deep learning with optional support for GPUs. You can find the original TensorFlow installation instructions here . This article describes how to set up TensorFlow with GPU support using Conda. This how-to assumes that you have just connected to a GPU node via qrsh -P gpu -l gpu=1 (with -P gpu , the gpu SGE project gives you access to the GPU node and -l gpu=1 reserves the GPU resource) . At the time of writing, Tensorflow was available with GPU support from conda in version 1.2. You're welcome to update this (and any other) wiki page with new information. This is the only way to keep the Wiki up to date. This tutorial assumes, that conda has been set up as described in [Software Management]((../../best-practice/software-installation-with-conda).","title":"How-To: Setup TensorFlow"},{"location":"how-to/software/tensorflow/#create-conda-environment","text":"$ conda create -n cuda python=3.6 $ source activate cuda $ conda install tensorflow-gpu==1.12.0","title":"Create conda environment"},{"location":"how-to/software/tensorflow/#run-tensorflow-example","text":"We now trying to run the example from the TensorFlow documentation. Note that the two export lines are important. ## these two lines are important $ export CUDA_HOME=$(dirname $(dirname $(which python))) $ export LD_LIBRARY_PATH=$CUDA_HOME/lib:$LD_LIBRARY_PATH $ python >>> import tensorflow as tf >>> hello = tf.constant('Hello, TensorFlow!') >>> sess = tf.Session() 2017-09-19 18:07:40.571996: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations. 2017-09-19 18:07:40.572090: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations. 2017-09-19 18:07:40.572129: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations. 2017-09-19 18:07:43.742453: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: name: Tesla K20Xm major: 3 minor: 5 memoryClockRate (GHz) 0.732 pciBusID 0000:04:00.0 Total memory: 5.57GiB Free memory: 5.50GiB 2017-09-19 18:07:43.983863: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x3b11f70 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that. 2017-09-19 18:07:43.985238: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 1 with properties: name: Tesla K20Xm major: 3 minor: 5 memoryClockRate (GHz) 0.732 pciBusID 0000:42:00.0 Total memory: 5.57GiB Free memory: 5.50GiB 2017-09-19 18:07:43.985403: I tensorflow/core/common_runtime/gpu/gpu_device.cc:832] Peer access not supported between device ordinals 0 and 1 2017-09-19 18:07:43.985440: I tensorflow/core/common_runtime/gpu/gpu_device.cc:832] Peer access not supported between device ordinals 1 and 0 2017-09-19 18:07:43.985498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 1 2017-09-19 18:07:43.985517: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0: Y N 2017-09-19 18:07:43.985533: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 1: N Y 2017-09-19 18:07:43.985657: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K20Xm, pci bus id: 0000:04:00.0) 2017-09-19 18:07:43.985683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla K20Xm, pci bus id: 0000:42:00.0) >>> print(sess.run(hello)) b'Hello, TensorFlow!' >>> a = tf.constant(10) >>> b = tf.constant(32) >>> print(sess.run(a + b)) 42 >>> exit() Check that you get the lines metioning Tesla K20Xm and the two lines mentioning gpu:0 and gpu:1 .","title":"Run TensorFlow example"},{"location":"misc/external-resources/","text":"External Resources \u00b6 Basic Linux \u00b6 The BIH HPC uses CentOS Linux. A basic understanding of Linux is required. Even better, you should already have intermediate to advanced Linux/Unix skills. BIH HPC IT cannot provide you with basic Unix training. Please ask your home organization (e.g., Charite or MDC) to provide you with basic Linux training. That said, here are some resources that we find useful: Internet Tutorials \u00b6 There is a large number of Linux tutorials online including: Ryans Linux Tutorial Digital Ocean Tutorials Linux Basics Environment Variables Using Jupyter Notebooks to manage SLURM jobs Internet Forums \u00b6 Unix & Linux Stack Exchange Global Organisation for Bioinformatics Learning, Education, and Training \u00b6 GOBLET has a number of Bioinformatics-focused tutorials. This includes \"A Critical Guide to Unix\"","title":"External Guides"},{"location":"misc/external-resources/#external-resources","text":"","title":"External Resources"},{"location":"misc/external-resources/#basic-linux","text":"The BIH HPC uses CentOS Linux. A basic understanding of Linux is required. Even better, you should already have intermediate to advanced Linux/Unix skills. BIH HPC IT cannot provide you with basic Unix training. Please ask your home organization (e.g., Charite or MDC) to provide you with basic Linux training. That said, here are some resources that we find useful:","title":"Basic Linux"},{"location":"misc/external-resources/#internet-tutorials","text":"There is a large number of Linux tutorials online including: Ryans Linux Tutorial Digital Ocean Tutorials Linux Basics Environment Variables Using Jupyter Notebooks to manage SLURM jobs","title":"Internet Tutorials"},{"location":"misc/external-resources/#internet-forums","text":"Unix & Linux Stack Exchange","title":"Internet Forums"},{"location":"misc/external-resources/#global-organisation-for-bioinformatics-learning-education-and-training","text":"GOBLET has a number of Bioinformatics-focused tutorials. This includes \"A Critical Guide to Unix\"","title":"Global Organisation for Bioinformatics Learning, Education, and Training"},{"location":"misc/publication-list/","text":"Publication List \u00b6 The BIH Cluster is a valuable resource. It has been used to support the publications listed below. Please add your publications here. Acknowledge usage of the cluster in your manuscript as \"Computation has been performed on the HPC for Research cluster of the Berlin Institute of Health\" . Articles & Preprints \u00b6 2020 \u00b6 Ehmke, N.; Cusmano-Ozog, K.; Koenig, R.; Holtgrewe, M.; Nur, B.; Mihci, E.; Babcock, H.; Gonzaga-Jauregui, C.; Overton, J. D.; Xiao, J.; et al. Biallelic Variants in KYNU Cause a Multisystemic Syndrome with Hand Hyperphalangism. Bone 2020, 115219. https://doi.org/10.1016/j.bone.2019.115219 . 2019 \u00b6 Boeddrich A., Babila J.T., Wiglenda T., Diez L., Jacob M., Nietfeld W., Huska M.R., Haenig C., Groenke N., Buntru A., Blanc E., Meier J.C., Vannoni E., Erck C., Friedrich B., Martens H., Neuendorf N., Schnoegl S., Wolfer DP., Loos M., Beule D., Andrade-Navarro M.A., Wanker E.E. (2019). \"The Anti-amyloid Compound DO1 Decreases Plaque Pathology and Neuroinflammation-Related Expression Changes in 5xFAD Transgenic Mice.\" Cell Chem Biol. 2019 Jan 17;26(1):109-120.e7. doi: 10.1016/j.chembiol.2018.10.013 . Fountain M.D., Oleson, D.S., Rech. M.E., Segebrecht, L., Hunter, J.V., McCarthy, J.M., Lupo, P.J., Holtgrewe, M., Mora, R., Rosenfeld, J.A., Isidor, B., Le Caignec, C., Saenz, M.S., Pedersen, R.C., Morgen, T.M., Pfotenhauer, J.P., Xia, F., Bi, W., Kang, S.-H.L., Patel, A., Krantz, I.D., Raible, S.E., Smith, W.E., Cristian, I., Tori, E., Juusola, J., Millan, F., Wentzensen, I.M., Person, R.E., K\u00fcry, S., B\u00e9zieau, S., Uguen, K., F\u00e9rec, C., Munnich, A., van Haelst, M., Lichtenbelt, K.D., van Gassen, K., Hagelstrom, T., Chawla, A., Perry, D.L., Taft, R.J., Jones, M., Masser-Frye, D., Dyment, D., Venkateswaran, S., Li, C., Escobar, L,.F., Horn, D., Spillmann, R.C., Pe\u00f1a, L., Wierzba, J., Strom, T.M. Parent, I. Kaiser, F.J., Ehmke, N., Schaaf, C.P. (2019). \"Pathogenic variants in USP7 cause a neurodevelopmental disorder with speech delays, altered behavior, and neurologic anomalies.\" Genet. Med. 2019 Jan 25. doi: 10.1038/s41436-019-0433-1 Holtgrewe,M., Messerschmidt,C., Nieminen,M. and Beule,D. (2019) DigestiFlow: from BCL to FASTQ with ease. Bioinformatics, 10.1093/bioinformatics/btz850. K\u00e4fer S., Paraskevopoulou S., Zirkel F., Wieseke N., Donath A., Petersen M., Jones T.C., Liu S., Zhou X., Middendorf M., Junglen S., Misof B., Drosten C. (2019). \"Re-assessing the diversity of negative strand RNA viruses in insects.\" PLOS Pathogens 2019 Dec 12. doi: 10.1371/journal.ppat.1008224 K\u00fchnisch,J., Herbst,C., Al\u2010Wakeel\u2010Marquard,N., Dartsch,J., Holtgrewe,M., Baban,A., Mearini,G., Hardt,J., Kolokotronis,K., Gerull,B., et al. (2019) Targeted panel sequencing in pediatric primary cardiomyopathy supports a critical role of TNNI3. Clin Genet, 96, 549\u2013559. https://doi.org/10.1111/cge.13645 Marklewitz M., Dutari L.C., Paraskevopoulou S., Page R.A., Loaiza J.R., Junglen S. (2019). \"Diverse novel phleboviruses in sandflies from the Panama Canal area, Central Panama.\" Journal of General Virology 2019 May 3. doi: 10.1099/jgv.0.001260 Quade,A., Thiel,A., Kurth,I., Holtgrewe,M., Elbracht,M., Beule,D., Eggermann,K., Scholl,U.I. and H\u00e4usler,M. (2019) Paroxysmal tonic upgaze: A heterogeneous clinical condition responsive to carbonic anhydrase inhibition. European Journal of Paediatric Neurology, 10.1016/j.ejpn.2019.11.002 . 2018 \u00b6 Blanc, E., Holtgrewe, M., Dhamodaran, A., Messerschmidt, C., Willimsky, G., Blankenstein, T., Beule, D. (2018). \"Identification and Ranking of Recurrent Neo-Epitopes in Cancer\". bioRxiv . 2018/389437, 2018. doi: 10.1101/389437 Brandt, R., Uhlitz, F., Riemer, P., Giesecke, C., Schulze, S., El-Shimy, I.A., Fauler, B., Mielke, T., Mages, N., Herrmann, B.G., Sers, C., Bl\u00fcthgen, N., Morkel, M. (2018). \"Cell type-dependent differential activation of ERK by oncogenic KRAS or BRAF in the mouse intestinal epithelium\". bioRxiv . 2018/340844. doi: 10.1101/340844 . Holtgrewe, M., Knaus, A., Hildebrand, G., Pantel, J.-T., Rodriguesz de los Santos, M., Neveling, K., Goldmann, J., Schubach, M., J\u00e4ger, M., Couterier, M., Mundlos, S., Beule, D., Sperling, K., Krawitz, P. (2018). \"Multisite de novo mutations in human offspring after paternal exposure to ionizing radiation\", Nature Scientific Reports . 2018 Oct 2;8(1):14611. doi: 10.1038/s41598-018-33066-x . Kircher M., Xiong C., Martin B, Schubach M, Inoue F, Bell R.JA., Costello J.F., Shendure J., Ahituv N. (2018). \"Saturation mutagenesis of disease-associated regulatory elements.\" bioRxiv (2018): 505362. doi: 10.1101/505362 PCAWG Transcriptome Core Group, Calabrese, C., Davidson, N.R., Fonseca1, N.A., He, Y., Kahles, A., Lehmann, K.-V., Liu, F., Shiraishi, Y., Soulette, C.M., Urban, L., Demircio\u011flu, D., Greger, L., Li, S., Liu, D., Perry, M.D., Xiang, L., Zhang, F., Zhang, J., Bailey, P., Erkek, S., Hoadley, K.A., Hou, Y., Kilpinen, H., Korbel, J.O., Marin, M.G., Markowski, J., Nandi11, T., Pan-Hammarstr\u00f6m, Q., Pedamallu, C.S., Siebert, R., Stark, S.G., Su, H., Tan, P., Waszak, S.M., Yung, C., Zhu, S., PCAWG Transcriptome Working Group, Awadalla, P., Creighton, C.J., Meyerson, M., Ouellette, B.F.F., Wu, K., Yang, H., ICGC/TCGA Pan-Cancer Analysis of Whole Genomes Network, Brazma1, A., Brooks, A.N., G\u00f6ke, J., R\u00e4tsch, G., Schwarz, R.F., Stegle, O., Zhang, Z. (2018). \"Genomic basis for RNA alterations revealed by whole-genome analyses of 27 cancer types\". bioRxiv . 2018/183889. doi: 10.1101/183889 Guneykaya D., Ivanov A., Hernandez D.P., Haage V., Wojtas B., Meyer N., Maricos M., Jordan P., Buonfiglioli A., Gielniewski B., Ochocka N., C\u00f6mert, C., Friedrich, C., Artiles, L. S., Kaminska, B., Mertins, P., Beule, D., Kettenmann, H. (2018). \"Transcriptional and translational differences of microglia from male and female brains\", Cell reports . 2018 Sep 4;24(10):2773-83. doi: 10.1016/j.celrep.2018.08.001 . Rentzsch P, Witten D, Cooper GM, Shendure J, Kircher M. (2018). \"CADD: predicting the deleteriousness of variants throughout the human genome\", Nucleic Acids Res . 2018 Oct 29. doi: 10.1093/nar/gky1016 . Salatzki J., Foryst-Ludwig A., Bentele K., Blumrich A., Smeir E., Ban Z., Brix S., Grune J., Beyhoff N., Klopfleisch R., Dunst S., Surma, M.A., Klose, C., Rothe, M., Heinzel, F.R., Krannich, A., Kershaw, E.E., Beule, D., Schulze, P.C., Marx, N., Kintscher, U. (2018). \"Adipose tissue ATGL modifies the cardiac lipidome in pressure-overload-induced left ventricular failure\", PLoS genetics . 2018 Jan 10;14(1):e1007171. doi: 10.1371/journal.pgen.100717 . Schubach M., Re M., Robinson P.N., Valentini G. (2017) \"Imbalance-aware machine learning for predicting rare and common disease-associated non-coding variants\", Scientific reports 7:1, 2959. doi: 10.1038/s41598-017-03011-5 . Schubert M., Klinge, B., Kl\u00fcnemann M., Sieber A., Uhlitz F., Sauer S., Garnett M., Bl\u00fcthgen N., Saez-Rodriguez J. (2018). \"Perturbation-response genes reveal signaling footprints in cancer gene expression\". Nature Communications . 9: 20, 2018. doi: 10.1038/s41467-017-02391-6 2017 \u00b6 Euskirchen, P., Bielle, F., Labreche, K., Kloosterman, W.P., Rosenberg, S., Daniau, M., Schmitt, C., Masliah-Planchon, J., Bourdeaut, F., Dehais, C., et al. (2017). Same-day genomic and epigenomic diagnosis of brain tumors using real-time nanopore sequencing. Acta Neuropathol 1\u201313. doi: 10.1007/s00401-017-1743-5 Euskirchen, P., Radke, J., Schmidt, M.S., Heuling, E.S., Kadikowski, E., Maricos, M., Knab, F., Grittner, U., Zerbe, N., Czabanka, M., et al. (2017). Cellular heterogeneity contributes to subtype-specific expression of ZEB1 in human glioblastoma. PLOS ONE 12, e0185376. doi: 10.1371/journal.pone.0185376 Mattei D., Ivanov A., Ferrai C., Jordan P., Guneykaya D., Buonfiglioli A., Schaafsma W., Przanowski P., Deuther-Conrad W., Brust P., Hesse S., Patt, M., Sabri, O., Ross, T.L., Eggen, B.J.L., Boddeke E.W.G.M., Kaminska, B., Beule, D., Pombo, A., Kettenmann, H., Wolf, S.A. (2017). \"Maternal immune activation results in complex microglial transcriptome signature in the adult offspring that is reversed by minocycline treatment.\" Translational psychiatry . 2017 May;7(5):e1120. doi: 10.1038/tp.2017.80 . Mamlouk, S., Childs, L. H., Aust, D., Heim, D., Melching, F., Oliveira, C., Wolf, T., Durek, P., Schumacher, D., Bl\u00e4ker, H., von Winterfeld, M., Gastl, B., M\u00f6hr, K., Menne, A., Zeugner, S., Redmer, T., Lenze, D., Tierling, S., M\u00f6bs, M., Weichert, W., Folprecht, G., Blanc, E., Beule, D., Sch\u00e4fer, R., Morkel, M., Klauschen, F., Leser, U. and Sers, C. (2017). \"DNA copy number changes define spatial patterns of heterogeneity in colorectal cancer\", Nature Communications . 2017; 8, p. 14093. doi: 10.1038/ncomms14093 . Messerschmidt, C., Holtgrewe, M. and Beule, D. (2017). \"HLA-MA: simple yet powerful matching of samples using HLA typing results\". Bioinformatics . 28, pp. 2592\u20132599. doi: 10.1093/bioinformatics/btx132 . Kammertoens, T., Friese, C., Arina, A., Idel, C., Briesemeister, D., Rothe, M., Ivanov, A., Szymborska, A., Patone, G., Kunz, S., Sommermeyer, D., Engels, B., Leisegang, M., Textor, A., Fehling, H. J., Fruttiger, M., Lohoff, M., Herrmann, A., Yu, H., Weichselbaum, R., Uckert, W., H\u00fcbner, N., Gerhardt, H., Beule, D., Schreiber, H. and Blankenstein, T. (2017). \"Tumour ischaemia by interferon-\u03b3 resembles physiological blood vessel regression\". Nature . 545(7652), pp. 98\u2013102. doi: 10.1038/nature22311 . Schulze Heuling, E., Knab, F., Radke, J., Eskilsson, E., Martinez-Ledesma, E., Koch, A., Czabanka, M., Dieterich, C., Verhaak, R.G., Harms, C., et al. (2017). Prognostic Relevance of Tumor Purity and Interaction with MGMT Methylation in Glioblastoma. Mol. Cancer Res. 15, 532\u2013540. doi: 10.1158/1541-7786.MCR-16-0322 Yaakov, G., Lerner, D., Bentele, K., Steinberger, J., Barkai, N., Bigger, J., Maisonneuve, E., Gerdes, K., Lewis, K., Dhar, N., McKinney, J. D., Gefen, O., Balaban, N. Q., Jayaraman, R., Balaban, N. Q., Merrin, J., Chait, R., Kowalik, L., Leibler, S., Balaban, N. Q., Allison, K. R., Brynildsen, M. P., Collins, J. J., Nathan, C., Lewis, K., Glickman, M. S., Sawyers, Knoechel, B., Welch, A. Z., Gibney, P. A., Botstein, D., Koshland, D. E., Levy, S. F., Ziv, N., Siegal, M. L., Stewart-Ornstein, J., Weissman, J. S., El-Samad, H., Gasch, A. P., Weinert, T., Hartwell, L., Weinert, T. A., Hartwell, L. H., Lisby, M., Rothstein, R., Mortensen, U. H., Lisby, M., Mortensen, U. H., Rothstein, R., Domkin, V., Thelander, L., Chabes, A., Hendry, J. A., Tan, G., Ou, J., Boone, C., Brown, G. W., Berry, D. B., Gasch, A. P., Lynch, M., Nishant, K. T., Serero, A., Jubin, C., Loeillet, S., Legoix-Ne, P., Nicolas, A. G., Huh, W. K., Janke, C., Lee, S. E., Blecher-Gonen, R., Martin, M., Cherry, J. M., McKenna, A., DePristo, M. A., Lawrence, M., Obenchain, V., Ye, K., Schulz, M. H., Long, Q., Apweiler, R., Ning, Z., Layer, R. M., Chiang, C., Quinlan, A. R., Hall, I. M., Faust, G. G., Hall, I. M., Boeva, V., Boeva, V., Li, H., Koren, A., Soifer, I. and Barkai, N. (2017). \"Coupling phenotypic persistence to DNA damage increases genetic diversity in severe stress\". Nature Ecology & Evolution . 1(1), pp. 497\u2013500. doi: 10.1038/s41559-016-0016 . Uhlitz, F., Sieber, A., Wyler, E., Fritsche-Guenther, R., Meisig, J., Landthaler, M., Klinger, B., Bl\u00fcthgen, N. (2017). \"An immediate-late gene expression module decodes ERK signal duration\". Molecular Systems Biology . 13: 928, 2017. doi: 10.15252/msb.20177554 . Theses \u00b6 2019 \u00b6 Schumann F. (2019). \"Establishing a pipeline for stable mutational signature detection and evaluation of variant filter effects\". Freie Universit\u00e4t Berlin . Bachelor Thesis, Bioinformatics. 2018 \u00b6 Borgsm\u00fcller N. (2018). \"Optimization of data processing in GC-MS metabolomics\", Technische Universit\u00e4t Berlin . Master Thesis, Biotechnology. Kuchenbecker, S.-L. (2018). \"Analysis of Antigen Receptor Repertoires Captured by High Throughput Sequencing\". Freie Universit\u00e4t Universit\u00e4t Berlin . PhD Thesis, Dr. rer. nat. URN:NBN: urn:nbn:de:kobv:188-refubium-22171-8 Schubach M. (2018). \"Learning the Non-Coding Genome\", Freie Universit\u00e4t Universit\u00e4t Berlin . PhD Thesis, Dr. rer. nat. URN:NBN: urn:nbn:de:kobv:188-refubium-23332-7 Posters \u00b6 2018 \u00b6 Roskosch, S., Hald\u00f3rsson B., Kehr, B. (2018). \"PopDel: Population-Scale Detection of Genomic Deletions\" ECCB 2018. Poster. White T., Kehr B. (2018). \"Comprehensive extraction of structural variations from long-read DNA sequences\" WABI 2018. Poster. 2017 \u00b6 Schubach M., Re R., Robinson P.N., Valentini G. (2017). \"Variant relevance prediction in extremely imbalanced training sets\" ISMB/ECCB 2017. Poster. White T., Kehr B. (2017). \"Improving long-read mapping with simple lossy sequence transforms\" ISMB/ECCB 2017. Poster.","title":"Publication List"},{"location":"misc/publication-list/#publication-list","text":"The BIH Cluster is a valuable resource. It has been used to support the publications listed below. Please add your publications here. Acknowledge usage of the cluster in your manuscript as \"Computation has been performed on the HPC for Research cluster of the Berlin Institute of Health\" .","title":"Publication List"},{"location":"misc/publication-list/#articles-preprints","text":"","title":"Articles &amp; Preprints"},{"location":"misc/publication-list/#2020","text":"Ehmke, N.; Cusmano-Ozog, K.; Koenig, R.; Holtgrewe, M.; Nur, B.; Mihci, E.; Babcock, H.; Gonzaga-Jauregui, C.; Overton, J. D.; Xiao, J.; et al. Biallelic Variants in KYNU Cause a Multisystemic Syndrome with Hand Hyperphalangism. Bone 2020, 115219. https://doi.org/10.1016/j.bone.2019.115219 .","title":"2020"},{"location":"misc/publication-list/#2019","text":"Boeddrich A., Babila J.T., Wiglenda T., Diez L., Jacob M., Nietfeld W., Huska M.R., Haenig C., Groenke N., Buntru A., Blanc E., Meier J.C., Vannoni E., Erck C., Friedrich B., Martens H., Neuendorf N., Schnoegl S., Wolfer DP., Loos M., Beule D., Andrade-Navarro M.A., Wanker E.E. (2019). \"The Anti-amyloid Compound DO1 Decreases Plaque Pathology and Neuroinflammation-Related Expression Changes in 5xFAD Transgenic Mice.\" Cell Chem Biol. 2019 Jan 17;26(1):109-120.e7. doi: 10.1016/j.chembiol.2018.10.013 . Fountain M.D., Oleson, D.S., Rech. M.E., Segebrecht, L., Hunter, J.V., McCarthy, J.M., Lupo, P.J., Holtgrewe, M., Mora, R., Rosenfeld, J.A., Isidor, B., Le Caignec, C., Saenz, M.S., Pedersen, R.C., Morgen, T.M., Pfotenhauer, J.P., Xia, F., Bi, W., Kang, S.-H.L., Patel, A., Krantz, I.D., Raible, S.E., Smith, W.E., Cristian, I., Tori, E., Juusola, J., Millan, F., Wentzensen, I.M., Person, R.E., K\u00fcry, S., B\u00e9zieau, S., Uguen, K., F\u00e9rec, C., Munnich, A., van Haelst, M., Lichtenbelt, K.D., van Gassen, K., Hagelstrom, T., Chawla, A., Perry, D.L., Taft, R.J., Jones, M., Masser-Frye, D., Dyment, D., Venkateswaran, S., Li, C., Escobar, L,.F., Horn, D., Spillmann, R.C., Pe\u00f1a, L., Wierzba, J., Strom, T.M. Parent, I. Kaiser, F.J., Ehmke, N., Schaaf, C.P. (2019). \"Pathogenic variants in USP7 cause a neurodevelopmental disorder with speech delays, altered behavior, and neurologic anomalies.\" Genet. Med. 2019 Jan 25. doi: 10.1038/s41436-019-0433-1 Holtgrewe,M., Messerschmidt,C., Nieminen,M. and Beule,D. (2019) DigestiFlow: from BCL to FASTQ with ease. Bioinformatics, 10.1093/bioinformatics/btz850. K\u00e4fer S., Paraskevopoulou S., Zirkel F., Wieseke N., Donath A., Petersen M., Jones T.C., Liu S., Zhou X., Middendorf M., Junglen S., Misof B., Drosten C. (2019). \"Re-assessing the diversity of negative strand RNA viruses in insects.\" PLOS Pathogens 2019 Dec 12. doi: 10.1371/journal.ppat.1008224 K\u00fchnisch,J., Herbst,C., Al\u2010Wakeel\u2010Marquard,N., Dartsch,J., Holtgrewe,M., Baban,A., Mearini,G., Hardt,J., Kolokotronis,K., Gerull,B., et al. (2019) Targeted panel sequencing in pediatric primary cardiomyopathy supports a critical role of TNNI3. Clin Genet, 96, 549\u2013559. https://doi.org/10.1111/cge.13645 Marklewitz M., Dutari L.C., Paraskevopoulou S., Page R.A., Loaiza J.R., Junglen S. (2019). \"Diverse novel phleboviruses in sandflies from the Panama Canal area, Central Panama.\" Journal of General Virology 2019 May 3. doi: 10.1099/jgv.0.001260 Quade,A., Thiel,A., Kurth,I., Holtgrewe,M., Elbracht,M., Beule,D., Eggermann,K., Scholl,U.I. and H\u00e4usler,M. (2019) Paroxysmal tonic upgaze: A heterogeneous clinical condition responsive to carbonic anhydrase inhibition. European Journal of Paediatric Neurology, 10.1016/j.ejpn.2019.11.002 .","title":"2019"},{"location":"misc/publication-list/#2018","text":"Blanc, E., Holtgrewe, M., Dhamodaran, A., Messerschmidt, C., Willimsky, G., Blankenstein, T., Beule, D. (2018). \"Identification and Ranking of Recurrent Neo-Epitopes in Cancer\". bioRxiv . 2018/389437, 2018. doi: 10.1101/389437 Brandt, R., Uhlitz, F., Riemer, P., Giesecke, C., Schulze, S., El-Shimy, I.A., Fauler, B., Mielke, T., Mages, N., Herrmann, B.G., Sers, C., Bl\u00fcthgen, N., Morkel, M. (2018). \"Cell type-dependent differential activation of ERK by oncogenic KRAS or BRAF in the mouse intestinal epithelium\". bioRxiv . 2018/340844. doi: 10.1101/340844 . Holtgrewe, M., Knaus, A., Hildebrand, G., Pantel, J.-T., Rodriguesz de los Santos, M., Neveling, K., Goldmann, J., Schubach, M., J\u00e4ger, M., Couterier, M., Mundlos, S., Beule, D., Sperling, K., Krawitz, P. (2018). \"Multisite de novo mutations in human offspring after paternal exposure to ionizing radiation\", Nature Scientific Reports . 2018 Oct 2;8(1):14611. doi: 10.1038/s41598-018-33066-x . Kircher M., Xiong C., Martin B, Schubach M, Inoue F, Bell R.JA., Costello J.F., Shendure J., Ahituv N. (2018). \"Saturation mutagenesis of disease-associated regulatory elements.\" bioRxiv (2018): 505362. doi: 10.1101/505362 PCAWG Transcriptome Core Group, Calabrese, C., Davidson, N.R., Fonseca1, N.A., He, Y., Kahles, A., Lehmann, K.-V., Liu, F., Shiraishi, Y., Soulette, C.M., Urban, L., Demircio\u011flu, D., Greger, L., Li, S., Liu, D., Perry, M.D., Xiang, L., Zhang, F., Zhang, J., Bailey, P., Erkek, S., Hoadley, K.A., Hou, Y., Kilpinen, H., Korbel, J.O., Marin, M.G., Markowski, J., Nandi11, T., Pan-Hammarstr\u00f6m, Q., Pedamallu, C.S., Siebert, R., Stark, S.G., Su, H., Tan, P., Waszak, S.M., Yung, C., Zhu, S., PCAWG Transcriptome Working Group, Awadalla, P., Creighton, C.J., Meyerson, M., Ouellette, B.F.F., Wu, K., Yang, H., ICGC/TCGA Pan-Cancer Analysis of Whole Genomes Network, Brazma1, A., Brooks, A.N., G\u00f6ke, J., R\u00e4tsch, G., Schwarz, R.F., Stegle, O., Zhang, Z. (2018). \"Genomic basis for RNA alterations revealed by whole-genome analyses of 27 cancer types\". bioRxiv . 2018/183889. doi: 10.1101/183889 Guneykaya D., Ivanov A., Hernandez D.P., Haage V., Wojtas B., Meyer N., Maricos M., Jordan P., Buonfiglioli A., Gielniewski B., Ochocka N., C\u00f6mert, C., Friedrich, C., Artiles, L. S., Kaminska, B., Mertins, P., Beule, D., Kettenmann, H. (2018). \"Transcriptional and translational differences of microglia from male and female brains\", Cell reports . 2018 Sep 4;24(10):2773-83. doi: 10.1016/j.celrep.2018.08.001 . Rentzsch P, Witten D, Cooper GM, Shendure J, Kircher M. (2018). \"CADD: predicting the deleteriousness of variants throughout the human genome\", Nucleic Acids Res . 2018 Oct 29. doi: 10.1093/nar/gky1016 . Salatzki J., Foryst-Ludwig A., Bentele K., Blumrich A., Smeir E., Ban Z., Brix S., Grune J., Beyhoff N., Klopfleisch R., Dunst S., Surma, M.A., Klose, C., Rothe, M., Heinzel, F.R., Krannich, A., Kershaw, E.E., Beule, D., Schulze, P.C., Marx, N., Kintscher, U. (2018). \"Adipose tissue ATGL modifies the cardiac lipidome in pressure-overload-induced left ventricular failure\", PLoS genetics . 2018 Jan 10;14(1):e1007171. doi: 10.1371/journal.pgen.100717 . Schubach M., Re M., Robinson P.N., Valentini G. (2017) \"Imbalance-aware machine learning for predicting rare and common disease-associated non-coding variants\", Scientific reports 7:1, 2959. doi: 10.1038/s41598-017-03011-5 . Schubert M., Klinge, B., Kl\u00fcnemann M., Sieber A., Uhlitz F., Sauer S., Garnett M., Bl\u00fcthgen N., Saez-Rodriguez J. (2018). \"Perturbation-response genes reveal signaling footprints in cancer gene expression\". Nature Communications . 9: 20, 2018. doi: 10.1038/s41467-017-02391-6","title":"2018"},{"location":"misc/publication-list/#2017","text":"Euskirchen, P., Bielle, F., Labreche, K., Kloosterman, W.P., Rosenberg, S., Daniau, M., Schmitt, C., Masliah-Planchon, J., Bourdeaut, F., Dehais, C., et al. (2017). Same-day genomic and epigenomic diagnosis of brain tumors using real-time nanopore sequencing. Acta Neuropathol 1\u201313. doi: 10.1007/s00401-017-1743-5 Euskirchen, P., Radke, J., Schmidt, M.S., Heuling, E.S., Kadikowski, E., Maricos, M., Knab, F., Grittner, U., Zerbe, N., Czabanka, M., et al. (2017). Cellular heterogeneity contributes to subtype-specific expression of ZEB1 in human glioblastoma. PLOS ONE 12, e0185376. doi: 10.1371/journal.pone.0185376 Mattei D., Ivanov A., Ferrai C., Jordan P., Guneykaya D., Buonfiglioli A., Schaafsma W., Przanowski P., Deuther-Conrad W., Brust P., Hesse S., Patt, M., Sabri, O., Ross, T.L., Eggen, B.J.L., Boddeke E.W.G.M., Kaminska, B., Beule, D., Pombo, A., Kettenmann, H., Wolf, S.A. (2017). \"Maternal immune activation results in complex microglial transcriptome signature in the adult offspring that is reversed by minocycline treatment.\" Translational psychiatry . 2017 May;7(5):e1120. doi: 10.1038/tp.2017.80 . Mamlouk, S., Childs, L. H., Aust, D., Heim, D., Melching, F., Oliveira, C., Wolf, T., Durek, P., Schumacher, D., Bl\u00e4ker, H., von Winterfeld, M., Gastl, B., M\u00f6hr, K., Menne, A., Zeugner, S., Redmer, T., Lenze, D., Tierling, S., M\u00f6bs, M., Weichert, W., Folprecht, G., Blanc, E., Beule, D., Sch\u00e4fer, R., Morkel, M., Klauschen, F., Leser, U. and Sers, C. (2017). \"DNA copy number changes define spatial patterns of heterogeneity in colorectal cancer\", Nature Communications . 2017; 8, p. 14093. doi: 10.1038/ncomms14093 . Messerschmidt, C., Holtgrewe, M. and Beule, D. (2017). \"HLA-MA: simple yet powerful matching of samples using HLA typing results\". Bioinformatics . 28, pp. 2592\u20132599. doi: 10.1093/bioinformatics/btx132 . Kammertoens, T., Friese, C., Arina, A., Idel, C., Briesemeister, D., Rothe, M., Ivanov, A., Szymborska, A., Patone, G., Kunz, S., Sommermeyer, D., Engels, B., Leisegang, M., Textor, A., Fehling, H. J., Fruttiger, M., Lohoff, M., Herrmann, A., Yu, H., Weichselbaum, R., Uckert, W., H\u00fcbner, N., Gerhardt, H., Beule, D., Schreiber, H. and Blankenstein, T. (2017). \"Tumour ischaemia by interferon-\u03b3 resembles physiological blood vessel regression\". Nature . 545(7652), pp. 98\u2013102. doi: 10.1038/nature22311 . Schulze Heuling, E., Knab, F., Radke, J., Eskilsson, E., Martinez-Ledesma, E., Koch, A., Czabanka, M., Dieterich, C., Verhaak, R.G., Harms, C., et al. (2017). Prognostic Relevance of Tumor Purity and Interaction with MGMT Methylation in Glioblastoma. Mol. Cancer Res. 15, 532\u2013540. doi: 10.1158/1541-7786.MCR-16-0322 Yaakov, G., Lerner, D., Bentele, K., Steinberger, J., Barkai, N., Bigger, J., Maisonneuve, E., Gerdes, K., Lewis, K., Dhar, N., McKinney, J. D., Gefen, O., Balaban, N. Q., Jayaraman, R., Balaban, N. Q., Merrin, J., Chait, R., Kowalik, L., Leibler, S., Balaban, N. Q., Allison, K. R., Brynildsen, M. P., Collins, J. J., Nathan, C., Lewis, K., Glickman, M. S., Sawyers, Knoechel, B., Welch, A. Z., Gibney, P. A., Botstein, D., Koshland, D. E., Levy, S. F., Ziv, N., Siegal, M. L., Stewart-Ornstein, J., Weissman, J. S., El-Samad, H., Gasch, A. P., Weinert, T., Hartwell, L., Weinert, T. A., Hartwell, L. H., Lisby, M., Rothstein, R., Mortensen, U. H., Lisby, M., Mortensen, U. H., Rothstein, R., Domkin, V., Thelander, L., Chabes, A., Hendry, J. A., Tan, G., Ou, J., Boone, C., Brown, G. W., Berry, D. B., Gasch, A. P., Lynch, M., Nishant, K. T., Serero, A., Jubin, C., Loeillet, S., Legoix-Ne, P., Nicolas, A. G., Huh, W. K., Janke, C., Lee, S. E., Blecher-Gonen, R., Martin, M., Cherry, J. M., McKenna, A., DePristo, M. A., Lawrence, M., Obenchain, V., Ye, K., Schulz, M. H., Long, Q., Apweiler, R., Ning, Z., Layer, R. M., Chiang, C., Quinlan, A. R., Hall, I. M., Faust, G. G., Hall, I. M., Boeva, V., Boeva, V., Li, H., Koren, A., Soifer, I. and Barkai, N. (2017). \"Coupling phenotypic persistence to DNA damage increases genetic diversity in severe stress\". Nature Ecology & Evolution . 1(1), pp. 497\u2013500. doi: 10.1038/s41559-016-0016 . Uhlitz, F., Sieber, A., Wyler, E., Fritsche-Guenther, R., Meisig, J., Landthaler, M., Klinger, B., Bl\u00fcthgen, N. (2017). \"An immediate-late gene expression module decodes ERK signal duration\". Molecular Systems Biology . 13: 928, 2017. doi: 10.15252/msb.20177554 .","title":"2017"},{"location":"misc/publication-list/#theses","text":"","title":"Theses"},{"location":"misc/publication-list/#2019_1","text":"Schumann F. (2019). \"Establishing a pipeline for stable mutational signature detection and evaluation of variant filter effects\". Freie Universit\u00e4t Berlin . Bachelor Thesis, Bioinformatics.","title":"2019"},{"location":"misc/publication-list/#2018_1","text":"Borgsm\u00fcller N. (2018). \"Optimization of data processing in GC-MS metabolomics\", Technische Universit\u00e4t Berlin . Master Thesis, Biotechnology. Kuchenbecker, S.-L. (2018). \"Analysis of Antigen Receptor Repertoires Captured by High Throughput Sequencing\". Freie Universit\u00e4t Universit\u00e4t Berlin . PhD Thesis, Dr. rer. nat. URN:NBN: urn:nbn:de:kobv:188-refubium-22171-8 Schubach M. (2018). \"Learning the Non-Coding Genome\", Freie Universit\u00e4t Universit\u00e4t Berlin . PhD Thesis, Dr. rer. nat. URN:NBN: urn:nbn:de:kobv:188-refubium-23332-7","title":"2018"},{"location":"misc/publication-list/#posters","text":"","title":"Posters"},{"location":"misc/publication-list/#2018_2","text":"Roskosch, S., Hald\u00f3rsson B., Kehr, B. (2018). \"PopDel: Population-Scale Detection of Genomic Deletions\" ECCB 2018. Poster. White T., Kehr B. (2018). \"Comprehensive extraction of structural variations from long-read DNA sequences\" WABI 2018. Poster.","title":"2018"},{"location":"misc/publication-list/#2017_1","text":"Schubach M., Re R., Robinson P.N., Valentini G. (2017). \"Variant relevance prediction in extremely imbalanced training sets\" ISMB/ECCB 2017. Poster. White T., Kehr B. (2017). \"Improving long-read mapping with simple lossy sequence transforms\" ISMB/ECCB 2017. Poster.","title":"2017"},{"location":"misc/ssh-basics/","text":"SSH Basics \u00b6 This document gives an introduction to SSH for Non-Techies. What is SSH? \u00b6 SSH stands for **S**ecure **Sh**ell. It is a software that allows to establish a user-connection to a remote UNIX/Linux machine over the network and remote-control it from your local work-station. Let's say you have an HPC cluster with hundreds of machines somewhere in a remote data-center and you want to connect to those machines to issue commands and run jobs. Then you would use SSH. Getting Started \u00b6 Installation \u00b6 Simply install your distributions openssh-client package. You should be able to find plenty of good tutorials online. On Windows you can consider using MobaXterm (recommended) or Putty . Connecting \u00b6 Let's call your local machine the client and the remote machine you want to connect to the server. You will usually have some kind of connection information, like a hostname, IP address and perhaps a port number. Additionally, you should also have received your user-account information stating your user-name, your password, etc. Follow the instructions below to establish a remote terminal-session. If your are on Linux Open a terminal and issue the following command while replacing all the <...> fields with the actual data: # default port ssh <username>@<hostname-or-ip-address> # non-default port ssh <username>@<hostname-or-ip-address> -p <port-number> If you are on windows Start putty.exe , go into the Session category and fill out the form, then click the Connect button. Putty also allows to save the connection information in different profiles so you don't have to memorize and retype all fields every time you want to connect. SSH-Keys \u00b6 When you connect to a remote machine via SSH, you will be prompted for your password. This will happen every single time you connect and can feel a bit repetitive at times, especially if you feel that your password is hard to memorize. For those who don't want to type in their password every single time they connect, an alternative way of authentication is available. Meet SSH-Keys. It is possible to create an SSH-Key that can be used as a replacement for the password. Instead if being prompted for a password, SSH will simply use the key to authenticate. You can generate a new key by issuing: client:~$ ssh-keygen -t rsa -b 4096 # 1. Choose file in which to save the key *(leave blank for default)* # 2. Choose a passphrase of at least five characters How do SSH-Keys work? \u00b6 An SSH-Key consists of two files, a private-key-file and a public-key-file. The public key can then be installed on an arbitrary amount of remote machines. If a server with the public key receives a connection from a client with the correct private key, access is granted without having to type a password. Passphrase \u00b6 The security problem with SSH keys is that anyone with access to the private key has full access to all machines that have the public key installed. Loosing the key or getting it compromised in another way imposes a serious security threat. Therefore, it is best to secure the private key with a passphrase. This passphrase is needed to unlock and use the private key. Once you have your key-pair generated, you can easily change the passphrase of that key by issuing: client:~$ ssh-keygen -p SSH-Agent \u00b6 In order to avoid having to type the passphrase of the key every time we want to use it, the key can be loaded into an SSH-Agent. For instance, if you have connected to a login-node via Putty and want to unlock your private key in order to be able to access cluster nodes, you cant configure the SSH-Agent. client:~$ source < ( ssh-agent ) (The above command will load the required environment variables of the SSH-Agent into your shell environment, effectively making the agent available for your consumption.) Next, you can load your private key: client:~$ ssh-add (You will be prompted for the passphrase of the key) You can verify that the agent is running and your key is loaded by issuing: client:~$ ssh-add -l # 'l' as in list-all-loaded-keys (The command should print at least one key, showing the key-size, the hash of the key-fingerprint and the location of the file in the file-system.) Since all home-directories are shared across the entire cluster and you created your key-pair inside your home-directory, you public-key (which is also in your home-directory) is automatically installed on all other cluster nodes, immediately. Try connecting to any cluster node. It should not prompt your for a password. There is nothing you have to do to \"unload\" or \"lock\" the key-file. Simply disconnect.","title":"SSH Basics"},{"location":"misc/ssh-basics/#ssh-basics","text":"This document gives an introduction to SSH for Non-Techies.","title":"SSH Basics"},{"location":"misc/ssh-basics/#what-is-ssh","text":"SSH stands for **S**ecure **Sh**ell. It is a software that allows to establish a user-connection to a remote UNIX/Linux machine over the network and remote-control it from your local work-station. Let's say you have an HPC cluster with hundreds of machines somewhere in a remote data-center and you want to connect to those machines to issue commands and run jobs. Then you would use SSH.","title":"What is SSH?"},{"location":"misc/ssh-basics/#getting-started","text":"","title":"Getting Started"},{"location":"misc/ssh-basics/#installation","text":"Simply install your distributions openssh-client package. You should be able to find plenty of good tutorials online. On Windows you can consider using MobaXterm (recommended) or Putty .","title":"Installation"},{"location":"misc/ssh-basics/#connecting","text":"Let's call your local machine the client and the remote machine you want to connect to the server. You will usually have some kind of connection information, like a hostname, IP address and perhaps a port number. Additionally, you should also have received your user-account information stating your user-name, your password, etc. Follow the instructions below to establish a remote terminal-session. If your are on Linux Open a terminal and issue the following command while replacing all the <...> fields with the actual data: # default port ssh <username>@<hostname-or-ip-address> # non-default port ssh <username>@<hostname-or-ip-address> -p <port-number> If you are on windows Start putty.exe , go into the Session category and fill out the form, then click the Connect button. Putty also allows to save the connection information in different profiles so you don't have to memorize and retype all fields every time you want to connect.","title":"Connecting"},{"location":"misc/ssh-basics/#ssh-keys","text":"When you connect to a remote machine via SSH, you will be prompted for your password. This will happen every single time you connect and can feel a bit repetitive at times, especially if you feel that your password is hard to memorize. For those who don't want to type in their password every single time they connect, an alternative way of authentication is available. Meet SSH-Keys. It is possible to create an SSH-Key that can be used as a replacement for the password. Instead if being prompted for a password, SSH will simply use the key to authenticate. You can generate a new key by issuing: client:~$ ssh-keygen -t rsa -b 4096 # 1. Choose file in which to save the key *(leave blank for default)* # 2. Choose a passphrase of at least five characters","title":"SSH-Keys"},{"location":"misc/ssh-basics/#how-do-ssh-keys-work","text":"An SSH-Key consists of two files, a private-key-file and a public-key-file. The public key can then be installed on an arbitrary amount of remote machines. If a server with the public key receives a connection from a client with the correct private key, access is granted without having to type a password.","title":"How do SSH-Keys work?"},{"location":"misc/ssh-basics/#passphrase","text":"The security problem with SSH keys is that anyone with access to the private key has full access to all machines that have the public key installed. Loosing the key or getting it compromised in another way imposes a serious security threat. Therefore, it is best to secure the private key with a passphrase. This passphrase is needed to unlock and use the private key. Once you have your key-pair generated, you can easily change the passphrase of that key by issuing: client:~$ ssh-keygen -p","title":"Passphrase"},{"location":"misc/ssh-basics/#ssh-agent","text":"In order to avoid having to type the passphrase of the key every time we want to use it, the key can be loaded into an SSH-Agent. For instance, if you have connected to a login-node via Putty and want to unlock your private key in order to be able to access cluster nodes, you cant configure the SSH-Agent. client:~$ source < ( ssh-agent ) (The above command will load the required environment variables of the SSH-Agent into your shell environment, effectively making the agent available for your consumption.) Next, you can load your private key: client:~$ ssh-add (You will be prompted for the passphrase of the key) You can verify that the agent is running and your key is loaded by issuing: client:~$ ssh-add -l # 'l' as in list-all-loaded-keys (The command should print at least one key, showing the key-size, the hash of the key-fingerprint and the location of the file in the file-system.) Since all home-directories are shared across the entire cluster and you created your key-pair inside your home-directory, you public-key (which is also in your home-directory) is automatically installed on all other cluster nodes, immediately. Try connecting to any cluster node. It should not prompt your for a password. There is nothing you have to do to \"unload\" or \"lock\" the key-file. Simply disconnect.","title":"SSH-Agent"},{"location":"misc/tips/","text":"Miscellaneous Tips \u00b6 This page collects some useful tips. Interpreting core Files \u00b6 A core file or core dump is a file that records the memory image of a running process and its process status (register values etc.). Its primary use is post-mortem debugging of a program that crashed while it ran outside a debugger. A program that crashes automatically produces a core file, unless this feature is disabled by the user. -- https://sourceware.org/gdb/onlinedocs/gdb/Core-File-Generation.html Quickstart This topic is too large to be discussed here (see the gdb and Wikipedia links). If you just want to find out what caused the crash, here is how with file : $ file core.111506 core.111506: ELF 64 -bit LSB core file x86-64, version 1 ( SYSV ) , SVR4-style, from '/fast/users/szyskam_c/work/miniconda/envs/fastq_search-env/bin/python3.7 -m sna' Also See https://en.wikipedia.org/wiki/Core_dump","title":"Misc. Tips"},{"location":"misc/tips/#miscellaneous-tips","text":"This page collects some useful tips.","title":"Miscellaneous Tips"},{"location":"misc/tips/#interpreting-core-files","text":"A core file or core dump is a file that records the memory image of a running process and its process status (register values etc.). Its primary use is post-mortem debugging of a program that crashed while it ran outside a debugger. A program that crashes automatically produces a core file, unless this feature is disabled by the user. -- https://sourceware.org/gdb/onlinedocs/gdb/Core-File-Generation.html Quickstart This topic is too large to be discussed here (see the gdb and Wikipedia links). If you just want to find out what caused the crash, here is how with file : $ file core.111506 core.111506: ELF 64 -bit LSB core file x86-64, version 1 ( SYSV ) , SVR4-style, from '/fast/users/szyskam_c/work/miniconda/envs/fastq_search-env/bin/python3.7 -m sna' Also See https://en.wikipedia.org/wiki/Core_dump","title":"Interpreting core Files"},{"location":"overview/architecture/","text":"Cluster Architecture \u00b6 BIH HPC IT provides acess to a high-performance compute (HPC) cluster system. A cluster system bundles a high number of nodes and in the case of HPC, the focus is on performance (with contrast to high availability clusters). The BIH HPC system consists of the following hardware: approx. 256 nodes (from three generations), 4 high-memory nodes (2 nodes with 512 GB RAM, 2 nodes with 1 TB RAM), 4 GPU nodes (with 4 Tesla GPUs each), and a high-perfomance parallel GPFS files system. Network Interconnect \u00b6 All nodes are connected with 2x10GbE, 32 nodes provide Infiniband interconnect (lower latency, but MPI library required). Cluster Management \u00b6 Users don't connect to nodes directly but rather create interactive or batch jobs to be executed by the cluster job scheduler Slurm . Interactive jobs open interactive sessions on compute nodes (e.g., R or iPython sessions). These jobs are run directly in the user's terminal. Batch jobs consist a job script with execution instructions (a name, resource requirements etc.) These are submitted to the cluster and then assigned to compute hosts by the job scheduler. Users can configure the scheduler to send them an email upon completion. Users can submit many batch jobs at the same time and the scheduler will execute them once the cluster offers sufficient resources. Head vs. Compute Nodes \u00b6 As common with HPC systems, users cannot directly access the compute nodes but rather connect to so-called head nodes . The BIH HPC system provides the following head nodes: med-login1 and med-login2 that accept SSH connections and are meant for low intensity , interactive work such as editing files, running screen/tmux sessions, and logging into the compute nodes. Users should run no computational tasks and no large-scale data transfer on these nodes. med-transfer1 and med-transfer2 also accept SSH connections. Users should run all large-scale data transfer through these nodes. Common Use Case \u00b6 After registration and client configurations, users with typically connect to the HPC system through the login nodes: local:~$ ssh -l jdoe_c med-login1.bihealth.org med-login1:~$ Subsequently, they might submit batch jobs to the cluster for execution through the Slurm scheduling system or open interactive sessions: med-login1:~$ sbatch job_script.sh med-login1:~$ srun --pty bash -i med0104:~$","title":"Architecture"},{"location":"overview/architecture/#cluster-architecture","text":"BIH HPC IT provides acess to a high-performance compute (HPC) cluster system. A cluster system bundles a high number of nodes and in the case of HPC, the focus is on performance (with contrast to high availability clusters). The BIH HPC system consists of the following hardware: approx. 256 nodes (from three generations), 4 high-memory nodes (2 nodes with 512 GB RAM, 2 nodes with 1 TB RAM), 4 GPU nodes (with 4 Tesla GPUs each), and a high-perfomance parallel GPFS files system.","title":"Cluster Architecture"},{"location":"overview/architecture/#network-interconnect","text":"All nodes are connected with 2x10GbE, 32 nodes provide Infiniband interconnect (lower latency, but MPI library required).","title":"Network Interconnect"},{"location":"overview/architecture/#cluster-management","text":"Users don't connect to nodes directly but rather create interactive or batch jobs to be executed by the cluster job scheduler Slurm . Interactive jobs open interactive sessions on compute nodes (e.g., R or iPython sessions). These jobs are run directly in the user's terminal. Batch jobs consist a job script with execution instructions (a name, resource requirements etc.) These are submitted to the cluster and then assigned to compute hosts by the job scheduler. Users can configure the scheduler to send them an email upon completion. Users can submit many batch jobs at the same time and the scheduler will execute them once the cluster offers sufficient resources.","title":"Cluster Management"},{"location":"overview/architecture/#head-vs-compute-nodes","text":"As common with HPC systems, users cannot directly access the compute nodes but rather connect to so-called head nodes . The BIH HPC system provides the following head nodes: med-login1 and med-login2 that accept SSH connections and are meant for low intensity , interactive work such as editing files, running screen/tmux sessions, and logging into the compute nodes. Users should run no computational tasks and no large-scale data transfer on these nodes. med-transfer1 and med-transfer2 also accept SSH connections. Users should run all large-scale data transfer through these nodes.","title":"Head vs. Compute Nodes"},{"location":"overview/architecture/#common-use-case","text":"After registration and client configurations, users with typically connect to the HPC system through the login nodes: local:~$ ssh -l jdoe_c med-login1.bihealth.org med-login1:~$ Subsequently, they might submit batch jobs to the cluster for execution through the Slurm scheduling system or open interactive sessions: med-login1:~$ sbatch job_script.sh med-login1:~$ srun --pty bash -i med0104:~$","title":"Common Use Case"},{"location":"overview/for-the-impatient/","text":"For the Impatient \u00b6 This document describes the fundamentals of using the BIH cluster. More detailed documentation is available in the rest of the Wiki. Start exploring from the Wiki home . Cluster Hardware and Scheduling \u00b6 The cluster consists of the following major components: 2 login nodes for users med-login1 and med-login2 (for interactive sessions only), 2 nodes for file transfers med-transfer1 and med-transfer2 , a scheduling system using Slurm, approximately 200 general purpose compute nodes med01XX , med02XX , med05XX , med06XX , med07XX . a few high memory nodes med040[1-4] , 4 nodes with 4 Tesla GPUs each (!) med030[1-4] , a high-performance, parallel GPFS file system with 2.1 PB, by DDN mounted at /fast , a slower \"classic\" ZFS file system available through NFS with ~250 TB mounted at /slow . This is shown by the following picture: Differences Between Workstations and Clusters \u00b6 The differences include: The directly reachable login nodes are not meant for computation! Use srun to go to a compute node. Every time you type srun to go to a compute node you might end up on a different host. Most directories on the nodes are not shared, including /tmp . The /fast directory is shared throughout the cluster which contains your home, group home, and project directories. You will not get root or sudo permissions on the cluster. You should use batch jobs ( sbatch ) over calling programs interactively. What the Cluster Is and Is NOT \u00b6 NB: the following might sound a bit harsh but is written with everyone's best intentions in mind (we actually like you, our user!) This addresses a lot of suboptimal (yet not dangerous, of course) points we observed in our users. IT IS It is scientific infrastructure just like a lab workbench or miscroscope. It is there to be used for you and your science. We trust you to behave in a collaboratively. We will monitor usage, though, and call out offenders. With its ~200 nodes, ~6400 threads and fast parallel I/O, it is a powerful resource for life science high performance computation, originally optimized at bioinformatics sequence processing. A place for data move data at the beginning of your project. By definition, every project has an end. Your project data needs to leave the cluster at the end of the project. A collaborative resource with central administration managed by BIH HPC IT and supported via hpc-helpdesk@bihealth.de IT IS NOT A self-administrated workstation or servers. You will not get sudo . We will not install software beyond those in broad use and available in CentOS Core or EPEL repositories. You can install software in your user/group/project directories, for example using Conda. A place to store primary copies of your data. You only get 1 GB of storage in your home for scripts, configuration, and documents. A safe place to store data. Only your 1 GB of home is in snapshots and backup. While data is stored on redundant disks, technical or administrative failure might eventually lead to data loss. We do everything humanly possible to prevent this. Despite this, it is your responsibility to keep important files in the snapshot/backup protected home, ideally even in copy (e.g., a git repository) elsewhere. Also, keeping safe copies of primary data files, your published results, and the steps in between reproducible is your responsibility. A place to store data indefinitely. The fast GPFS storage is expensive and \"sparse\" in a way. The general workflow is: (1) copy data to cluster, (2) process it, creating intermediate and final results, (3) copy data elsewhere and remove it from the cluster Generally suitable for primary software development. The I/O system might get overloaded and saving scripts might take some time. We know of people who do this and it works for them. Your mileage might vary. Locations on the Cluster \u00b6 Your home directory is located in /fast/users/$USER . Your home is for scripts, source code, and configuration only. Use your work directory for large files. The quota in the home directory is 1 GB but we have nightly snapshots and backups thereof. Your work directory is located in /fast/users/$USER/work . This is where you should place large files. Files in this location do not have snapshots or backups. The directory (actually a GPFS file set) /fast/users/$USER/scratch should be used for temporary data. All data placed there will be removed after 4 weeks. If you are part of an AG/lab working on the cluster, the group directory is in /fast/groups/$AG . Projects are located in /fast/projects/$PROJECT . So-called dot files/directories filling up your home? Files and directories starting with a dot \" . \" are not shown with the \"ls\" command. May users run into problems with directories such as $HOME/.local but also non-dot directories such as $HOME/R filling up their storage. You should move such large directories to your work volume and only keep a symlink in your $HOME . Here is how you find large directories: host:~$ du -shc ~/.??* ~/* Here is how you move them to your work and replace them with a symlink: host:~$ mv ~/.local ~/work/.local host:~$ ln -s ~/work/.local ~/.local Temporary Directories \u00b6 Note that you also have access to /tmp on the individual nodes but the disk is small and might be a slow spinning disk. If you are processing large NGS data, we recommend you create /fast/users/$USER/scratch/tmp and set the environment variable TMPDIR to point there. However, for creating locks special Unix files such as sockets or fifos, /tmp is the right place. Note that files placed in your scratch directory will be removed automatically after 4 weeks. Do not place any valueable files in there. First Steps on the Cluster \u00b6 Connecting to the Cluster \u00b6 From the Charite, MDC, and BIH networks, you can connect to the cluster login nodes med-login{1,2}.bihealth.org . For Charite users, your name is ${USER}_c , for MDC users, your account is ${USER}_m where $USER is the login name of your primary location. From the outside, for MDC users , the cluster is accessible via ssh1.mdc-berlin.de (you need to enable SSH key agent forwarding for this) Note that you have to use your MDC user name (without any suffix _m ) for connecting to this host. Also note that BIH HPC IT does not have control over ssh1.mdc-berlin.de . You have to contact MDC IT in case of any issues. From the outside, for Charite users, there is no SSH hop node. Instead, you have to apply for VPN through Charite Gesch\u00e4ftsbereich IT. You can use this form availble in Charite Intranet for this. Please refer to the Charite intranet or helpdesk@charite.de for more information. Connecting to Compute Node through Login Node \u00b6 After logging into the cluster, you are on the login node med-login<X> ( <X> can be either 1 or 2 ). When transferring files, use the med-transfer1 or med-transfer2 nodes. You should not do computation or other work on the login or file transfer nodes, but use the compute nodes instead. Typically, you'll create an interactive session on a compute node using the srun command. Submitting Jobs \u00b6 While not recommended, you can perform computations (such as using BWA) in the interactive session. However, when the connection is interrupted, your computation process will be stopped. It is therefore recommended you submit jobs using the sbatch command (or use screen or tmux ). Inspecting Jobs and the Cluster \u00b6 You can inspect your currently running jobs with squeue , and kill them using scancel . You can inspect jobs that have finished with sacct , and see the cluster nodes using sinfo .","title":"For the Impatient"},{"location":"overview/for-the-impatient/#for-the-impatient","text":"This document describes the fundamentals of using the BIH cluster. More detailed documentation is available in the rest of the Wiki. Start exploring from the Wiki home .","title":"For the Impatient"},{"location":"overview/for-the-impatient/#cluster-hardware-and-scheduling","text":"The cluster consists of the following major components: 2 login nodes for users med-login1 and med-login2 (for interactive sessions only), 2 nodes for file transfers med-transfer1 and med-transfer2 , a scheduling system using Slurm, approximately 200 general purpose compute nodes med01XX , med02XX , med05XX , med06XX , med07XX . a few high memory nodes med040[1-4] , 4 nodes with 4 Tesla GPUs each (!) med030[1-4] , a high-performance, parallel GPFS file system with 2.1 PB, by DDN mounted at /fast , a slower \"classic\" ZFS file system available through NFS with ~250 TB mounted at /slow . This is shown by the following picture:","title":"Cluster Hardware and Scheduling"},{"location":"overview/for-the-impatient/#differences-between-workstations-and-clusters","text":"The differences include: The directly reachable login nodes are not meant for computation! Use srun to go to a compute node. Every time you type srun to go to a compute node you might end up on a different host. Most directories on the nodes are not shared, including /tmp . The /fast directory is shared throughout the cluster which contains your home, group home, and project directories. You will not get root or sudo permissions on the cluster. You should use batch jobs ( sbatch ) over calling programs interactively.","title":"Differences Between Workstations and Clusters"},{"location":"overview/for-the-impatient/#what-the-cluster-is-and-is-not","text":"NB: the following might sound a bit harsh but is written with everyone's best intentions in mind (we actually like you, our user!) This addresses a lot of suboptimal (yet not dangerous, of course) points we observed in our users. IT IS It is scientific infrastructure just like a lab workbench or miscroscope. It is there to be used for you and your science. We trust you to behave in a collaboratively. We will monitor usage, though, and call out offenders. With its ~200 nodes, ~6400 threads and fast parallel I/O, it is a powerful resource for life science high performance computation, originally optimized at bioinformatics sequence processing. A place for data move data at the beginning of your project. By definition, every project has an end. Your project data needs to leave the cluster at the end of the project. A collaborative resource with central administration managed by BIH HPC IT and supported via hpc-helpdesk@bihealth.de IT IS NOT A self-administrated workstation or servers. You will not get sudo . We will not install software beyond those in broad use and available in CentOS Core or EPEL repositories. You can install software in your user/group/project directories, for example using Conda. A place to store primary copies of your data. You only get 1 GB of storage in your home for scripts, configuration, and documents. A safe place to store data. Only your 1 GB of home is in snapshots and backup. While data is stored on redundant disks, technical or administrative failure might eventually lead to data loss. We do everything humanly possible to prevent this. Despite this, it is your responsibility to keep important files in the snapshot/backup protected home, ideally even in copy (e.g., a git repository) elsewhere. Also, keeping safe copies of primary data files, your published results, and the steps in between reproducible is your responsibility. A place to store data indefinitely. The fast GPFS storage is expensive and \"sparse\" in a way. The general workflow is: (1) copy data to cluster, (2) process it, creating intermediate and final results, (3) copy data elsewhere and remove it from the cluster Generally suitable for primary software development. The I/O system might get overloaded and saving scripts might take some time. We know of people who do this and it works for them. Your mileage might vary.","title":"What the Cluster Is and Is NOT"},{"location":"overview/for-the-impatient/#locations-on-the-cluster","text":"Your home directory is located in /fast/users/$USER . Your home is for scripts, source code, and configuration only. Use your work directory for large files. The quota in the home directory is 1 GB but we have nightly snapshots and backups thereof. Your work directory is located in /fast/users/$USER/work . This is where you should place large files. Files in this location do not have snapshots or backups. The directory (actually a GPFS file set) /fast/users/$USER/scratch should be used for temporary data. All data placed there will be removed after 4 weeks. If you are part of an AG/lab working on the cluster, the group directory is in /fast/groups/$AG . Projects are located in /fast/projects/$PROJECT . So-called dot files/directories filling up your home? Files and directories starting with a dot \" . \" are not shown with the \"ls\" command. May users run into problems with directories such as $HOME/.local but also non-dot directories such as $HOME/R filling up their storage. You should move such large directories to your work volume and only keep a symlink in your $HOME . Here is how you find large directories: host:~$ du -shc ~/.??* ~/* Here is how you move them to your work and replace them with a symlink: host:~$ mv ~/.local ~/work/.local host:~$ ln -s ~/work/.local ~/.local","title":"Locations on the Cluster"},{"location":"overview/for-the-impatient/#temporary-directories","text":"Note that you also have access to /tmp on the individual nodes but the disk is small and might be a slow spinning disk. If you are processing large NGS data, we recommend you create /fast/users/$USER/scratch/tmp and set the environment variable TMPDIR to point there. However, for creating locks special Unix files such as sockets or fifos, /tmp is the right place. Note that files placed in your scratch directory will be removed automatically after 4 weeks. Do not place any valueable files in there.","title":"Temporary Directories"},{"location":"overview/for-the-impatient/#first-steps-on-the-cluster","text":"","title":"First Steps on the Cluster"},{"location":"overview/for-the-impatient/#connecting-to-the-cluster","text":"From the Charite, MDC, and BIH networks, you can connect to the cluster login nodes med-login{1,2}.bihealth.org . For Charite users, your name is ${USER}_c , for MDC users, your account is ${USER}_m where $USER is the login name of your primary location. From the outside, for MDC users , the cluster is accessible via ssh1.mdc-berlin.de (you need to enable SSH key agent forwarding for this) Note that you have to use your MDC user name (without any suffix _m ) for connecting to this host. Also note that BIH HPC IT does not have control over ssh1.mdc-berlin.de . You have to contact MDC IT in case of any issues. From the outside, for Charite users, there is no SSH hop node. Instead, you have to apply for VPN through Charite Gesch\u00e4ftsbereich IT. You can use this form availble in Charite Intranet for this. Please refer to the Charite intranet or helpdesk@charite.de for more information.","title":"Connecting to the Cluster"},{"location":"overview/for-the-impatient/#connecting-to-compute-node-through-login-node","text":"After logging into the cluster, you are on the login node med-login<X> ( <X> can be either 1 or 2 ). When transferring files, use the med-transfer1 or med-transfer2 nodes. You should not do computation or other work on the login or file transfer nodes, but use the compute nodes instead. Typically, you'll create an interactive session on a compute node using the srun command.","title":"Connecting to Compute Node through Login Node"},{"location":"overview/for-the-impatient/#submitting-jobs","text":"While not recommended, you can perform computations (such as using BWA) in the interactive session. However, when the connection is interrupted, your computation process will be stopped. It is therefore recommended you submit jobs using the sbatch command (or use screen or tmux ).","title":"Submitting Jobs"},{"location":"overview/for-the-impatient/#inspecting-jobs-and-the-cluster","text":"You can inspect your currently running jobs with squeue , and kill them using scancel . You can inspect jobs that have finished with sacct , and see the cluster nodes using sinfo .","title":"Inspecting Jobs and the Cluster"},{"location":"overview/job-scheduler/","text":"Job Scheduler \u00b6 Once logged into the cluster through the login nodes, users use the Slurm scheduler for job submission. In Slurm nomenclature, the nodes are assigned to one or more partitions and jobs are then assigned to nodes according to the partition's configuration. Selecting Partitions Users should only select partitions explictely if they need special resources such as GPUs, high-memory nodes, Infiniband, or the critical partition. Otherwise, they should let Slurm decide about the optimal execution. MPI Partition We recently introduced an mpi partition that allows you to allocate more than one node. We will update this soon. Partitions \u00b6 The BIH HPC has the following partitions. Use mpi Partition for Multi-Node Jobs All partitions except for the mpi partition have the maximal number of availble nodes set to 1 . This makes the system easier to use for single-node/multi-core apps as users don't have to specify --min-nodes=1 --max-nodes=1 . Simply use the mpi partition for multi-node jobs. debug \u00b6 debug/default partition Actually, the default partition is the \"debug\" partition for now. We will rename things soon. default \u00b6 This partition is for normal jobs running in less than 4 hours. The number of overall allows jobs by one user is high to reward users for chopping down their tasks into smaller parts and making them scheduler friendly. maximal running time: 4 hours maximal cores: >6000 cores (all nodes, no limit ) priority: default < medium < long < critical < mini argument string: maximal running time: --time 04:00:00 medium \u00b6 This partition is for jobs running for multiple days. maximal running time: 7 days maximal cores: 128 cores/slots (4 nodes) priority: default < medium < long < critical < mini argument string: maximal running time: -t 7-00:00:00 long \u00b6 This partition is for long-running tasks. maximal running time: 28 days maximal cores: 32 cores/slots (1 node) project name: long queue name: long.q priority: default < medium < long < critical < mini argument string: maximal running time: -t 28-00:00:00 critical \u00b6 This partition is for time-critical jobs with deadlines. For access to it you have to first ask hpc-gatekeeper@bihealth.de . See Resource Registration: Critical Partition for details. As long as the cluster is not very busy, requests for critical jobs will be granted most of the time. However, do not use this queue without arranging with hpc-gatekeeper as killing jobs will be used as the ultima ratio in case of such policy violations. maximal running time: 7 days of maximal running time maximal cores: 1536 cores/slots (48 nodes) project name: critical priority: default < medium < long < critical < mini argument string: maximal running time: -t 7-00:00:00 gpu \u00b6 The GPU nodes are only part of the gpu partition so they are not blocked by normal compute jobs. The maximal running time is relatively high (14 days) to allow for longer training jobs. Contact hpc-helpdesk@bihealth.de if you have longer running jobs that you really cannot make run any shorter for assistance. For access to it you have register hpc-gatekeeper@bihealth.de (who will grant all requests). See Resource Registration: GPU Nodes for details. maximal running time: 14 days of maximal running time partition name: gpu argument string: select $count nodes: -p gpu --gres=gpu:tesla:$count , maximal running time: -t 14-00:00:00 highmem \u00b6 The high membory nodes are only part of the highmem partition so they are not blocked by normal compute jobs. The maximal running time is relatively high (14 days) to allow for longer jobs. Contact hpc-helpdesk@bihealth.de if you have longer running jobs that you really cannot make run any shorter for assistance. For access to it you have register hpc-gatekeeper@bihealth.de (who will grant all requests). See Resource Registration: GPU Nodes for details. maximal running time: 14 days of maximal running time partition name: highmem argument string: -p highmem , maximal running time: -t 14-00:00:00 mpi \u00b6 You can submit multi-node jobs into the mpi partition. The maximal running time is relatively high (14 days) to allow for longer jobs. Don't abuse this. Contact hpc-helpdesk@bihealth.de if you have longer running jobs that you really cannot make run any shorter for assistance. For access to it you have register hpc-gatekeeper@bihealth.de (who will grant all requests). See Resource Registration: GPU Nodes for details. maximal running time: 14 days of maximal running time partition name: highmem argument string: -p mpi , maximal running time: -t 14-00:00:00","title":"Job Scheduler"},{"location":"overview/job-scheduler/#job-scheduler","text":"Once logged into the cluster through the login nodes, users use the Slurm scheduler for job submission. In Slurm nomenclature, the nodes are assigned to one or more partitions and jobs are then assigned to nodes according to the partition's configuration. Selecting Partitions Users should only select partitions explictely if they need special resources such as GPUs, high-memory nodes, Infiniband, or the critical partition. Otherwise, they should let Slurm decide about the optimal execution. MPI Partition We recently introduced an mpi partition that allows you to allocate more than one node. We will update this soon.","title":"Job Scheduler"},{"location":"overview/job-scheduler/#partitions","text":"The BIH HPC has the following partitions. Use mpi Partition for Multi-Node Jobs All partitions except for the mpi partition have the maximal number of availble nodes set to 1 . This makes the system easier to use for single-node/multi-core apps as users don't have to specify --min-nodes=1 --max-nodes=1 . Simply use the mpi partition for multi-node jobs.","title":"Partitions"},{"location":"overview/job-scheduler/#debug","text":"debug/default partition Actually, the default partition is the \"debug\" partition for now. We will rename things soon.","title":"debug"},{"location":"overview/job-scheduler/#default","text":"This partition is for normal jobs running in less than 4 hours. The number of overall allows jobs by one user is high to reward users for chopping down their tasks into smaller parts and making them scheduler friendly. maximal running time: 4 hours maximal cores: >6000 cores (all nodes, no limit ) priority: default < medium < long < critical < mini argument string: maximal running time: --time 04:00:00","title":"default"},{"location":"overview/job-scheduler/#medium","text":"This partition is for jobs running for multiple days. maximal running time: 7 days maximal cores: 128 cores/slots (4 nodes) priority: default < medium < long < critical < mini argument string: maximal running time: -t 7-00:00:00","title":"medium"},{"location":"overview/job-scheduler/#long","text":"This partition is for long-running tasks. maximal running time: 28 days maximal cores: 32 cores/slots (1 node) project name: long queue name: long.q priority: default < medium < long < critical < mini argument string: maximal running time: -t 28-00:00:00","title":"long"},{"location":"overview/job-scheduler/#critical","text":"This partition is for time-critical jobs with deadlines. For access to it you have to first ask hpc-gatekeeper@bihealth.de . See Resource Registration: Critical Partition for details. As long as the cluster is not very busy, requests for critical jobs will be granted most of the time. However, do not use this queue without arranging with hpc-gatekeeper as killing jobs will be used as the ultima ratio in case of such policy violations. maximal running time: 7 days of maximal running time maximal cores: 1536 cores/slots (48 nodes) project name: critical priority: default < medium < long < critical < mini argument string: maximal running time: -t 7-00:00:00","title":"critical"},{"location":"overview/job-scheduler/#gpu","text":"The GPU nodes are only part of the gpu partition so they are not blocked by normal compute jobs. The maximal running time is relatively high (14 days) to allow for longer training jobs. Contact hpc-helpdesk@bihealth.de if you have longer running jobs that you really cannot make run any shorter for assistance. For access to it you have register hpc-gatekeeper@bihealth.de (who will grant all requests). See Resource Registration: GPU Nodes for details. maximal running time: 14 days of maximal running time partition name: gpu argument string: select $count nodes: -p gpu --gres=gpu:tesla:$count , maximal running time: -t 14-00:00:00","title":"gpu"},{"location":"overview/job-scheduler/#highmem","text":"The high membory nodes are only part of the highmem partition so they are not blocked by normal compute jobs. The maximal running time is relatively high (14 days) to allow for longer jobs. Contact hpc-helpdesk@bihealth.de if you have longer running jobs that you really cannot make run any shorter for assistance. For access to it you have register hpc-gatekeeper@bihealth.de (who will grant all requests). See Resource Registration: GPU Nodes for details. maximal running time: 14 days of maximal running time partition name: highmem argument string: -p highmem , maximal running time: -t 14-00:00:00","title":"highmem"},{"location":"overview/job-scheduler/#mpi","text":"You can submit multi-node jobs into the mpi partition. The maximal running time is relatively high (14 days) to allow for longer jobs. Don't abuse this. Contact hpc-helpdesk@bihealth.de if you have longer running jobs that you really cannot make run any shorter for assistance. For access to it you have register hpc-gatekeeper@bihealth.de (who will grant all requests). See Resource Registration: GPU Nodes for details. maximal running time: 14 days of maximal running time partition name: highmem argument string: -p mpi , maximal running time: -t 14-00:00:00","title":"mpi"},{"location":"overview/storage/","text":"update old GPUs TODO: Old GPUs are listed. Nodes and Storage Volumes \u00b6 No mounting on the cluster itself. For various technical and security-related reasons, it is not possible to mount anything on the cluster nodes by users. That is, it is not possible to get file server mounts on the cluster nodes. For mounting the cluster storage on your computer, see Connecting: SSHFS Mounts . This document gives an overview of the nodes and volumes on the cluster. Cluster Layout \u00b6 Cluster Nodes \u00b6 The following groups of nodes are available to cluster users. There are a number of nodes that are invisible to non-admin staff, hosting the queue master and monitoring tools and providing backup storage for key critical data, but these are not shown here. med-login1..3 available as med-login{1,2,3}.bihealth.org login nodes (IPs: 172.16.45.209 , 172.16.45.210 , 172.16.45.211 ) do not perform any computation on these nodes! these nodes are not execution nodes each process may at most use 1GB of RAM to increase stability of the node med0101..0124,0127 25 standard nodes Intel Xeon E5-2650 v2 @2 .60Ghz, 16 cores x2 threading 128 GB RAM med0133..0164 32 standard nodes Intel Xeon E5-2667 v4 @3 .20GHz, 16 cores x 2 threading 192 GB RAM med0201..0264 64 nodes with Infiniband interconnect Intel Xeon E5-2650 v2 @2 .60Ghz, 16 cores x2 threading 128 GB RAM med0301..0304 4 nodes with 4 Tesla V100 GPUs each med0401..0405 special purpose/high-memory machines Intel Xeon E5-4650 v2 @2 .40GHz, 40 cores x2 threading med0401 and med0402 1 TB RAM med0403 and med0404 500 GB RAM med0405 2x \"Tesla K20Xm\" GPU accelleration cards (cluster resource gpu ) access limited to explicit GPU users med0601..0616 16 nodes owned by CUBI Intel Xeon E5-2640 v3 @2 .60Ghz 192 GB RAM med0618..0633 16 nodes owned by CUBI Intel Xeon E5-2667 v4 @3 .20GHz, 16 cores x 2 threading 192 GB RAM med0701..0764 64 standard nodes Intel Xeon E5-2667 v4 @3 .20GHz, 16 cores x 2 threading 192 GB RAM If not noted anyway, currently no access restrictions apply per se. Cluster Volumes and Locations \u00b6 The cluster has 2.1 PB of fast storage, currently available at /fast . The storage runs on a DDN appliance using the IBM GPFS file system and is designed for massively parallel access from an HPC system. In contrast to \"single server\" NFS systems, the system can provide large bandwidth to all cluster nodes in parallel as long as large data means relatively \"few\" files are read and written. The GPFS storage is split into three sections: home -- small, persistent, and safe storage , e.g., for documents and configuration files (default quota of 1GB). work -- larger and persistent storage , e.g., for your large data files (default quota of 1TB). scratch -- large and non-persistent storage , e.g., for temporary files, files are automatically deleted after 4 weeks (default quota of 100TB; deletion not implemented yet).) Each user, group, and project has one of the sections each, e.g., for users: /fast/users/$NAME /fast/users/$NAME/work /fast/users/$USER/scratch See Storage and Volumes: Locations for more informatin.","title":"Nodes and Volumes"},{"location":"overview/storage/#nodes-and-storage-volumes","text":"No mounting on the cluster itself. For various technical and security-related reasons, it is not possible to mount anything on the cluster nodes by users. That is, it is not possible to get file server mounts on the cluster nodes. For mounting the cluster storage on your computer, see Connecting: SSHFS Mounts . This document gives an overview of the nodes and volumes on the cluster.","title":"Nodes and Storage Volumes"},{"location":"overview/storage/#cluster-layout","text":"","title":"Cluster Layout"},{"location":"overview/storage/#cluster-nodes","text":"The following groups of nodes are available to cluster users. There are a number of nodes that are invisible to non-admin staff, hosting the queue master and monitoring tools and providing backup storage for key critical data, but these are not shown here. med-login1..3 available as med-login{1,2,3}.bihealth.org login nodes (IPs: 172.16.45.209 , 172.16.45.210 , 172.16.45.211 ) do not perform any computation on these nodes! these nodes are not execution nodes each process may at most use 1GB of RAM to increase stability of the node med0101..0124,0127 25 standard nodes Intel Xeon E5-2650 v2 @2 .60Ghz, 16 cores x2 threading 128 GB RAM med0133..0164 32 standard nodes Intel Xeon E5-2667 v4 @3 .20GHz, 16 cores x 2 threading 192 GB RAM med0201..0264 64 nodes with Infiniband interconnect Intel Xeon E5-2650 v2 @2 .60Ghz, 16 cores x2 threading 128 GB RAM med0301..0304 4 nodes with 4 Tesla V100 GPUs each med0401..0405 special purpose/high-memory machines Intel Xeon E5-4650 v2 @2 .40GHz, 40 cores x2 threading med0401 and med0402 1 TB RAM med0403 and med0404 500 GB RAM med0405 2x \"Tesla K20Xm\" GPU accelleration cards (cluster resource gpu ) access limited to explicit GPU users med0601..0616 16 nodes owned by CUBI Intel Xeon E5-2640 v3 @2 .60Ghz 192 GB RAM med0618..0633 16 nodes owned by CUBI Intel Xeon E5-2667 v4 @3 .20GHz, 16 cores x 2 threading 192 GB RAM med0701..0764 64 standard nodes Intel Xeon E5-2667 v4 @3 .20GHz, 16 cores x 2 threading 192 GB RAM If not noted anyway, currently no access restrictions apply per se.","title":"Cluster Nodes"},{"location":"overview/storage/#cluster-volumes-and-locations","text":"The cluster has 2.1 PB of fast storage, currently available at /fast . The storage runs on a DDN appliance using the IBM GPFS file system and is designed for massively parallel access from an HPC system. In contrast to \"single server\" NFS systems, the system can provide large bandwidth to all cluster nodes in parallel as long as large data means relatively \"few\" files are read and written. The GPFS storage is split into three sections: home -- small, persistent, and safe storage , e.g., for documents and configuration files (default quota of 1GB). work -- larger and persistent storage , e.g., for your large data files (default quota of 1TB). scratch -- large and non-persistent storage , e.g., for temporary files, files are automatically deleted after 4 weeks (default quota of 100TB; deletion not implemented yet).) Each user, group, and project has one of the sections each, e.g., for users: /fast/users/$NAME /fast/users/$NAME/work /fast/users/$USER/scratch See Storage and Volumes: Locations for more informatin.","title":"Cluster Volumes and Locations"},{"location":"slurm/background/","text":"Introduction to Scheduling \u00b6 As explained elsewhere in more detail, an HPC cluster consists of multiple computers that are connected via network and work together. Multiple users can use the system to do their work. This means that the system needs to join multiple computers ( nodes ) to provide a coherent view on them and the same time partition the system to allow for multiple users working concurrently. user 1 user 2 ... .---. .---. .---. .---. | J | | J | | J | | J | | o | | o | | o | | o | ... | b | | b | | b | | b | | 1 | | 2 | | 3 | | 4 | '---' '---' '---' '---' .------------------------------------------. | Cluster Scheduler | '------------------------------------------' .----------. .------------. .------------. | multiple | | separate | | computers | '----------' '------------' '------------' Interlude: Partitioning Single Computers \u00b6 Overall, this partitioning is not so much different from how your workstation or laptop works. Most likely, your computer (or even your smartphone) has multiple processors (or cores). You can run multiple programs on the same computer and the fact that (a) there is more than one core and (b) there is more than one program running is opaque to the running programs (except if they explicitely communicate with each other). The programs can explicitely take advantage of the multiple processor cores The main difference is that you use your computer in an interactive fashion (you perform an action and expect an immediate reaction). Even with a single processor (and core), your computer manages to run more than program at the same time. This is done with the so-called time-slicing approach where the operating system lets each programs run in turn for a short time (a few milliseconds). A program with a higher priority might get more time slices than one with a lower (e.g., your audio player has soft real-time requirements and you would hear artifacts if it is starved for compute resources). Your operating system protects each program from the next by creating an address space for each program. When two programs are running, the value of the memory at any given position in one program is independent from the value in the other program. Your operating system offers explicit functionality for sharing certain memory areas that two programs can use to exchange data efficiently. Similarly, file permissions with Unix users/groups or Unix/Windows ACLs (access control lists) are used to isolate users from each other. Programs can share data by accessing the same file if they can both access it. There are special files called sockets that allow for network-like inter-process communication but of course two programs on the same computer can also connect (virtually) via the computer network (no data will actually go through a cable). Interlude: Resource Types \u00b6 As another diversion, let us consider how Unix manages its resources. This fact is important to understand how to request resources from the scheduler later on. First of all, a computer might offer a certain feature such as a specific hardware platform or special network connection. Examples for this on the BIH HPC are specific Intel processor generations such as haswell or the availability of Infiniband networking. You can request these with so-called constraints; they are not allocated to specific jobs. Second, there are resources that are allocated to specific jobs. The most important resources here are: computing resources (processors/CPUs (central progressing units) and cores, details are explained below), main memory / RAM, and special hardware such as GPUs, and (wall-clock) time that a job wants to run as an upper bound. Generally, once a resource has been allocated to a job, they are not available to another. This means if you allocating more resources to your job that you actually need ( overallocation ) then these resources are not available to other jobs (whether they are from you or other users). This will be explained further below. Another example for resource allocation are licenses . The BIH HPC has a few Matlab 2016b licenses that users can request. As long as a license is allocated to one job, they are unavailable to another. Nodes, Sockets, Processors, Cores, Threads \u00b6 Regarding compute resources, Slurm differentiates between: nodes: a compute server, sockets: a socket in the compute server that hosts one physical processor, processor: a CPU or a CPU core in a multi-core computer (all CPUs in the BIH HPC are multi-core), and (hardware) threads: most Intel CPUs feature hardware threads (also known as \"hyperthreading\") where each each core looks like two cores. In most cases, you will use one compute node only. For using two nodes, you would have to use message passing, e.g., MPI, for processes on more than one node to communicate. Thus, you would mostly use single-threaded and multi-threaded processes or multiple processes on the same node. Above: Slurm's nomenclature for sockets, processors, cores, and threads (from Slurm Documentation ). Co-locating processes/threads on the same socket has certain implications that are mostly useful for numerical applications. We will not further go into detail here. Slurm provides many different features of ways to specify allocation of \"pinning\" to specific process locations. If you need this feature, we trust that you find sufficient explanation in the Slurm documentation. Usually, you would allocate multiple cores (which Slurm uses synonymously with processors) on a single node (allocation on a single node is the default). How Scheduling Works \u00b6 Slurm is an acronym for \"Simple Linux Unix Resource Manager\" and the word \"scheduler\" does not occur. Actually, one classically differentiate managing of resources and scheduling. The resource manager would allocate the user resources according to request for their job and ensure that there are no conflicts. In the case of no resources being available, the scheduler will put the user's job into a queue and then distribute compute sources to them as they become available. In the following, both is simplified as the term scheduler . Also, the interesting case occurs if there currently not enough resources available for at least two jobs that are submitted to the scheduler. The scheduler has to decide how to proceed. When simplifying to only scheduling cores, each job will request a number of cores and a number of cores. The scheduler will then generate a schedule that might look as follows. core ^ 4 | |---job2---| 3 | |---job2---| 2 | |---job2---| 1 | |--job1--| +--------------------------> t time 5 1 1 2 0 5 0 job1 has allocated one core and job2 has allocated two cores. When a job3 for one core is submitted at t = 5 then it has to wait at least as long until job1 is finished. If job3 would allocate two or more cores, it would have to wait at least until job2 has finished as well. We can now ask several questions, including the following: What if a job runs shorter than the allocated time? -- In this case, resources become free and the scheduler will attempt to select the next job(s) to run. What if a job runs longer than the allocated time? -- In this case, the scheduler will send a Unix signal to the process first. The job will be given a bit more time and then it will be terminated. You will find a note about this at the end of your job log file. What if multiple jobs compete for resources? -- The scheduler will prefer certain jobs over certain others using the Slurm Multifactor Priority Plugin . In practice, small jobs will be preferred over large jobs, users with few used resources in the last month will be favored over heavy consumers, long-waiting jobs will be favored over recently submitted jobs, and many other factors. You can use the sprio utility for inspecting the factors in real-time. How does the scheduler handle new requests? -- Generally, the scheduler will add new jobs into the waiting queue. The scheduler runs priority and scheduler recomputations with different computational effort. Slurm is configured to perform computationally simple schedule recomputations quite regularly and larger recomputations will be done more infrequently. Also see the Slurm Frequently Asked Questions . Please note that even if all jobs were known from the beginning then scheduling is a so-called NP-complete problem and whole computer science journals and books are dedicated only to scheduling. Things get more complex in the case of online scheduling where jobs can appear at any time. In practice, Slurm does a fantastic job with its heuristics but it heavily relies on parameter tuning. HPC administration is constantly working on optimizing the scheduler settings. In the case that you observe inexplicable behaviour, please notify us at hpc-helpdesk@bihealth.de . Slurm Partitions \u00b6 In Slurm, the nodes of a cluster are split into partitions . Nodes are assigned to one or more partition (see the Job Scheduler section for details). Jobs can also be assigned to one or more partitions and are executed in nodes of the given partition. In the BIH HPC, partitions are used to stratify jobs of certain running times and provide different quality of service (e.g., maximal number of CPU cores available to a user for jobs of a certain running time and size). The partitions gpu and highmem provide special hardware (the nodes are not assigned to other partitions) and the partition mpi allows MPI-parallelism and the allocation of jobs to more than one node. The Job Scheduler provides further details.","title":"Introduction to Scheduling"},{"location":"slurm/background/#introduction-to-scheduling","text":"As explained elsewhere in more detail, an HPC cluster consists of multiple computers that are connected via network and work together. Multiple users can use the system to do their work. This means that the system needs to join multiple computers ( nodes ) to provide a coherent view on them and the same time partition the system to allow for multiple users working concurrently. user 1 user 2 ... .---. .---. .---. .---. | J | | J | | J | | J | | o | | o | | o | | o | ... | b | | b | | b | | b | | 1 | | 2 | | 3 | | 4 | '---' '---' '---' '---' .------------------------------------------. | Cluster Scheduler | '------------------------------------------' .----------. .------------. .------------. | multiple | | separate | | computers | '----------' '------------' '------------'","title":"Introduction to Scheduling"},{"location":"slurm/background/#interlude-partitioning-single-computers","text":"Overall, this partitioning is not so much different from how your workstation or laptop works. Most likely, your computer (or even your smartphone) has multiple processors (or cores). You can run multiple programs on the same computer and the fact that (a) there is more than one core and (b) there is more than one program running is opaque to the running programs (except if they explicitely communicate with each other). The programs can explicitely take advantage of the multiple processor cores The main difference is that you use your computer in an interactive fashion (you perform an action and expect an immediate reaction). Even with a single processor (and core), your computer manages to run more than program at the same time. This is done with the so-called time-slicing approach where the operating system lets each programs run in turn for a short time (a few milliseconds). A program with a higher priority might get more time slices than one with a lower (e.g., your audio player has soft real-time requirements and you would hear artifacts if it is starved for compute resources). Your operating system protects each program from the next by creating an address space for each program. When two programs are running, the value of the memory at any given position in one program is independent from the value in the other program. Your operating system offers explicit functionality for sharing certain memory areas that two programs can use to exchange data efficiently. Similarly, file permissions with Unix users/groups or Unix/Windows ACLs (access control lists) are used to isolate users from each other. Programs can share data by accessing the same file if they can both access it. There are special files called sockets that allow for network-like inter-process communication but of course two programs on the same computer can also connect (virtually) via the computer network (no data will actually go through a cable).","title":"Interlude: Partitioning Single Computers"},{"location":"slurm/background/#interlude-resource-types","text":"As another diversion, let us consider how Unix manages its resources. This fact is important to understand how to request resources from the scheduler later on. First of all, a computer might offer a certain feature such as a specific hardware platform or special network connection. Examples for this on the BIH HPC are specific Intel processor generations such as haswell or the availability of Infiniband networking. You can request these with so-called constraints; they are not allocated to specific jobs. Second, there are resources that are allocated to specific jobs. The most important resources here are: computing resources (processors/CPUs (central progressing units) and cores, details are explained below), main memory / RAM, and special hardware such as GPUs, and (wall-clock) time that a job wants to run as an upper bound. Generally, once a resource has been allocated to a job, they are not available to another. This means if you allocating more resources to your job that you actually need ( overallocation ) then these resources are not available to other jobs (whether they are from you or other users). This will be explained further below. Another example for resource allocation are licenses . The BIH HPC has a few Matlab 2016b licenses that users can request. As long as a license is allocated to one job, they are unavailable to another.","title":"Interlude: Resource Types"},{"location":"slurm/background/#nodes-sockets-processors-cores-threads","text":"Regarding compute resources, Slurm differentiates between: nodes: a compute server, sockets: a socket in the compute server that hosts one physical processor, processor: a CPU or a CPU core in a multi-core computer (all CPUs in the BIH HPC are multi-core), and (hardware) threads: most Intel CPUs feature hardware threads (also known as \"hyperthreading\") where each each core looks like two cores. In most cases, you will use one compute node only. For using two nodes, you would have to use message passing, e.g., MPI, for processes on more than one node to communicate. Thus, you would mostly use single-threaded and multi-threaded processes or multiple processes on the same node. Above: Slurm's nomenclature for sockets, processors, cores, and threads (from Slurm Documentation ). Co-locating processes/threads on the same socket has certain implications that are mostly useful for numerical applications. We will not further go into detail here. Slurm provides many different features of ways to specify allocation of \"pinning\" to specific process locations. If you need this feature, we trust that you find sufficient explanation in the Slurm documentation. Usually, you would allocate multiple cores (which Slurm uses synonymously with processors) on a single node (allocation on a single node is the default).","title":"Nodes, Sockets, Processors, Cores, Threads"},{"location":"slurm/background/#how-scheduling-works","text":"Slurm is an acronym for \"Simple Linux Unix Resource Manager\" and the word \"scheduler\" does not occur. Actually, one classically differentiate managing of resources and scheduling. The resource manager would allocate the user resources according to request for their job and ensure that there are no conflicts. In the case of no resources being available, the scheduler will put the user's job into a queue and then distribute compute sources to them as they become available. In the following, both is simplified as the term scheduler . Also, the interesting case occurs if there currently not enough resources available for at least two jobs that are submitted to the scheduler. The scheduler has to decide how to proceed. When simplifying to only scheduling cores, each job will request a number of cores and a number of cores. The scheduler will then generate a schedule that might look as follows. core ^ 4 | |---job2---| 3 | |---job2---| 2 | |---job2---| 1 | |--job1--| +--------------------------> t time 5 1 1 2 0 5 0 job1 has allocated one core and job2 has allocated two cores. When a job3 for one core is submitted at t = 5 then it has to wait at least as long until job1 is finished. If job3 would allocate two or more cores, it would have to wait at least until job2 has finished as well. We can now ask several questions, including the following: What if a job runs shorter than the allocated time? -- In this case, resources become free and the scheduler will attempt to select the next job(s) to run. What if a job runs longer than the allocated time? -- In this case, the scheduler will send a Unix signal to the process first. The job will be given a bit more time and then it will be terminated. You will find a note about this at the end of your job log file. What if multiple jobs compete for resources? -- The scheduler will prefer certain jobs over certain others using the Slurm Multifactor Priority Plugin . In practice, small jobs will be preferred over large jobs, users with few used resources in the last month will be favored over heavy consumers, long-waiting jobs will be favored over recently submitted jobs, and many other factors. You can use the sprio utility for inspecting the factors in real-time. How does the scheduler handle new requests? -- Generally, the scheduler will add new jobs into the waiting queue. The scheduler runs priority and scheduler recomputations with different computational effort. Slurm is configured to perform computationally simple schedule recomputations quite regularly and larger recomputations will be done more infrequently. Also see the Slurm Frequently Asked Questions . Please note that even if all jobs were known from the beginning then scheduling is a so-called NP-complete problem and whole computer science journals and books are dedicated only to scheduling. Things get more complex in the case of online scheduling where jobs can appear at any time. In practice, Slurm does a fantastic job with its heuristics but it heavily relies on parameter tuning. HPC administration is constantly working on optimizing the scheduler settings. In the case that you observe inexplicable behaviour, please notify us at hpc-helpdesk@bihealth.de .","title":"How Scheduling Works"},{"location":"slurm/background/#slurm-partitions","text":"In Slurm, the nodes of a cluster are split into partitions . Nodes are assigned to one or more partition (see the Job Scheduler section for details). Jobs can also be assigned to one or more partitions and are executed in nodes of the given partition. In the BIH HPC, partitions are used to stratify jobs of certain running times and provide different quality of service (e.g., maximal number of CPU cores available to a user for jobs of a certain running time and size). The partitions gpu and highmem provide special hardware (the nodes are not assigned to other partitions) and the partition mpi allows MPI-parallelism and the allocation of jobs to more than one node. The Job Scheduler provides further details.","title":"Slurm Partitions"},{"location":"slurm/cheat-sheet/","text":"Slurm Cheat Sheet \u00b6 This page contains assorted Slurm commands and Bash snippets that should be helpful. man pages! $ man sinfo $ man scontrol $ man squeue # etc... interactive sessions med-login1:~$ srun --pty bash med0740:~$ echo \"Hello World\" med0740:~$ exit batch submission med-login1:~$ sbatch script.sh Submitted batch job 2 med-login1:~$ squeue JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 27 debug script.s holtgrem R 0 :06 1 med0703 listing nodes $ sinfo -N NODELIST NODES PARTITION STATE med0740 1 debug* idle med0741 1 debug* down* med0742 1 debug* down* $ scontrol show nodes NodeName = med0740 Arch = x86_64 CoresPerSocket = 8 CPUAlloc = 0 CPUTot = 32 CPULoad = 0 .06 AvailableFeatures =( null ) [ ... ] $ scontrol show nodes med0740 NodeName = med0740 Arch = x86_64 CoresPerSocket = 8 CPUAlloc = 0 CPUTot = 32 CPULoad = 0 .06 AvailableFeatures =( null ) ActiveFeatures =( null ) Gres =( null ) NodeAddr = med0740 NodeHostName = med0740 Version = 20 .02.0 OS = Linux 3 .10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 RealMemory = 1 AllocMem = 0 FreeMem = 174388 Sockets = 2 Boards = 1 State = IDLE ThreadsPerCore = 2 TmpDisk = 0 Weight = 1 Owner = N/A MCS_label = N/A Partitions = debug BootTime = 2020 -03-05T00:54:15 SlurmdStartTime = 2020 -03-05T16:23:25 CfgTRES = cpu = 32 ,mem = 1M,billing = 32 AllocTRES = CapWatts = n/a CurrentWatts = 0 AveWatts = 0 ExtSensorsJoules = n/s ExtSensorsWatts = 0 ExtSensorsTemp = n/s queue states $ squeue JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) $ squeue -u holtgrem_c JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) node resources $ sinfo -o \"%20N %10c %10m %25f %10G \" additional resources such as GPUs $ sinfo -o \"%N %G\" listing job details $ scontrol show job 225 JobId=225 JobName=bash UserId=XXX(135001) GroupId=XXX(30069) MCS_label=N/A Priority=4294901580 Nice=0 Account=(null) QOS=normal JobState=FAILED Reason=NonZeroExitCode Dependency=(null) Requeue=1 Restarts=0 BatchFlag=0 Reboot=0 ExitCode=130:0 RunTime=00:16:27 TimeLimit=14-00:00:00 TimeMin=N/A SubmitTime=2020-03-23T11:34:26 EligibleTime=2020-03-23T11:34:26 AccrueTime=Unknown StartTime=2020-03-23T11:34:26 EndTime=2020-03-23T11:50:53 Deadline=N/A SuspendTime=None SecsPreSuspend=0 LastSchedEval=2020-03-23T11:34:26 Partition=gpu AllocNode:Sid=med-login1:1918 ReqNodeList=(null) ExcNodeList=(null) NodeList=med0301 BatchHost=med0301 NumNodes=1 NumCPUs=2 NumTasks=0 CPUs/Task=1 ReqB:S:C:T=0:0:*:* TRES=cpu=2,node=1,billing=2 Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=* MinCPUsNode=1 MinMemoryNode=0 MinTmpDiskNode=0 Features=(null) DelayBoot=00:00:00 OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null) Command=bash WorkDir=XXX Power= TresPerNode=gpu:tesla:4 MailUser=(null) MailType=NONE host:~$ squeue JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 1177 medium bash jweiner_ R 4 -21:52:24 1 med0127 1192 medium bash jweiner_ R 4 -07:08:40 1 med0127 1209 highmem bash mkuhrin_ R 2 -01:07:17 1 med0402 1210 gpu bash hilberta R 1 -10:30:34 1 med0304 1213 long bash schubacm R 1 -09:42:27 1 med0127 2401 gpu bash ramkem_c R 1 -05:14:53 1 med0303 2431 medium ngs_mapp holtgrem R 1 -05:01:41 1 med0127 2437 critical snakejob holtgrem R 1 -05:01:34 1 med0135 2733 debug bash schubacm R 7 :36:42 1 med0127 3029 critical ngs_mapp holtgrem R 5 :59:07 1 med0127 3030 critical snakejob holtgrem R 5 :56:23 1 med0134 3031 critical snakejob holtgrem R 5 :56:23 1 med0137 3032 critical snakejob holtgrem R 5 :56:23 1 med0137 3033 critical snakejob holtgrem R 5 :56:23 1 med0138 3034 critical snakejob holtgrem R 5 :56:23 1 med0138 3035 critical snakejob holtgrem R 5 :56:20 1 med0139 3036 critical snakejob holtgrem R 5 :56:20 1 med0139 3037 critical snakejob holtgrem R 5 :56:20 1 med0140 3038 critical snakejob holtgrem R 5 :56:20 1 med0140 3039 critical snakejob holtgrem R 5 :56:20 1 med0141 3040 critical snakejob holtgrem R 5 :56:20 1 med0141 3041 critical snakejob holtgrem R 5 :56:20 1 med0142 3042 critical snakejob holtgrem R 5 :56:20 1 med0142 3043 critical snakejob holtgrem R 5 :56:20 1 med0143 3044 critical snakejob holtgrem R 5 :56:20 1 med0143 3063 long bash schubacm R 4 :12:37 1 med0127 3066 long bash schubacm R 4 :11:47 1 med0127 3113 medium ngs_mapp holtgrem R 1 :52:33 1 med0708 3118 medium snakejob holtgrem R 1 :50:38 1 med0133 3119 medium snakejob holtgrem R 1 :50:38 1 med0703 3126 medium snakejob holtgrem R 1 :50:38 1 med0706 3127 medium snakejob holtgrem R 1 :50:38 1 med0144 3128 medium snakejob holtgrem R 1 :50:38 1 med0144 3133 medium snakejob holtgrem R 1 :50:35 1 med0147 3134 medium snakejob holtgrem R 1 :50:35 1 med0147 3135 medium snakejob holtgrem R 1 :50:35 1 med0148 3136 medium snakejob holtgrem R 1 :50:35 1 med0148 3138 medium snakejob holtgrem R 1 :50:35 1 med0104 host:~$ squeue -o \"%.10i %9P %20j %10u %.2t %.10M %.6D %10R %b\" JOBID PARTITION NAME USER ST TIME NODES NODELIST ( R TRES_PER_NODE 1177 medium bash jweiner_m R 4 -21:52:22 1 med0127 N/A 1192 medium bash jweiner_m R 4 -07:08:38 1 med0127 N/A 1209 highmem bash mkuhrin_m R 2 -01:07:15 1 med0402 N/A 1210 gpu bash hilberta_c R 1 -10:30:32 1 med0304 gpu:tesla:4 1213 long bash schubacm_c R 1 -09:42:25 1 med0127 N/A 2401 gpu bash ramkem_c R 1 -05:14:51 1 med0303 gpu:tesla:1 2431 medium ngs_mapping holtgrem_c R 1 -05:01:39 1 med0127 N/A 2437 critical snakejob.ngs_mapping holtgrem_c R 1 -05:01:32 1 med0135 N/A 2733 debug bash schubacm_c R 7 :36:40 1 med0127 N/A 3029 critical ngs_mapping holtgrem_c R 5 :59:05 1 med0127 N/A 3030 critical snakejob.ngs_mapping holtgrem_c R 5 :56:21 1 med0134 N/A 3031 critical snakejob.ngs_mapping holtgrem_c R 5 :56:21 1 med0137 N/A 3032 critical snakejob.ngs_mapping holtgrem_c R 5 :56:21 1 med0137 N/A 3033 critical snakejob.ngs_mapping holtgrem_c R 5 :56:21 1 med0138 N/A 3034 critical snakejob.ngs_mapping holtgrem_c R 5 :56:21 1 med0138 N/A 3035 critical snakejob.ngs_mapping holtgrem_c R 5 :56:18 1 med0139 N/A 3036 critical snakejob.ngs_mapping holtgrem_c R 5 :56:18 1 med0139 N/A 3037 critical snakejob.ngs_mapping holtgrem_c R 5 :56:18 1 med0140 N/A 3038 critical snakejob.ngs_mapping holtgrem_c R 5 :56:18 1 med0140 N/A 3039 critical snakejob.ngs_mapping holtgrem_c R 5 :56:18 1 med0141 N/A 3040 critical snakejob.ngs_mapping holtgrem_c R 5 :56:18 1 med0141 N/A 3041 critical snakejob.ngs_mapping holtgrem_c R 5 :56:18 1 med0142 N/A 3042 critical snakejob.ngs_mapping holtgrem_c R 5 :56:18 1 med0142 N/A 3043 critical snakejob.ngs_mapping holtgrem_c R 5 :56:18 1 med0143 N/A 3044 critical snakejob.ngs_mapping holtgrem_c R 5 :56:18 1 med0143 N/A 3063 long bash schubacm_c R 4 :12:35 1 med0127 N/A 3066 long bash schubacm_c R 4 :11:45 1 med0127 N/A 3113 medium ngs_mapping holtgrem_c R 1 :52:31 1 med0708 N/A 3118 medium snakejob.ngs_mapping holtgrem_c R 1 :50:36 1 med0133 N/A 3119 medium snakejob.ngs_mapping holtgrem_c R 1 :50:36 1 med0703 N/A 3126 medium snakejob.ngs_mapping holtgrem_c R 1 :50:36 1 med0706 N/A 3127 medium snakejob.ngs_mapping holtgrem_c R 1 :50:36 1 med0144 N/A 3128 medium snakejob.ngs_mapping holtgrem_c R 1 :50:36 1 med0144 N/A 3133 medium snakejob.ngs_mapping holtgrem_c R 1 :50:33 1 med0147 N/A 3134 medium snakejob.ngs_mapping holtgrem_c R 1 :50:33 1 med0147 N/A 3135 medium snakejob.ngs_mapping holtgrem_c R 1 :50:33 1 med0148 N/A 3136 medium snakejob.ngs_mapping holtgrem_c R 1 :50:33 1 med0148 N/A 3138 medium snakejob.ngs_mapping holtgrem_c R 1 :50:33 1 med0104 N/A host:~$ sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST debug* up 8 :00:00 11 drain med [ 0707 ,0709-0710,0740-0742,0744-0745,0749,0752,0755 ] debug* up 8 :00:00 8 mix med [ 0104 ,0127,0133-0135,0703,0706,0708 ] debug* up 8 :00:00 10 alloc med [ 0137 -0144,0147-0148 ] debug* up 8 :00:00 103 idle med [ 0105 -0124,0136,0145-0146,0151-0164,0201-0264,0704-0705 ] medium up 7 -00:00:00 11 drain med [ 0707 ,0709-0710,0740-0742,0744-0745,0749,0752,0755 ] medium up 7 -00:00:00 8 mix med [ 0104 ,0127,0133-0135,0703,0706,0708 ] medium up 7 -00:00:00 10 alloc med [ 0137 -0144,0147-0148 ] medium up 7 -00:00:00 103 idle med [ 0105 -0124,0136,0145-0146,0151-0164,0201-0264,0704-0705 ] long up 28 -00:00:0 11 drain med [ 0707 ,0709-0710,0740-0742,0744-0745,0749,0752,0755 ] long up 28 -00:00:0 8 mix med [ 0104 ,0127,0133-0135,0703,0706,0708 ] long up 28 -00:00:0 10 alloc med [ 0137 -0144,0147-0148 ] long up 28 -00:00:0 103 idle med [ 0105 -0124,0136,0145-0146,0151-0164,0201-0264,0704-0705 ] critical up 7 -00:00:00 11 drain med [ 0707 ,0709-0710,0740-0742,0744-0745,0749,0752,0755 ] critical up 7 -00:00:00 8 mix med [ 0104 ,0127,0133-0135,0703,0706,0708 ] critical up 7 -00:00:00 10 alloc med [ 0137 -0144,0147-0148 ] critical up 7 -00:00:00 103 idle med [ 0105 -0124,0136,0145-0146,0151-0164,0201-0264,0704-0705 ] highmem up 14 -00:00:0 1 mix med0402 highmem up 14 -00:00:0 3 idle med [ 0401 ,0403-0404 ] gpu up 14 -00:00:0 2 mix med [ 0303 -0304 ] gpu up 14 -00:00:0 2 idle med [ 0301 -0302 ]","title":"Slurm Cheat Sheet"},{"location":"slurm/cheat-sheet/#slurm-cheat-sheet","text":"This page contains assorted Slurm commands and Bash snippets that should be helpful. man pages! $ man sinfo $ man scontrol $ man squeue # etc... interactive sessions med-login1:~$ srun --pty bash med0740:~$ echo \"Hello World\" med0740:~$ exit batch submission med-login1:~$ sbatch script.sh Submitted batch job 2 med-login1:~$ squeue JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 27 debug script.s holtgrem R 0 :06 1 med0703 listing nodes $ sinfo -N NODELIST NODES PARTITION STATE med0740 1 debug* idle med0741 1 debug* down* med0742 1 debug* down* $ scontrol show nodes NodeName = med0740 Arch = x86_64 CoresPerSocket = 8 CPUAlloc = 0 CPUTot = 32 CPULoad = 0 .06 AvailableFeatures =( null ) [ ... ] $ scontrol show nodes med0740 NodeName = med0740 Arch = x86_64 CoresPerSocket = 8 CPUAlloc = 0 CPUTot = 32 CPULoad = 0 .06 AvailableFeatures =( null ) ActiveFeatures =( null ) Gres =( null ) NodeAddr = med0740 NodeHostName = med0740 Version = 20 .02.0 OS = Linux 3 .10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 RealMemory = 1 AllocMem = 0 FreeMem = 174388 Sockets = 2 Boards = 1 State = IDLE ThreadsPerCore = 2 TmpDisk = 0 Weight = 1 Owner = N/A MCS_label = N/A Partitions = debug BootTime = 2020 -03-05T00:54:15 SlurmdStartTime = 2020 -03-05T16:23:25 CfgTRES = cpu = 32 ,mem = 1M,billing = 32 AllocTRES = CapWatts = n/a CurrentWatts = 0 AveWatts = 0 ExtSensorsJoules = n/s ExtSensorsWatts = 0 ExtSensorsTemp = n/s queue states $ squeue JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) $ squeue -u holtgrem_c JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) node resources $ sinfo -o \"%20N %10c %10m %25f %10G \" additional resources such as GPUs $ sinfo -o \"%N %G\" listing job details $ scontrol show job 225 JobId=225 JobName=bash UserId=XXX(135001) GroupId=XXX(30069) MCS_label=N/A Priority=4294901580 Nice=0 Account=(null) QOS=normal JobState=FAILED Reason=NonZeroExitCode Dependency=(null) Requeue=1 Restarts=0 BatchFlag=0 Reboot=0 ExitCode=130:0 RunTime=00:16:27 TimeLimit=14-00:00:00 TimeMin=N/A SubmitTime=2020-03-23T11:34:26 EligibleTime=2020-03-23T11:34:26 AccrueTime=Unknown StartTime=2020-03-23T11:34:26 EndTime=2020-03-23T11:50:53 Deadline=N/A SuspendTime=None SecsPreSuspend=0 LastSchedEval=2020-03-23T11:34:26 Partition=gpu AllocNode:Sid=med-login1:1918 ReqNodeList=(null) ExcNodeList=(null) NodeList=med0301 BatchHost=med0301 NumNodes=1 NumCPUs=2 NumTasks=0 CPUs/Task=1 ReqB:S:C:T=0:0:*:* TRES=cpu=2,node=1,billing=2 Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=* MinCPUsNode=1 MinMemoryNode=0 MinTmpDiskNode=0 Features=(null) DelayBoot=00:00:00 OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null) Command=bash WorkDir=XXX Power= TresPerNode=gpu:tesla:4 MailUser=(null) MailType=NONE host:~$ squeue JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 1177 medium bash jweiner_ R 4 -21:52:24 1 med0127 1192 medium bash jweiner_ R 4 -07:08:40 1 med0127 1209 highmem bash mkuhrin_ R 2 -01:07:17 1 med0402 1210 gpu bash hilberta R 1 -10:30:34 1 med0304 1213 long bash schubacm R 1 -09:42:27 1 med0127 2401 gpu bash ramkem_c R 1 -05:14:53 1 med0303 2431 medium ngs_mapp holtgrem R 1 -05:01:41 1 med0127 2437 critical snakejob holtgrem R 1 -05:01:34 1 med0135 2733 debug bash schubacm R 7 :36:42 1 med0127 3029 critical ngs_mapp holtgrem R 5 :59:07 1 med0127 3030 critical snakejob holtgrem R 5 :56:23 1 med0134 3031 critical snakejob holtgrem R 5 :56:23 1 med0137 3032 critical snakejob holtgrem R 5 :56:23 1 med0137 3033 critical snakejob holtgrem R 5 :56:23 1 med0138 3034 critical snakejob holtgrem R 5 :56:23 1 med0138 3035 critical snakejob holtgrem R 5 :56:20 1 med0139 3036 critical snakejob holtgrem R 5 :56:20 1 med0139 3037 critical snakejob holtgrem R 5 :56:20 1 med0140 3038 critical snakejob holtgrem R 5 :56:20 1 med0140 3039 critical snakejob holtgrem R 5 :56:20 1 med0141 3040 critical snakejob holtgrem R 5 :56:20 1 med0141 3041 critical snakejob holtgrem R 5 :56:20 1 med0142 3042 critical snakejob holtgrem R 5 :56:20 1 med0142 3043 critical snakejob holtgrem R 5 :56:20 1 med0143 3044 critical snakejob holtgrem R 5 :56:20 1 med0143 3063 long bash schubacm R 4 :12:37 1 med0127 3066 long bash schubacm R 4 :11:47 1 med0127 3113 medium ngs_mapp holtgrem R 1 :52:33 1 med0708 3118 medium snakejob holtgrem R 1 :50:38 1 med0133 3119 medium snakejob holtgrem R 1 :50:38 1 med0703 3126 medium snakejob holtgrem R 1 :50:38 1 med0706 3127 medium snakejob holtgrem R 1 :50:38 1 med0144 3128 medium snakejob holtgrem R 1 :50:38 1 med0144 3133 medium snakejob holtgrem R 1 :50:35 1 med0147 3134 medium snakejob holtgrem R 1 :50:35 1 med0147 3135 medium snakejob holtgrem R 1 :50:35 1 med0148 3136 medium snakejob holtgrem R 1 :50:35 1 med0148 3138 medium snakejob holtgrem R 1 :50:35 1 med0104 host:~$ squeue -o \"%.10i %9P %20j %10u %.2t %.10M %.6D %10R %b\" JOBID PARTITION NAME USER ST TIME NODES NODELIST ( R TRES_PER_NODE 1177 medium bash jweiner_m R 4 -21:52:22 1 med0127 N/A 1192 medium bash jweiner_m R 4 -07:08:38 1 med0127 N/A 1209 highmem bash mkuhrin_m R 2 -01:07:15 1 med0402 N/A 1210 gpu bash hilberta_c R 1 -10:30:32 1 med0304 gpu:tesla:4 1213 long bash schubacm_c R 1 -09:42:25 1 med0127 N/A 2401 gpu bash ramkem_c R 1 -05:14:51 1 med0303 gpu:tesla:1 2431 medium ngs_mapping holtgrem_c R 1 -05:01:39 1 med0127 N/A 2437 critical snakejob.ngs_mapping holtgrem_c R 1 -05:01:32 1 med0135 N/A 2733 debug bash schubacm_c R 7 :36:40 1 med0127 N/A 3029 critical ngs_mapping holtgrem_c R 5 :59:05 1 med0127 N/A 3030 critical snakejob.ngs_mapping holtgrem_c R 5 :56:21 1 med0134 N/A 3031 critical snakejob.ngs_mapping holtgrem_c R 5 :56:21 1 med0137 N/A 3032 critical snakejob.ngs_mapping holtgrem_c R 5 :56:21 1 med0137 N/A 3033 critical snakejob.ngs_mapping holtgrem_c R 5 :56:21 1 med0138 N/A 3034 critical snakejob.ngs_mapping holtgrem_c R 5 :56:21 1 med0138 N/A 3035 critical snakejob.ngs_mapping holtgrem_c R 5 :56:18 1 med0139 N/A 3036 critical snakejob.ngs_mapping holtgrem_c R 5 :56:18 1 med0139 N/A 3037 critical snakejob.ngs_mapping holtgrem_c R 5 :56:18 1 med0140 N/A 3038 critical snakejob.ngs_mapping holtgrem_c R 5 :56:18 1 med0140 N/A 3039 critical snakejob.ngs_mapping holtgrem_c R 5 :56:18 1 med0141 N/A 3040 critical snakejob.ngs_mapping holtgrem_c R 5 :56:18 1 med0141 N/A 3041 critical snakejob.ngs_mapping holtgrem_c R 5 :56:18 1 med0142 N/A 3042 critical snakejob.ngs_mapping holtgrem_c R 5 :56:18 1 med0142 N/A 3043 critical snakejob.ngs_mapping holtgrem_c R 5 :56:18 1 med0143 N/A 3044 critical snakejob.ngs_mapping holtgrem_c R 5 :56:18 1 med0143 N/A 3063 long bash schubacm_c R 4 :12:35 1 med0127 N/A 3066 long bash schubacm_c R 4 :11:45 1 med0127 N/A 3113 medium ngs_mapping holtgrem_c R 1 :52:31 1 med0708 N/A 3118 medium snakejob.ngs_mapping holtgrem_c R 1 :50:36 1 med0133 N/A 3119 medium snakejob.ngs_mapping holtgrem_c R 1 :50:36 1 med0703 N/A 3126 medium snakejob.ngs_mapping holtgrem_c R 1 :50:36 1 med0706 N/A 3127 medium snakejob.ngs_mapping holtgrem_c R 1 :50:36 1 med0144 N/A 3128 medium snakejob.ngs_mapping holtgrem_c R 1 :50:36 1 med0144 N/A 3133 medium snakejob.ngs_mapping holtgrem_c R 1 :50:33 1 med0147 N/A 3134 medium snakejob.ngs_mapping holtgrem_c R 1 :50:33 1 med0147 N/A 3135 medium snakejob.ngs_mapping holtgrem_c R 1 :50:33 1 med0148 N/A 3136 medium snakejob.ngs_mapping holtgrem_c R 1 :50:33 1 med0148 N/A 3138 medium snakejob.ngs_mapping holtgrem_c R 1 :50:33 1 med0104 N/A host:~$ sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST debug* up 8 :00:00 11 drain med [ 0707 ,0709-0710,0740-0742,0744-0745,0749,0752,0755 ] debug* up 8 :00:00 8 mix med [ 0104 ,0127,0133-0135,0703,0706,0708 ] debug* up 8 :00:00 10 alloc med [ 0137 -0144,0147-0148 ] debug* up 8 :00:00 103 idle med [ 0105 -0124,0136,0145-0146,0151-0164,0201-0264,0704-0705 ] medium up 7 -00:00:00 11 drain med [ 0707 ,0709-0710,0740-0742,0744-0745,0749,0752,0755 ] medium up 7 -00:00:00 8 mix med [ 0104 ,0127,0133-0135,0703,0706,0708 ] medium up 7 -00:00:00 10 alloc med [ 0137 -0144,0147-0148 ] medium up 7 -00:00:00 103 idle med [ 0105 -0124,0136,0145-0146,0151-0164,0201-0264,0704-0705 ] long up 28 -00:00:0 11 drain med [ 0707 ,0709-0710,0740-0742,0744-0745,0749,0752,0755 ] long up 28 -00:00:0 8 mix med [ 0104 ,0127,0133-0135,0703,0706,0708 ] long up 28 -00:00:0 10 alloc med [ 0137 -0144,0147-0148 ] long up 28 -00:00:0 103 idle med [ 0105 -0124,0136,0145-0146,0151-0164,0201-0264,0704-0705 ] critical up 7 -00:00:00 11 drain med [ 0707 ,0709-0710,0740-0742,0744-0745,0749,0752,0755 ] critical up 7 -00:00:00 8 mix med [ 0104 ,0127,0133-0135,0703,0706,0708 ] critical up 7 -00:00:00 10 alloc med [ 0137 -0144,0147-0148 ] critical up 7 -00:00:00 103 idle med [ 0105 -0124,0136,0145-0146,0151-0164,0201-0264,0704-0705 ] highmem up 14 -00:00:0 1 mix med0402 highmem up 14 -00:00:0 3 idle med [ 0401 ,0403-0404 ] gpu up 14 -00:00:0 2 mix med [ 0303 -0304 ] gpu up 14 -00:00:0 2 idle med [ 0301 -0302 ]","title":"Slurm Cheat Sheet"},{"location":"slurm/commands-sacct/","text":"Slurm Command: sacct \u00b6 Perform queries to the Slurm accounting information. Representative Example med-login:~$ sacct -j 1607103 JobID JobName Partition Account AllocCPUS State ExitCode ------------ ---------- ---------- ---------- ---------- ---------- -------- 1607103 wgs_sv_an+ medium 1 PENDING 0 :0 The sacct command displays information from the Slurm accounting service. The Slurm scheduler only knows about active or completing (very recently active) jobs. The accouting system also knows about currently running jobs so it is the more robust way to query information about jobs. However, not all information is available to the accouting system, so scontrol show job and squeue provide more information about current and pending jbos. Slurm Documentation: sacct Please also see the official Slurm documentation on sacct . Important Arguments \u00b6 Also see all important arguments of the sbatch command. --jobs -- The job(s) to query for. --format -- Define attributes to retrieve. --long -- Get a lot of information from the database, consider to pipe into | less -S . Notes \u00b6 If you need to get information about a job regardless of it being in the past, present, or future execution, use sacct over scontrol and squeue .","title":"sacct"},{"location":"slurm/commands-sacct/#slurm-command-sacct","text":"Perform queries to the Slurm accounting information. Representative Example med-login:~$ sacct -j 1607103 JobID JobName Partition Account AllocCPUS State ExitCode ------------ ---------- ---------- ---------- ---------- ---------- -------- 1607103 wgs_sv_an+ medium 1 PENDING 0 :0 The sacct command displays information from the Slurm accounting service. The Slurm scheduler only knows about active or completing (very recently active) jobs. The accouting system also knows about currently running jobs so it is the more robust way to query information about jobs. However, not all information is available to the accouting system, so scontrol show job and squeue provide more information about current and pending jbos. Slurm Documentation: sacct Please also see the official Slurm documentation on sacct .","title":"Slurm Command: sacct"},{"location":"slurm/commands-sacct/#important-arguments","text":"Also see all important arguments of the sbatch command. --jobs -- The job(s) to query for. --format -- Define attributes to retrieve. --long -- Get a lot of information from the database, consider to pipe into | less -S .","title":"Important Arguments"},{"location":"slurm/commands-sacct/#notes","text":"If you need to get information about a job regardless of it being in the past, present, or future execution, use sacct over scontrol and squeue .","title":"Notes"},{"location":"slurm/commands-sbatch/","text":"Slurm Command: sinfo \u00b6 The srun command allows you to put a job into the scheduler's queue to be executed at a later time. Representative Example # Execute job.sh in partition medium with 4 threads and 4GB of RAM total for a # running time of up to one day. med-login:~$ sbatch --partition = medium --mem = 4G --ntasks 4 --time = 1 -00 job.sh Submitted batch job JOB_ID The command will create a batch job and add it to the queue to be executed at a later point in time. Slurm Documentation: srun Please also see the official Slurm documentation on srun . Important Arguments \u00b6 --nodes -- The number of nodes to allocate. This is only given here as an important argument as the maximum number of nodes allocatable to any partition but mpi is set to one (1). This is done as there are few users on the BIH HPC that actually use multi-node paralleilsm. Rather, most users will use multi-core parallelism and might forget to limit the number of nodes which causes inefficient allocation of resources. --ntasks -- This corresponds to the number of threads allocated to each node. --mem -- The memory to allocate for the job. As you can define minimal and maximal number of tasks/CPUs/cores, you could also specify --mem-per-cpu and get more flexible scheduling of your job. --gres -- Generic resource allocation. On the BIH HPC, this is only used for allocating GPUS, e.g., with --gres=gpu:tesla:2 , a user could allocate two NVIDIA Tesla GPUs on the same hsot. --licenses -- On the BIH HPC, this is used for the allocation of MATLAB 2016b licenses only. --partition -- The partition to run in. Also see the Job Scheduler section. --time -- Specify the running time, see man sbatch or the official Slurm documentation on srun for supported formats. **Please note that the DRMA API only accepts the hours:minutes format. --dependency -- Specify dependencies on other jobs, e.g., using --dependency afterok:JOBID to only execute if the job with ID JOBID finished successfully or --dependency after:JOBID to wait for a job to finish regardless of its termination status. --constraint -- Require one or more features from your node. On the BIH HPC, the processor generation is defined as a feature on the nodes, e.g., haswell , or special networking such as infiniband . You can have a look at /etc/slurm/slurm.conf on all configured features. --output -- The path to the output log file (by default joining stdout and stderr, see the man page on --error on how to redirect stderr separately). A various number of placeholders is available, see the \"filename pattern\" section of man sbatch or the official Slurm documentation on srun . Ensure your --output directory exists! In the case that the path to the log/output file does not exist, the job will just fail. scontrol show job ID will report JobState=FAILED Reason=NonZeroExitCode . Regrettably, no further information is displayed to you as the user. Always check that the path to the directories in StdErr and StdOut exists when checking scontrol show job ID . Other Arguments \u00b6 --job-name Job Scripts \u00b6 Also see the section Slurm Job Scripts on how to embed the sbatch parameters in #SBATCH lines. Notes \u00b6 This is the primary entry point for creating batch jobs to be executed at a later point in time. As with all jobs allocated by Slurm, interactive sessions executed with sbatch are governed by resource allocations, in particular: sbatch jobs have a maximal running time set, sbatch jobs have a maximal memory and number of cores set, and also see scontrol show job JOBID .","title":"sbatch"},{"location":"slurm/commands-sbatch/#slurm-command-sinfo","text":"The srun command allows you to put a job into the scheduler's queue to be executed at a later time. Representative Example # Execute job.sh in partition medium with 4 threads and 4GB of RAM total for a # running time of up to one day. med-login:~$ sbatch --partition = medium --mem = 4G --ntasks 4 --time = 1 -00 job.sh Submitted batch job JOB_ID The command will create a batch job and add it to the queue to be executed at a later point in time. Slurm Documentation: srun Please also see the official Slurm documentation on srun .","title":"Slurm Command: sinfo"},{"location":"slurm/commands-sbatch/#important-arguments","text":"--nodes -- The number of nodes to allocate. This is only given here as an important argument as the maximum number of nodes allocatable to any partition but mpi is set to one (1). This is done as there are few users on the BIH HPC that actually use multi-node paralleilsm. Rather, most users will use multi-core parallelism and might forget to limit the number of nodes which causes inefficient allocation of resources. --ntasks -- This corresponds to the number of threads allocated to each node. --mem -- The memory to allocate for the job. As you can define minimal and maximal number of tasks/CPUs/cores, you could also specify --mem-per-cpu and get more flexible scheduling of your job. --gres -- Generic resource allocation. On the BIH HPC, this is only used for allocating GPUS, e.g., with --gres=gpu:tesla:2 , a user could allocate two NVIDIA Tesla GPUs on the same hsot. --licenses -- On the BIH HPC, this is used for the allocation of MATLAB 2016b licenses only. --partition -- The partition to run in. Also see the Job Scheduler section. --time -- Specify the running time, see man sbatch or the official Slurm documentation on srun for supported formats. **Please note that the DRMA API only accepts the hours:minutes format. --dependency -- Specify dependencies on other jobs, e.g., using --dependency afterok:JOBID to only execute if the job with ID JOBID finished successfully or --dependency after:JOBID to wait for a job to finish regardless of its termination status. --constraint -- Require one or more features from your node. On the BIH HPC, the processor generation is defined as a feature on the nodes, e.g., haswell , or special networking such as infiniband . You can have a look at /etc/slurm/slurm.conf on all configured features. --output -- The path to the output log file (by default joining stdout and stderr, see the man page on --error on how to redirect stderr separately). A various number of placeholders is available, see the \"filename pattern\" section of man sbatch or the official Slurm documentation on srun . Ensure your --output directory exists! In the case that the path to the log/output file does not exist, the job will just fail. scontrol show job ID will report JobState=FAILED Reason=NonZeroExitCode . Regrettably, no further information is displayed to you as the user. Always check that the path to the directories in StdErr and StdOut exists when checking scontrol show job ID .","title":"Important Arguments"},{"location":"slurm/commands-sbatch/#other-arguments","text":"--job-name","title":"Other Arguments"},{"location":"slurm/commands-sbatch/#job-scripts","text":"Also see the section Slurm Job Scripts on how to embed the sbatch parameters in #SBATCH lines.","title":"Job Scripts"},{"location":"slurm/commands-sbatch/#notes","text":"This is the primary entry point for creating batch jobs to be executed at a later point in time. As with all jobs allocated by Slurm, interactive sessions executed with sbatch are governed by resource allocations, in particular: sbatch jobs have a maximal running time set, sbatch jobs have a maximal memory and number of cores set, and also see scontrol show job JOBID .","title":"Notes"},{"location":"slurm/commands-scancel/","text":"Slurm Command: scancel \u00b6 Terminate a running Slurm job. Representative Example med-login1:~$ scancel 1703828 med-login1:~$ This command allows to terminate one or more running jobs (of course, non-superusers can only terminate their own jobs). Slurm Documentation: scancel Please also see the official Slurm documentation on srun .","title":"scancel"},{"location":"slurm/commands-scancel/#slurm-command-scancel","text":"Terminate a running Slurm job. Representative Example med-login1:~$ scancel 1703828 med-login1:~$ This command allows to terminate one or more running jobs (of course, non-superusers can only terminate their own jobs). Slurm Documentation: scancel Please also see the official Slurm documentation on srun .","title":"Slurm Command: scancel"},{"location":"slurm/commands-scontrol/","text":"Slurm Command: scontrol \u00b6 The scontrol allows to query detailed information from the scheduler and perform manipulation. Object manipulation is less important for normal users. Representative Example med-login:~$ scontrol show job 1607103 JobId = 1607103 JobName = wgs_sv_annotation UserId = holtgrem_c ( 100131 ) GroupId = hpc-ag-cubi ( 5272 ) MCS_label = N/A Priority = 748 Nice = 0 Account =( null ) QOS = normal [ ... ] med-login:~$ scontrol show node med02 [ 01 -32 ] NodeName = med0201 Arch = x86_64 CoresPerSocket = 8 CPUAlloc = 0 CPUTot = 32 CPULoad = 0 .01 AvailableFeatures = ivybridge,infiniband ActiveFeatures = ivybridge,infiniband [ ... ] med-login:~$ scontrol show partition medium PartitionName = medium AllowGroups = ALL AllowAccounts = ALL AllowQos = ALL AllocNodes = ALL Default = NO QoS = medium DefaultTime = NONE DisableRootJobs = NO ExclusiveUser = NO GraceTime = 0 Hidden = NO [ ... ] This command allows to query all information for an object from Slurm, e.g., jobs, nodes, or partitions. The command also accepts ranges of jobs and hosts. It is most useful to get the information of one or a few objects from the scheduler. Slurm Documentation: scontrol Please also see the official Slurm documentation on scontrol . Important Sub commands \u00b6 scontrol show job -- Show details on jobs. scontrol show partition -- Show details on partitions. scontrol show node -- Show details on nodes. scontrol help -- Show help. scontrol -- Start an interactive scontrol shell / REPL (read-eval-print loop). Notes \u00b6 scontrol can only work on jobs that are pending (in the queue), running, or in \"completing' state. For jobs that have finished, you have to use Slurm's accounting features, e.g., with the sacct command.","title":"scontrol"},{"location":"slurm/commands-scontrol/#slurm-command-scontrol","text":"The scontrol allows to query detailed information from the scheduler and perform manipulation. Object manipulation is less important for normal users. Representative Example med-login:~$ scontrol show job 1607103 JobId = 1607103 JobName = wgs_sv_annotation UserId = holtgrem_c ( 100131 ) GroupId = hpc-ag-cubi ( 5272 ) MCS_label = N/A Priority = 748 Nice = 0 Account =( null ) QOS = normal [ ... ] med-login:~$ scontrol show node med02 [ 01 -32 ] NodeName = med0201 Arch = x86_64 CoresPerSocket = 8 CPUAlloc = 0 CPUTot = 32 CPULoad = 0 .01 AvailableFeatures = ivybridge,infiniband ActiveFeatures = ivybridge,infiniband [ ... ] med-login:~$ scontrol show partition medium PartitionName = medium AllowGroups = ALL AllowAccounts = ALL AllowQos = ALL AllocNodes = ALL Default = NO QoS = medium DefaultTime = NONE DisableRootJobs = NO ExclusiveUser = NO GraceTime = 0 Hidden = NO [ ... ] This command allows to query all information for an object from Slurm, e.g., jobs, nodes, or partitions. The command also accepts ranges of jobs and hosts. It is most useful to get the information of one or a few objects from the scheduler. Slurm Documentation: scontrol Please also see the official Slurm documentation on scontrol .","title":"Slurm Command: scontrol"},{"location":"slurm/commands-scontrol/#important-sub-commands","text":"scontrol show job -- Show details on jobs. scontrol show partition -- Show details on partitions. scontrol show node -- Show details on nodes. scontrol help -- Show help. scontrol -- Start an interactive scontrol shell / REPL (read-eval-print loop).","title":"Important Sub commands"},{"location":"slurm/commands-scontrol/#notes","text":"scontrol can only work on jobs that are pending (in the queue), running, or in \"completing' state. For jobs that have finished, you have to use Slurm's accounting features, e.g., with the sacct command.","title":"Notes"},{"location":"slurm/commands-sinfo/","text":"Slurm Command: sinfo \u00b6 The sinfo command allows you to query the current cluster status. Representative Example med-login1:~$ sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST [ ... ] medium up 7 -00:00:00 10 drain* med [ 0101 -0103,0125-0126,0128-0132 ] medium up 7 -00:00:00 1 down* med0243 medium up 7 -00:00:00 31 mix med [ 0104 ,0106-0122,0124,0133,0232-0233,0237-0238,0241-0242,0244,0263-0264,0503,0506 ] medium up 7 -00:00:00 5 alloc med [ 0105 ,0123,0127,0239-0240 ] medium up 7 -00:00:00 193 idle med [ 0134 -0164,0201-0231,0234-0236,0245-0262,0501-0502,0504-0505,0507-0516,0601-0632,0701-0764 ] [ ... ] med-login1:$ sinfo --summarize PARTITION AVAIL TIMELIMIT NODES ( A/I/O/T ) NODELIST debug* up 8 :00:00 38 /191/11/240 med [ 0101 -0164,0201-0264,0501-0516,0601-0632,0701-0764 ] medium up 7 -00:00:00 38 /191/11/240 med [ 0101 -0164,0201-0264,0501-0516,0601-0632,0701-0764 ] long up 28 -00:00:0 38 /191/11/240 med [ 0101 -0164,0201-0264,0501-0516,0601-0632,0701-0764 ] critical up 7 -00:00:00 25 /141/10/176 med [ 0101 -0164,0501-0516,0601-0632,0701-0764 ] highmem up 14 -00:00:0 1 /2/1/4 med [ 0401 -0404 ] gpu up 14 -00:00:0 3 /0/1/4 med [ 0301 -0304 ] mpi up 14 -00:00:0 38 /191/11/240 med [ 0101 -0164,0201-0264,0501-0516,0601-0632,0701-0764 ] This command will summaries the state of nodes by different criteria (e.g., by partition or globally). Slurm Documentation: sinfo Please also see the official Slurm documentation on srun . Important Arguments \u00b6 Also see all important arguments of the sinfo command. --summarize -- Summarize the node state by partition. --nodes -- Select the nodes to show the status for, e.g., display the status of all GPU nodes with sinfo -n med030[1-4] . Node States \u00b6 The most important node states are: down -- node is marked as offline draining -- node will not accept any more jobs but has jobs running on it drained -- node will not accept any more jobs and has no jobs running on it, but is not offline yet idle -- node is ready to run jobs allocated -- node is fully allocated (e.g., CPU, RAM, or GPU limit has been reached) mixed -- node is running jobs but there is space for more Notes \u00b6 Also see the Slurm Format Strings section.","title":"sinfo"},{"location":"slurm/commands-sinfo/#slurm-command-sinfo","text":"The sinfo command allows you to query the current cluster status. Representative Example med-login1:~$ sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST [ ... ] medium up 7 -00:00:00 10 drain* med [ 0101 -0103,0125-0126,0128-0132 ] medium up 7 -00:00:00 1 down* med0243 medium up 7 -00:00:00 31 mix med [ 0104 ,0106-0122,0124,0133,0232-0233,0237-0238,0241-0242,0244,0263-0264,0503,0506 ] medium up 7 -00:00:00 5 alloc med [ 0105 ,0123,0127,0239-0240 ] medium up 7 -00:00:00 193 idle med [ 0134 -0164,0201-0231,0234-0236,0245-0262,0501-0502,0504-0505,0507-0516,0601-0632,0701-0764 ] [ ... ] med-login1:$ sinfo --summarize PARTITION AVAIL TIMELIMIT NODES ( A/I/O/T ) NODELIST debug* up 8 :00:00 38 /191/11/240 med [ 0101 -0164,0201-0264,0501-0516,0601-0632,0701-0764 ] medium up 7 -00:00:00 38 /191/11/240 med [ 0101 -0164,0201-0264,0501-0516,0601-0632,0701-0764 ] long up 28 -00:00:0 38 /191/11/240 med [ 0101 -0164,0201-0264,0501-0516,0601-0632,0701-0764 ] critical up 7 -00:00:00 25 /141/10/176 med [ 0101 -0164,0501-0516,0601-0632,0701-0764 ] highmem up 14 -00:00:0 1 /2/1/4 med [ 0401 -0404 ] gpu up 14 -00:00:0 3 /0/1/4 med [ 0301 -0304 ] mpi up 14 -00:00:0 38 /191/11/240 med [ 0101 -0164,0201-0264,0501-0516,0601-0632,0701-0764 ] This command will summaries the state of nodes by different criteria (e.g., by partition or globally). Slurm Documentation: sinfo Please also see the official Slurm documentation on srun .","title":"Slurm Command: sinfo"},{"location":"slurm/commands-sinfo/#important-arguments","text":"Also see all important arguments of the sinfo command. --summarize -- Summarize the node state by partition. --nodes -- Select the nodes to show the status for, e.g., display the status of all GPU nodes with sinfo -n med030[1-4] .","title":"Important Arguments"},{"location":"slurm/commands-sinfo/#node-states","text":"The most important node states are: down -- node is marked as offline draining -- node will not accept any more jobs but has jobs running on it drained -- node will not accept any more jobs and has no jobs running on it, but is not offline yet idle -- node is ready to run jobs allocated -- node is fully allocated (e.g., CPU, RAM, or GPU limit has been reached) mixed -- node is running jobs but there is space for more","title":"Node States"},{"location":"slurm/commands-sinfo/#notes","text":"Also see the Slurm Format Strings section.","title":"Notes"},{"location":"slurm/commands-squeue/","text":"Slurm Command: squeue \u00b6 The squeue command allows you to view currently running and pending jobs. Representative Example med-login:~$ squeue JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 1583165 highmem 20200702 usr PD 0 :00 1 ( DependencyNeverSatisfied ) 1605901 critical variant_ holtgrem PD 0 :00 1 ( DependencyNeverSatisfied ) 1605902 critical variant_ holtgrem PD 0 :00 1 ( Dependency ) 1605905 critical variant_ holtgrem PD 0 :00 1 ( DependencyNeverSatisfied ) 1605916 critical wgs_sv_c holtgrem PD 0 :00 1 ( Dependency ) 1607103 medium wgs_sv_a holtgrem PD 0 :00 1 ( DependencyNeverSatisfied ) [ ... ] Slurm Documentation: squeue Please also see the official Slurm documentation on squeue . Important Arguments \u00b6 --nodelist -- Only display jobs running on certain nodes (e.g., GPU nodes). --format -- Define the format to print, see man squeue for details. See below for a format string that includes the jobid, partition, job name, user name, job status, running time, number of nodes, number of CPU cores, and allocated GPUs. Notes \u00b6 The following aliases in ~/.bashrc will allow you to print a long and informative squeue output with sq , pipe it into less with sql , get only your jobs (adjust the alias to your account) using sqme and pipe that into less with sqmel . alias sq = 'squeue -o \"%.10i %9P %60j %10u %.2t %.10M %.6D %.4C %10R %b\" \"$@\"' alias sql = 'sq \"$@\" | less -S' alias sqme = 'sq -u YOURUSER_c_or_m \"$@\"' alias sqmel = 'sqme \"$@\" | less -S'","title":"squeue"},{"location":"slurm/commands-squeue/#slurm-command-squeue","text":"The squeue command allows you to view currently running and pending jobs. Representative Example med-login:~$ squeue JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 1583165 highmem 20200702 usr PD 0 :00 1 ( DependencyNeverSatisfied ) 1605901 critical variant_ holtgrem PD 0 :00 1 ( DependencyNeverSatisfied ) 1605902 critical variant_ holtgrem PD 0 :00 1 ( Dependency ) 1605905 critical variant_ holtgrem PD 0 :00 1 ( DependencyNeverSatisfied ) 1605916 critical wgs_sv_c holtgrem PD 0 :00 1 ( Dependency ) 1607103 medium wgs_sv_a holtgrem PD 0 :00 1 ( DependencyNeverSatisfied ) [ ... ] Slurm Documentation: squeue Please also see the official Slurm documentation on squeue .","title":"Slurm Command: squeue"},{"location":"slurm/commands-squeue/#important-arguments","text":"--nodelist -- Only display jobs running on certain nodes (e.g., GPU nodes). --format -- Define the format to print, see man squeue for details. See below for a format string that includes the jobid, partition, job name, user name, job status, running time, number of nodes, number of CPU cores, and allocated GPUs.","title":"Important Arguments"},{"location":"slurm/commands-squeue/#notes","text":"The following aliases in ~/.bashrc will allow you to print a long and informative squeue output with sq , pipe it into less with sql , get only your jobs (adjust the alias to your account) using sqme and pipe that into less with sqmel . alias sq = 'squeue -o \"%.10i %9P %60j %10u %.2t %.10M %.6D %.4C %10R %b\" \"$@\"' alias sql = 'sq \"$@\" | less -S' alias sqme = 'sq -u YOURUSER_c_or_m \"$@\"' alias sqmel = 'sqme \"$@\" | less -S'","title":"Notes"},{"location":"slurm/commands-srun/","text":"Slurm Command: srun \u00b6 The srun command allows you to run a command now . Representative Example med-login:~$ srun --pty bash -i med0201:~$ The command will perform a resource allocation with the scheduler (and wait until it has allocated the requested resources) first. Most importantly, you can specify the --pty argument which will connect the current terminal's standard output, error, and input to your current one. This allows you to run interactive jobs such as shells with srun --pty bash -i . Slurm Documentation: srun Please also see the official Slurm documentation on srun . Important Arguments \u00b6 Also see all important arguments of the sbatch command. --pty -- Connect current terminal to the job's stdoud/stderr/stdin. --x11 -- Setup X11 forwarding. --immediate -- Immediately terminate if the resources to run the job are not available, do not wait. --test-only -- Don't run anything, but only estimate when the job would be scheduled. Notes \u00b6 This is the primary entry point for creating interactive shell sessions on the cluster. As with all jobs allocated by Slurm, interactive sessions executed with srun are governed by resource allocations, in particular: srun jobs have a maximal running time set, srun jobs have a maximal memory and number of cores set, and also see scontrol show job JOBID .","title":"srun"},{"location":"slurm/commands-srun/#slurm-command-srun","text":"The srun command allows you to run a command now . Representative Example med-login:~$ srun --pty bash -i med0201:~$ The command will perform a resource allocation with the scheduler (and wait until it has allocated the requested resources) first. Most importantly, you can specify the --pty argument which will connect the current terminal's standard output, error, and input to your current one. This allows you to run interactive jobs such as shells with srun --pty bash -i . Slurm Documentation: srun Please also see the official Slurm documentation on srun .","title":"Slurm Command: srun"},{"location":"slurm/commands-srun/#important-arguments","text":"Also see all important arguments of the sbatch command. --pty -- Connect current terminal to the job's stdoud/stderr/stdin. --x11 -- Setup X11 forwarding. --immediate -- Immediately terminate if the resources to run the job are not available, do not wait. --test-only -- Don't run anything, but only estimate when the job would be scheduled.","title":"Important Arguments"},{"location":"slurm/commands-srun/#notes","text":"This is the primary entry point for creating interactive shell sessions on the cluster. As with all jobs allocated by Slurm, interactive sessions executed with srun are governed by resource allocations, in particular: srun jobs have a maximal running time set, srun jobs have a maximal memory and number of cores set, and also see scontrol show job JOBID .","title":"Notes"},{"location":"slurm/format-strings/","text":"Slurm Command Format Strings \u00b6 In the sections Slurm Quickstart and Slurm Cheat Sheet , we have seen that sinfo and squeue allow for the compact display partitions/nodes and node information. In contrast, scontrol show job <id> and scontrol show partition <id> and scontrol show node <id> show comprehensive information that quickly gets hard to comprehend for multiple entries. Now you might ask: is there anything in between? And: yes, there is . You can tune the output of sinfo and squeue using parameters, in particular by providing format strings . All of this is described in the man pages of the commands that you can display with man sinfo and man squeue on the cluster. Tuning sinfo Output \u00b6 Notable arguments of sinfo are: -N, --Node -- uncompress the usual lines and display one line per node and partition. -s, --summarize -- compress the node state, more compact display. -R, --list-reasons -- for nodes that are not up , display reason string provided by admin. -o <fmt>, --format=<fmt> -- use format string for display. The most interesting argument is -o/--format . The man page lists the following values that are used when using other arguments. In other words, many of the display modifications could also be applied with -o/--format . default \"%#P %.5a %.10l %.6D %.6t %N\" --summarize \"%#P %.5a %.10l %.16F %N\" --long \"%#P %.5a %.10l %.10s %.4r %.8h %.10g %.6D %.11T %N\" --Node \"%#N %.6D %#P %6t\" --long --Node \"%#N %.6D %#P %.11T %.4c %.8z %.6m %.8d %.6w %.8f %20E\" --list-reasons \"%20E %9u %19H %N\" --long --list-reasons \"%20E %12U %19H %6t %N\" The best way to learn more about this is to play around with sinfo -o , starting out with one of the format strings above. Details about the format strings are described in man sinfo . Some remarks here: %<num><char> displays the value represented by <char> padded with spaces to the right such that a width of <num> is reached, %.<num><char> displays the value represented by <char> padded with spaces to the left such that a width of <num> is reached, and %#<char> displays the value represented by <char> padded with spaces to the max length of the value represented by <char> (this is a \"virtual\" value, used internally only, you cannot use this and you will have to place an integer here). For example, to create a grouped display with reasons for being down use: med-login1:~$ sinfo -o \"%10P %.5a %.10l %.16F %40N %E\" PARTITION AVAIL TIMELIMIT NODES ( A/I/O/T ) NODELIST REASON debug* up 8 :00:00 0 /0/16/16 med [ 0703 -0710,0740-0742,0744-0745,0749,0 bogus node debug* up 8 :00:00 18 /98/0/116 med [ 0104 -0124,0127,0133-0148,0151-0164,0 none medium up 7 -00:00:00 0 /0/16/16 med [ 0703 -0710,0740-0742,0744-0745,0749,0 bogus node medium up 7 -00:00:00 18 /98/0/116 med [ 0104 -0124,0127,0133-0148,0151-0164,0 none long up 28 -00:00:0 0 /0/16/16 med [ 0703 -0710,0740-0742,0744-0745,0749,0 bogus node long up 28 -00:00:0 18 /98/0/116 med [ 0104 -0124,0127,0133-0148,0151-0164,0 none critical up 7 -00:00:00 0 /0/16/16 med [ 0703 -0710,0740-0742,0744-0745,0749,0 bogus node critical up 7 -00:00:00 18 /98/0/116 med [ 0104 -0124,0127,0133-0148,0151-0164,0 none highmem up 14 -00:00:0 0 /4/0/4 med [ 0401 -0404 ] none gpu up 14 -00:00:0 3 /1/0/4 med [ 0301 -0304 ] none Tuning squeue Output \u00b6 The standard squeue output might yield the following med-login1:~$ squeue | head JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 3149 medium variant_ holtgrem PD 0 :00 1 ( Dependency ) 1177 medium bash jweiner_ R 6 -03:32:41 1 med0127 1192 medium bash jweiner_ R 5 -12:48:57 1 med0127 1210 gpu bash hilberta R 2 -16:10:51 1 med0304 1213 long bash schubacm R 2 -15:22:44 1 med0127 2401 gpu bash ramkem_c R 2 -10:55:10 1 med0303 3063 long bash schubacm R 1 -09:52:54 1 med0127 3066 long bash schubacm R 1 -09:52:04 1 med0127 3147 medium ngs_mapp holtgrem R 1 -03:13:42 1 med0148 Looking at man squeue , we learn that the default format strings are: default \"%.18i %.9P %.8j %.8u %.2t %.10M %.6D %R\" -l, --long \"%.18i %.9P %.8j %.8u %.8T %.10M %.9l %.6D %R\" -s, --steps \"%.15i %.8j %.9P %.8u %.9M %N\" This looks a bit wasteful. Let's cut down on the padding of the job ID and expand on the job name and remove some right paddings. med-login1:~$ squeue -o \"%.6i %9P %30j %.10u %.2t %.10M %.6D %R %b\" | head JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 3149 medium variant_calling holtgrem_c PD 0 :00 1 ( Dependency ) 1177 medium bash jweiner_m R 6 -03:35:55 1 med0127 1192 medium bash jweiner_m R 5 -12:52:11 1 med0127 1210 gpu bash hilberta_c R 2 -16:14:05 1 med0304 1213 long bash schubacm_c R 2 -15:25:58 1 med0127 2401 gpu bash ramkem_c R 2 -10:58:24 1 med0303 3063 long bash schubacm_c R 1 -09:56:08 1 med0127 3066 long bash schubacm_c R 1 -09:55:18 1 med0127 3147 medium ngs_mapping holtgrem_c R 1 -03:16:56 1 med0148 Displaying Resources \u00b6 Now display how many of our internal projects still exist. med-login1:~$ squeue -o \"%.6i %9P %30j %.10u %.2t %.10M %.6D %10R %s\" | head The next steps are (TODO): setup of certificate for containers opening firewall apropriately integrate with openmpi documentation","title":"Format Strings"},{"location":"slurm/format-strings/#slurm-command-format-strings","text":"In the sections Slurm Quickstart and Slurm Cheat Sheet , we have seen that sinfo and squeue allow for the compact display partitions/nodes and node information. In contrast, scontrol show job <id> and scontrol show partition <id> and scontrol show node <id> show comprehensive information that quickly gets hard to comprehend for multiple entries. Now you might ask: is there anything in between? And: yes, there is . You can tune the output of sinfo and squeue using parameters, in particular by providing format strings . All of this is described in the man pages of the commands that you can display with man sinfo and man squeue on the cluster.","title":"Slurm Command Format Strings"},{"location":"slurm/format-strings/#tuning-sinfo-output","text":"Notable arguments of sinfo are: -N, --Node -- uncompress the usual lines and display one line per node and partition. -s, --summarize -- compress the node state, more compact display. -R, --list-reasons -- for nodes that are not up , display reason string provided by admin. -o <fmt>, --format=<fmt> -- use format string for display. The most interesting argument is -o/--format . The man page lists the following values that are used when using other arguments. In other words, many of the display modifications could also be applied with -o/--format . default \"%#P %.5a %.10l %.6D %.6t %N\" --summarize \"%#P %.5a %.10l %.16F %N\" --long \"%#P %.5a %.10l %.10s %.4r %.8h %.10g %.6D %.11T %N\" --Node \"%#N %.6D %#P %6t\" --long --Node \"%#N %.6D %#P %.11T %.4c %.8z %.6m %.8d %.6w %.8f %20E\" --list-reasons \"%20E %9u %19H %N\" --long --list-reasons \"%20E %12U %19H %6t %N\" The best way to learn more about this is to play around with sinfo -o , starting out with one of the format strings above. Details about the format strings are described in man sinfo . Some remarks here: %<num><char> displays the value represented by <char> padded with spaces to the right such that a width of <num> is reached, %.<num><char> displays the value represented by <char> padded with spaces to the left such that a width of <num> is reached, and %#<char> displays the value represented by <char> padded with spaces to the max length of the value represented by <char> (this is a \"virtual\" value, used internally only, you cannot use this and you will have to place an integer here). For example, to create a grouped display with reasons for being down use: med-login1:~$ sinfo -o \"%10P %.5a %.10l %.16F %40N %E\" PARTITION AVAIL TIMELIMIT NODES ( A/I/O/T ) NODELIST REASON debug* up 8 :00:00 0 /0/16/16 med [ 0703 -0710,0740-0742,0744-0745,0749,0 bogus node debug* up 8 :00:00 18 /98/0/116 med [ 0104 -0124,0127,0133-0148,0151-0164,0 none medium up 7 -00:00:00 0 /0/16/16 med [ 0703 -0710,0740-0742,0744-0745,0749,0 bogus node medium up 7 -00:00:00 18 /98/0/116 med [ 0104 -0124,0127,0133-0148,0151-0164,0 none long up 28 -00:00:0 0 /0/16/16 med [ 0703 -0710,0740-0742,0744-0745,0749,0 bogus node long up 28 -00:00:0 18 /98/0/116 med [ 0104 -0124,0127,0133-0148,0151-0164,0 none critical up 7 -00:00:00 0 /0/16/16 med [ 0703 -0710,0740-0742,0744-0745,0749,0 bogus node critical up 7 -00:00:00 18 /98/0/116 med [ 0104 -0124,0127,0133-0148,0151-0164,0 none highmem up 14 -00:00:0 0 /4/0/4 med [ 0401 -0404 ] none gpu up 14 -00:00:0 3 /1/0/4 med [ 0301 -0304 ] none","title":"Tuning sinfo Output"},{"location":"slurm/format-strings/#tuning-squeue-output","text":"The standard squeue output might yield the following med-login1:~$ squeue | head JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 3149 medium variant_ holtgrem PD 0 :00 1 ( Dependency ) 1177 medium bash jweiner_ R 6 -03:32:41 1 med0127 1192 medium bash jweiner_ R 5 -12:48:57 1 med0127 1210 gpu bash hilberta R 2 -16:10:51 1 med0304 1213 long bash schubacm R 2 -15:22:44 1 med0127 2401 gpu bash ramkem_c R 2 -10:55:10 1 med0303 3063 long bash schubacm R 1 -09:52:54 1 med0127 3066 long bash schubacm R 1 -09:52:04 1 med0127 3147 medium ngs_mapp holtgrem R 1 -03:13:42 1 med0148 Looking at man squeue , we learn that the default format strings are: default \"%.18i %.9P %.8j %.8u %.2t %.10M %.6D %R\" -l, --long \"%.18i %.9P %.8j %.8u %.8T %.10M %.9l %.6D %R\" -s, --steps \"%.15i %.8j %.9P %.8u %.9M %N\" This looks a bit wasteful. Let's cut down on the padding of the job ID and expand on the job name and remove some right paddings. med-login1:~$ squeue -o \"%.6i %9P %30j %.10u %.2t %.10M %.6D %R %b\" | head JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 3149 medium variant_calling holtgrem_c PD 0 :00 1 ( Dependency ) 1177 medium bash jweiner_m R 6 -03:35:55 1 med0127 1192 medium bash jweiner_m R 5 -12:52:11 1 med0127 1210 gpu bash hilberta_c R 2 -16:14:05 1 med0304 1213 long bash schubacm_c R 2 -15:25:58 1 med0127 2401 gpu bash ramkem_c R 2 -10:58:24 1 med0303 3063 long bash schubacm_c R 1 -09:56:08 1 med0127 3066 long bash schubacm_c R 1 -09:55:18 1 med0127 3147 medium ngs_mapping holtgrem_c R 1 -03:16:56 1 med0148","title":"Tuning squeue Output"},{"location":"slurm/format-strings/#displaying-resources","text":"Now display how many of our internal projects still exist. med-login1:~$ squeue -o \"%.6i %9P %30j %.10u %.2t %.10M %.6D %10R %s\" | head The next steps are (TODO): setup of certificate for containers opening firewall apropriately integrate with openmpi documentation","title":"Displaying Resources"},{"location":"slurm/job-scripts/","text":"Slurm Job Scripts \u00b6 This page describes how to create SLURM job scripts. SLURM job scripts look as follows. On the top you have lines starting with #SBATCH . These appear as comments to bash scripts. These lines are interpreted by sbatch in the same way as command line arguments. That is, when later submitting the script with sbatch my-job.sh you can either have the parameter to the sbatch call or in the file. Multi-Node Allocation in Slurm Classically, jobs on HPC systems are written in a way that they can run on multiple nodes at once, using the network to communicate. Slurm comes from this world and when allocating more than one CPU/core, it might allocate them on different nodes. Please use --nodes=1 to force Slurm to allocate them on a single node. Creating the Script host:example$ cat >my-job.sh << \"EOF\" #!/bin/bash # #SBATCH --job-name=this-is-my-job #SBATCH --output=output.txt # #SBATCH --ntasks=1 #SBATCH --nodes=1 #SBATCH --time=10:00 #SBATCH --mem-per-cpu=100M date hostname > & 2 echo \"Hello World\" sleep 1m date EOF Also see the SLURM Rosetta Stone for more options. Submit, Look at Queue & Result host:example$ sbatch script.sh Submitted batch job 315 host:example$ squeue -u holtgrem_c JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 315 debug this-is- holtgrem R 0:40 1 med0127 host:example$ sleep 2m host:example$ squeue -u holtgrem_c JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) host:example$ cat output.txt Wed Mar 25 13:30:56 CET 2020 med0127 Hello World Wed Mar 25 13:31:56 CET 2020","title":"Slurm Job Scripts"},{"location":"slurm/job-scripts/#slurm-job-scripts","text":"This page describes how to create SLURM job scripts. SLURM job scripts look as follows. On the top you have lines starting with #SBATCH . These appear as comments to bash scripts. These lines are interpreted by sbatch in the same way as command line arguments. That is, when later submitting the script with sbatch my-job.sh you can either have the parameter to the sbatch call or in the file. Multi-Node Allocation in Slurm Classically, jobs on HPC systems are written in a way that they can run on multiple nodes at once, using the network to communicate. Slurm comes from this world and when allocating more than one CPU/core, it might allocate them on different nodes. Please use --nodes=1 to force Slurm to allocate them on a single node. Creating the Script host:example$ cat >my-job.sh << \"EOF\" #!/bin/bash # #SBATCH --job-name=this-is-my-job #SBATCH --output=output.txt # #SBATCH --ntasks=1 #SBATCH --nodes=1 #SBATCH --time=10:00 #SBATCH --mem-per-cpu=100M date hostname > & 2 echo \"Hello World\" sleep 1m date EOF Also see the SLURM Rosetta Stone for more options. Submit, Look at Queue & Result host:example$ sbatch script.sh Submitted batch job 315 host:example$ squeue -u holtgrem_c JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 315 debug this-is- holtgrem R 0:40 1 med0127 host:example$ sleep 2m host:example$ squeue -u holtgrem_c JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) host:example$ cat output.txt Wed Mar 25 13:30:56 CET 2020 med0127 Hello World Wed Mar 25 13:31:56 CET 2020","title":"Slurm Job Scripts"},{"location":"slurm/memory-allocation/","text":"Memory Allocation \u00b6 Memory allocation is one of the topics that users find confusing most often. This section first gives some technical background and then explains how to implement this properly with Slurm on the BIH HPC. Technical Background \u00b6 Technical Background Summary virtual memory is what your programs tells the operating system what it wants to use resident set size is the amount of memory that your program actually uses most memory will reside on the heap Main memory used to be one of the most important topics when programming computers as computers had so little. There is the infamous quote \"640KB ought ot be enough for anybody\" wrongly attribute to Bill Gates which refers to the fact that early computers could only address that amount of memory and in MS DOS, one had to use special libraries for a program to use more memory. Today, computers are very fast and memory is plenty and people can (rightfully) forget about memory allocation ... as long as they don't use \"much\" memory by today's standards. The Linux operating system differentiates between the following types of memory: virtual memory size (vsize) , the amount of memory that a process (virtually) allocates, resident set size (rss) , the amount of memory actually used and that is currently in the computer's main memory, the swap memory usage , the amount of active memory that is not present in main memory but on the computer's disk, sometimes, the shared memory is also interesting, and it might be interesting to know about heap and stack size. Note that above we are talking about processes, not Slurm jobs yet. Let us look at this in detail: Each program uses some kind of memory management. For example, in C the malloc / free series of library commands allocate and free memory and in Java, R, and Python, memory allocation and release is done automatically using a concept called garbage collection. Each program starts out with a certain virtual memory size , that is the amount of memory it can address, say 128MB. When the program allocates memory, the memory allocation mechanism will look whether it has sufficient space left. If not, it will request an increase in virtual memory from the operating system, say to 256MB. If this fails then the program can try to handle this error, e.g., terminate gracefully, and many programs will just panic and stop. Otherwise, the program will get access to more memory and happily continue to run. However, programs can allocate humonguous amounts of virtual memory and only use a little. Memory is organized in pages (classically 4096 bytes but they can be larger using so-called \"huge page\" features). The operating system tracks which memory pages are actually used by a process. The total size of this is the resident set size and this is the amount of memory that is actually currently used by a program. Programs can also mark pages as unused again and thus free resident memory and also decrease their virtual memory. In some cases it is still interesting to use swap memory . Here, the contents of resident memory can be copied to the disk by the operating system. This process is completely transparent for the program; the memory is available at the original position in the virtual memory ! However, accessing it will take some time as it is read from the disk. This way, it was possible for a computer with 4MB of RAM and a disk of 100MB to run programs that used 8MB. Of course, this was only really useable for programs running in the background and one could really feel the latency if a graphical program was swapped in (you would actually hear the hard drive working). Today, swap is only interesting if you want to put your computer into hibernation. Given the large main memory on the cluster node and their small hard drives (only used for loading the operating system), the BIH HPC has no swap memory. Most HPC users will also use shared memory implicitely. Whenever a program uses fork to create a subprocess (this is not a thread), the program can chose to \"copy\" its current address space. The second process now has access to the same memory than the parent process in a copy-on-write fashion. This allows to pre-load some database but also allows to re-use loaded library code used by the child process as well. Once the child writes to the copy-on-write memory of the parent, the memory page will be copied and attributed to the child. Two or more processes can share the same amount of memory explicitely which is mostly used for inter-process communication but the program Bowtie uses this for sharing the memory of indices. For example, the Python multiprocessing module will use this but also if you have two MPI processes running on the same host. Memory is also separated into segments, the most interesting ones are heap and stack memory. For compiled languages, memory can be allocated on either. For C, an int x = 0 is allocated on the stack. Every time that you call a function, another stack frame is created with the stack-local variables. The stack thus mostly grows through function calls and your program and shrinks through returning from a function. The stack size is limited (by ulimit -s ) and a too deep (e.g., infinite recursion) will crash your program. Again for C, int * ptr = malloc(10 * sizeof(int)); will allocate memory for one integer pointer on the stack and memory for 10 integers on the heap. If the function returns, the pointer on the stack will be freed but for freeing the integer array, you'd have to do free(ptr) . If the memory is not freed then this constitutes a memory leak but that is another topic. Other relevant segments are code where the compile code lives and data where static data such as strings that are displayed to the user live. As a side node, for interpreted languages such as R or Python, the code and data segments will refer to the code and data of Python while the actual program text will be on the heap. Interlude: Memory in Java \u00b6 Memory in Java Summary set -XX:MaxHeapSize=<size> (e.g., <size>=2G ) for your program and only tune the other parameters if needed also consider the amount of memory that Java needs for heap management in your Slurm allocations Java's memory management provides for some interesting artifacts. When running simple Java programs, you will never run into this but if you need to use gigabytes of memory in Java then you will have to learn a bit about Java memory management. This is the case when running the GATK programs, for example. As different operating systems handle memory management slightly different, the Java virtual machine performs it own memory management to provide a consistent interface. The following three settings are important in governing memory usage of Java: -Xmx<size> / -XX:MaxHeapSize=<size> -- the maximal Java heap size -Xms<size> / -XX:InitialHeapSize=<size> -- the initial Java heap size -Xss<size> / -XX:ThreadStackSize=<size> -- maximal stack size available to a Java thread (e.g., the main thread) Above, <size> is a memory specification, either in bytes or with a suffix, e.g., 80M , or 1G . On startup, Java will roughly do the following: Setup the core virtual machine, load libraries, etc. and allocate (vsize) consume (rss) memory on the OS (operating system) heap . Setup the Java heap allocate (vsize) and consume (rss) memory on the OS heap . In particular, Java will need to setup data structures for the memory management of each individual object. Run the program where Java data and Java threads will lead to memory allocationg (vsize) and consuming (rss) of memory. Memory freed by the Java garbage collector can be re-used by other Java objects (rss remains the same) or be freed in the operating system (rss decreases). The Java VM program itself will also consume memory on the OS stack but that is negligible. Overall, the Java VM needs to store in main memory: The Java VM, program code, Java thread stacks etc. (very few memory). The Java heap (potentially a lot of memory). The Java heap management data structures (so-called \"off-heap\", but of course on the OS heap ) (potentially also considerable memory). In the BIH HPC context, the following is recommended: Set the Java heap to an appropriate size (use trial-and-error to determin the correct size or look through internet forums). Only tune initial heap size in the case of performance issues (unlikely in batch processing). Only bump the stack size when problems occur. Consider the \"off-heap\" memory when performing Slurm allocations. Memory Allocation in Slurm \u00b6 Memory Allocation in Slurm Summary most user will simply use --memory=<size> (e.g., <size>=3G ) to allocate memory per node both interactive srun and batch sbatch jobs are governed by Slurm memory allocation the sum of all memory of all processes started by your job may not go over the job reservation please don't over-allocate memory, see \"Memory Accounting in Slurm\" below for details Our Slurm configuration uses Linux cgroups to enforce a maximum amount of resident memory. You simply specify it using --memory=<size> to your srun and sbatch command. In the (rare) case that you provide more flexible number of threads (Slurm tasks) or GPUs then you could also look into --mem-per-cpu and --mem-per-gpu . The official Slurm sbatch manual is quite helpful as is man sbatch on the cluster command line. Slurm (or rather Linux via cgroups) will track all memory started by all jobs by your process. If each process works independently (e.g., you put the output through a pipe prog1 | prog2 ) then the amount of memory consumed will at any given time be the sum of the RSS of both processes at that time . If you your program uses for fork which transfers memory in a copy-on-write fashion then of course, the shared memory is only counted once. Note that Python's multiprocessing is not using copy on write but the data will be explicitely copied and consume additional memory. Refer to the Scipy/Numpy/Pandas etc. documentation on how to achieve parallelism without copying data too much. The amount of virtual memory that your program can reserve is only \"virtually\" unlimited (pun not intended). However, in practice, the operating system will not like you allocating more than physically available. When you program attempts to allocate more memory than requested via Slurm, then you program will be killed. You can inspect the amount of memory available on each node in total with sinfo --format \"%.10P %.10l %.6D %.6m %N\" as shown below. med-login1:~# sinfo --format \"%.10P %.10l %.6D %.6m %N\" PARTITION TIMELIMIT NODES MEMORY NODELIST debug* 8 :00:00 240 128722 med [ 0101 -0164,0201-0264,0501-0516,0601-0632,0701-0764 ] medium 7 -00:00:00 240 128722 med [ 0101 -0164,0201-0264,0501-0516,0601-0632,0701-0764 ] long 28 -00:00:0 240 128722 med [ 0101 -0164,0201-0264,0501-0516,0601-0632,0701-0764 ] critical 7 -00:00:00 176 128722 med [ 0101 -0164,0501-0516,0601-0632,0701-0764 ] highmem 14 -00:00:0 4 515762 med [ 0401 -0404 ] gpu 14 -00:00:0 4 385215 med [ 0301 -0304 ] mpi 14 -00:00:0 240 128722 med [ 0101 -0164,0201-0264,0501-0516,0601-0632,0701-0764 ] Memory/CPU Accounting in Slurm \u00b6 Memory Accounting in Slurm Summary you can use Slurm accounting to find out about memory and also CPU usage of your program use sacct -j JOBID --format=JobID,MaxRSS to display the RSS usage of your program use sacct -j JOBID --format=Elapsed,AllocCPUs,TotalCPU to display information about CPU usage consider using the helpful script below to compute overallocated memory While Slurm runs your job it collect information about your job such as the running time, exit status, and memory usage. This information is available through the scheduling system with its commands squeue and scontrol only while the job is pending execution, executing, or currently completing. After completion, the information is only available through the Slurm accounting system. You can query information about jobs, e.g., using sacct : med-login1:~# sacct -j 1607166 JobID JobName Partition Account AllocCPUS State ExitCode ------------ ---------- ---------- ---------- ---------- ---------- -------- 1607166 snakejob.+ critical 16 COMPLETED 0 :0 1607166 .bat+ batch 16 COMPLETED 0 :0 1607166 .ext+ extern 16 COMPLETED 0 :0 This shows that the job with ID 1607166 with a job ID starting with snakejob. has been run in the critical partition, been allocated 16 cores and had an exit code of 0:0 . For technical reasons, there is a batch and an extern sub step. Actually, Slurm allows to run various steps in one batch as documented in the Slurm documentation . The sacct command has various command line parameters that you can see with man sacct or in the Slurm documentation . We can use --brief / -b to show only a brief summary. med-login1:~# sacct -j 1607166 --brief JobID State ExitCode ------------ ---------- -------- 1607166 COMPLETED 0 :0 1607166 .bat+ COMPLETED 0 :0 1607166 .ext+ COMPLETED 0 :0 Similarly, you can use --long to display extended information (see the manual for the displayed columns). Very long report lines can be piped into less -S for easier display. You can fine-tune the information to display with a format string to --format med-login1:~# sacct -j 1607166 --format = JobID,ReqMem,MaxRSS,Elapsed,TotalCPU,AllocCPUS JobID ReqMem MaxRSS Elapsed TotalCPU AllocCPUS ------------ --------- ---------- ---------- ---------- ---------- 1607166 60Gn 13 :07:31 7 -16:21:29 16 1607166 .bat+ 60Gn 4314560K 13 :07:31 7 -16:21:29 16 1607166 .ext+ 60Gn 0 13 :07:31 00 :00.001 16 From this command, we can read that we allocate 60GB memory of memory per node (suffix n ) and the maximum RSS is reported to be 4.3GB. You can use this information to fine-tune your memory allocations. Further, the program ran for 13 hours and 7 minutes with allocated 16 CPU cores and consumed a total of 17 days, 16 hours, and 21 minutes of CPU time. Thus, a total of 10'061 CPU minutes were spent in 787 minutes wall-clock time. This yields an overall empirical degree of parallelism of about 10061 / 787 = 14 and a parallel efficiency of 14 / 16 = 88%. The discussion of parallel efficiency is a topic not covered here. However, you can use the awk script below to compute the empirical parallelism ( EmpPar ) and the parallel efficiency ( ParEff ). The script also displays the difference i requested and used RSS ( DiffRSS ). The script can be found here . med-login1:~# sacct -j 1607166 --format = JobID,ReqMem,MaxRSS,Elapsed,TotalCPU,AllocCPUS \\ | awk -f quick-sacct.awk JobID ReqMem MaxRSS Elapsed TotalCPU AllocCPUS EmpPar ParEff DiffMEM ------------ ---------- ---------- ---------- ---------- ---------- --------- -------- -------- 1607166 60Gn 13 :07:31 7 -16:21:29 16 0 .00 0 .00 - 1607166 .bat+ 60Gn 4314560K 13 :07:31 7 -16:21:29 16 14 .05 0 .88 55 .89 1607166 .ext+ 60Gn 0 13 :07:31 00 :00.001 16 0 .00 0 .00 -","title":"Memory Allocation"},{"location":"slurm/memory-allocation/#memory-allocation","text":"Memory allocation is one of the topics that users find confusing most often. This section first gives some technical background and then explains how to implement this properly with Slurm on the BIH HPC.","title":"Memory Allocation"},{"location":"slurm/memory-allocation/#technical-background","text":"Technical Background Summary virtual memory is what your programs tells the operating system what it wants to use resident set size is the amount of memory that your program actually uses most memory will reside on the heap Main memory used to be one of the most important topics when programming computers as computers had so little. There is the infamous quote \"640KB ought ot be enough for anybody\" wrongly attribute to Bill Gates which refers to the fact that early computers could only address that amount of memory and in MS DOS, one had to use special libraries for a program to use more memory. Today, computers are very fast and memory is plenty and people can (rightfully) forget about memory allocation ... as long as they don't use \"much\" memory by today's standards. The Linux operating system differentiates between the following types of memory: virtual memory size (vsize) , the amount of memory that a process (virtually) allocates, resident set size (rss) , the amount of memory actually used and that is currently in the computer's main memory, the swap memory usage , the amount of active memory that is not present in main memory but on the computer's disk, sometimes, the shared memory is also interesting, and it might be interesting to know about heap and stack size. Note that above we are talking about processes, not Slurm jobs yet. Let us look at this in detail: Each program uses some kind of memory management. For example, in C the malloc / free series of library commands allocate and free memory and in Java, R, and Python, memory allocation and release is done automatically using a concept called garbage collection. Each program starts out with a certain virtual memory size , that is the amount of memory it can address, say 128MB. When the program allocates memory, the memory allocation mechanism will look whether it has sufficient space left. If not, it will request an increase in virtual memory from the operating system, say to 256MB. If this fails then the program can try to handle this error, e.g., terminate gracefully, and many programs will just panic and stop. Otherwise, the program will get access to more memory and happily continue to run. However, programs can allocate humonguous amounts of virtual memory and only use a little. Memory is organized in pages (classically 4096 bytes but they can be larger using so-called \"huge page\" features). The operating system tracks which memory pages are actually used by a process. The total size of this is the resident set size and this is the amount of memory that is actually currently used by a program. Programs can also mark pages as unused again and thus free resident memory and also decrease their virtual memory. In some cases it is still interesting to use swap memory . Here, the contents of resident memory can be copied to the disk by the operating system. This process is completely transparent for the program; the memory is available at the original position in the virtual memory ! However, accessing it will take some time as it is read from the disk. This way, it was possible for a computer with 4MB of RAM and a disk of 100MB to run programs that used 8MB. Of course, this was only really useable for programs running in the background and one could really feel the latency if a graphical program was swapped in (you would actually hear the hard drive working). Today, swap is only interesting if you want to put your computer into hibernation. Given the large main memory on the cluster node and their small hard drives (only used for loading the operating system), the BIH HPC has no swap memory. Most HPC users will also use shared memory implicitely. Whenever a program uses fork to create a subprocess (this is not a thread), the program can chose to \"copy\" its current address space. The second process now has access to the same memory than the parent process in a copy-on-write fashion. This allows to pre-load some database but also allows to re-use loaded library code used by the child process as well. Once the child writes to the copy-on-write memory of the parent, the memory page will be copied and attributed to the child. Two or more processes can share the same amount of memory explicitely which is mostly used for inter-process communication but the program Bowtie uses this for sharing the memory of indices. For example, the Python multiprocessing module will use this but also if you have two MPI processes running on the same host. Memory is also separated into segments, the most interesting ones are heap and stack memory. For compiled languages, memory can be allocated on either. For C, an int x = 0 is allocated on the stack. Every time that you call a function, another stack frame is created with the stack-local variables. The stack thus mostly grows through function calls and your program and shrinks through returning from a function. The stack size is limited (by ulimit -s ) and a too deep (e.g., infinite recursion) will crash your program. Again for C, int * ptr = malloc(10 * sizeof(int)); will allocate memory for one integer pointer on the stack and memory for 10 integers on the heap. If the function returns, the pointer on the stack will be freed but for freeing the integer array, you'd have to do free(ptr) . If the memory is not freed then this constitutes a memory leak but that is another topic. Other relevant segments are code where the compile code lives and data where static data such as strings that are displayed to the user live. As a side node, for interpreted languages such as R or Python, the code and data segments will refer to the code and data of Python while the actual program text will be on the heap.","title":"Technical Background"},{"location":"slurm/memory-allocation/#interlude-memory-in-java","text":"Memory in Java Summary set -XX:MaxHeapSize=<size> (e.g., <size>=2G ) for your program and only tune the other parameters if needed also consider the amount of memory that Java needs for heap management in your Slurm allocations Java's memory management provides for some interesting artifacts. When running simple Java programs, you will never run into this but if you need to use gigabytes of memory in Java then you will have to learn a bit about Java memory management. This is the case when running the GATK programs, for example. As different operating systems handle memory management slightly different, the Java virtual machine performs it own memory management to provide a consistent interface. The following three settings are important in governing memory usage of Java: -Xmx<size> / -XX:MaxHeapSize=<size> -- the maximal Java heap size -Xms<size> / -XX:InitialHeapSize=<size> -- the initial Java heap size -Xss<size> / -XX:ThreadStackSize=<size> -- maximal stack size available to a Java thread (e.g., the main thread) Above, <size> is a memory specification, either in bytes or with a suffix, e.g., 80M , or 1G . On startup, Java will roughly do the following: Setup the core virtual machine, load libraries, etc. and allocate (vsize) consume (rss) memory on the OS (operating system) heap . Setup the Java heap allocate (vsize) and consume (rss) memory on the OS heap . In particular, Java will need to setup data structures for the memory management of each individual object. Run the program where Java data and Java threads will lead to memory allocationg (vsize) and consuming (rss) of memory. Memory freed by the Java garbage collector can be re-used by other Java objects (rss remains the same) or be freed in the operating system (rss decreases). The Java VM program itself will also consume memory on the OS stack but that is negligible. Overall, the Java VM needs to store in main memory: The Java VM, program code, Java thread stacks etc. (very few memory). The Java heap (potentially a lot of memory). The Java heap management data structures (so-called \"off-heap\", but of course on the OS heap ) (potentially also considerable memory). In the BIH HPC context, the following is recommended: Set the Java heap to an appropriate size (use trial-and-error to determin the correct size or look through internet forums). Only tune initial heap size in the case of performance issues (unlikely in batch processing). Only bump the stack size when problems occur. Consider the \"off-heap\" memory when performing Slurm allocations.","title":"Interlude: Memory in Java"},{"location":"slurm/memory-allocation/#memory-allocation-in-slurm","text":"Memory Allocation in Slurm Summary most user will simply use --memory=<size> (e.g., <size>=3G ) to allocate memory per node both interactive srun and batch sbatch jobs are governed by Slurm memory allocation the sum of all memory of all processes started by your job may not go over the job reservation please don't over-allocate memory, see \"Memory Accounting in Slurm\" below for details Our Slurm configuration uses Linux cgroups to enforce a maximum amount of resident memory. You simply specify it using --memory=<size> to your srun and sbatch command. In the (rare) case that you provide more flexible number of threads (Slurm tasks) or GPUs then you could also look into --mem-per-cpu and --mem-per-gpu . The official Slurm sbatch manual is quite helpful as is man sbatch on the cluster command line. Slurm (or rather Linux via cgroups) will track all memory started by all jobs by your process. If each process works independently (e.g., you put the output through a pipe prog1 | prog2 ) then the amount of memory consumed will at any given time be the sum of the RSS of both processes at that time . If you your program uses for fork which transfers memory in a copy-on-write fashion then of course, the shared memory is only counted once. Note that Python's multiprocessing is not using copy on write but the data will be explicitely copied and consume additional memory. Refer to the Scipy/Numpy/Pandas etc. documentation on how to achieve parallelism without copying data too much. The amount of virtual memory that your program can reserve is only \"virtually\" unlimited (pun not intended). However, in practice, the operating system will not like you allocating more than physically available. When you program attempts to allocate more memory than requested via Slurm, then you program will be killed. You can inspect the amount of memory available on each node in total with sinfo --format \"%.10P %.10l %.6D %.6m %N\" as shown below. med-login1:~# sinfo --format \"%.10P %.10l %.6D %.6m %N\" PARTITION TIMELIMIT NODES MEMORY NODELIST debug* 8 :00:00 240 128722 med [ 0101 -0164,0201-0264,0501-0516,0601-0632,0701-0764 ] medium 7 -00:00:00 240 128722 med [ 0101 -0164,0201-0264,0501-0516,0601-0632,0701-0764 ] long 28 -00:00:0 240 128722 med [ 0101 -0164,0201-0264,0501-0516,0601-0632,0701-0764 ] critical 7 -00:00:00 176 128722 med [ 0101 -0164,0501-0516,0601-0632,0701-0764 ] highmem 14 -00:00:0 4 515762 med [ 0401 -0404 ] gpu 14 -00:00:0 4 385215 med [ 0301 -0304 ] mpi 14 -00:00:0 240 128722 med [ 0101 -0164,0201-0264,0501-0516,0601-0632,0701-0764 ]","title":"Memory Allocation in Slurm"},{"location":"slurm/memory-allocation/#memorycpu-accounting-in-slurm","text":"Memory Accounting in Slurm Summary you can use Slurm accounting to find out about memory and also CPU usage of your program use sacct -j JOBID --format=JobID,MaxRSS to display the RSS usage of your program use sacct -j JOBID --format=Elapsed,AllocCPUs,TotalCPU to display information about CPU usage consider using the helpful script below to compute overallocated memory While Slurm runs your job it collect information about your job such as the running time, exit status, and memory usage. This information is available through the scheduling system with its commands squeue and scontrol only while the job is pending execution, executing, or currently completing. After completion, the information is only available through the Slurm accounting system. You can query information about jobs, e.g., using sacct : med-login1:~# sacct -j 1607166 JobID JobName Partition Account AllocCPUS State ExitCode ------------ ---------- ---------- ---------- ---------- ---------- -------- 1607166 snakejob.+ critical 16 COMPLETED 0 :0 1607166 .bat+ batch 16 COMPLETED 0 :0 1607166 .ext+ extern 16 COMPLETED 0 :0 This shows that the job with ID 1607166 with a job ID starting with snakejob. has been run in the critical partition, been allocated 16 cores and had an exit code of 0:0 . For technical reasons, there is a batch and an extern sub step. Actually, Slurm allows to run various steps in one batch as documented in the Slurm documentation . The sacct command has various command line parameters that you can see with man sacct or in the Slurm documentation . We can use --brief / -b to show only a brief summary. med-login1:~# sacct -j 1607166 --brief JobID State ExitCode ------------ ---------- -------- 1607166 COMPLETED 0 :0 1607166 .bat+ COMPLETED 0 :0 1607166 .ext+ COMPLETED 0 :0 Similarly, you can use --long to display extended information (see the manual for the displayed columns). Very long report lines can be piped into less -S for easier display. You can fine-tune the information to display with a format string to --format med-login1:~# sacct -j 1607166 --format = JobID,ReqMem,MaxRSS,Elapsed,TotalCPU,AllocCPUS JobID ReqMem MaxRSS Elapsed TotalCPU AllocCPUS ------------ --------- ---------- ---------- ---------- ---------- 1607166 60Gn 13 :07:31 7 -16:21:29 16 1607166 .bat+ 60Gn 4314560K 13 :07:31 7 -16:21:29 16 1607166 .ext+ 60Gn 0 13 :07:31 00 :00.001 16 From this command, we can read that we allocate 60GB memory of memory per node (suffix n ) and the maximum RSS is reported to be 4.3GB. You can use this information to fine-tune your memory allocations. Further, the program ran for 13 hours and 7 minutes with allocated 16 CPU cores and consumed a total of 17 days, 16 hours, and 21 minutes of CPU time. Thus, a total of 10'061 CPU minutes were spent in 787 minutes wall-clock time. This yields an overall empirical degree of parallelism of about 10061 / 787 = 14 and a parallel efficiency of 14 / 16 = 88%. The discussion of parallel efficiency is a topic not covered here. However, you can use the awk script below to compute the empirical parallelism ( EmpPar ) and the parallel efficiency ( ParEff ). The script also displays the difference i requested and used RSS ( DiffRSS ). The script can be found here . med-login1:~# sacct -j 1607166 --format = JobID,ReqMem,MaxRSS,Elapsed,TotalCPU,AllocCPUS \\ | awk -f quick-sacct.awk JobID ReqMem MaxRSS Elapsed TotalCPU AllocCPUS EmpPar ParEff DiffMEM ------------ ---------- ---------- ---------- ---------- ---------- --------- -------- -------- 1607166 60Gn 13 :07:31 7 -16:21:29 16 0 .00 0 .00 - 1607166 .bat+ 60Gn 4314560K 13 :07:31 7 -16:21:29 16 14 .05 0 .88 55 .89 1607166 .ext+ 60Gn 0 13 :07:31 00 :00.001 16 0 .00 0 .00 -","title":"Memory/CPU Accounting in Slurm"},{"location":"slurm/migrating/","text":"Migrating to Slurm \u00b6 This page describes list of steps that are commonly needed to migrate from SGE to Slurm. This section is deprecated and will be removed soon. Summary \u00b6 Use #SBATCH instead of #$ in your job scripts for resource definitions. Update your resource requirements parameters from SGE to Slurm using the Slurm Rosetta Stone . Unset environment variable DRMAA_LIBRARY_PATH and remove from your ~/.bashrc , job scripts, etc. Use ${SLURMD_NODENAME-${HOSTNAME}} instead of $HOSTNAME in your ~/.bashrc file. These steps are explained in detail below. Using #SBATCH \u00b6 Slurm uses #SBATCH as the line marker at the head of your script file. So instead of #$ -N my_job You now use #SBATCH --job-name my_job Use Slurm Parameters \u00b6 Of course, the Slurm scheduler parameters differ from the SGE ones. So you also have to use Slurm syntax instead of SGE syntax (e.g., swich from -N to -J/--job-name as show in the previous section). The page Slurm Rosetta Stone provides you with a translation table. Getting Rid of DRMAA_LIBRARY_PATH \u00b6 For SGE, it was necessary to define the environment variable DRMAA_LIBRARY_PATH and the old document told you to put this into your ~/.bashrc file. You should remove the setting from your file. To check the results, you can use echo $DRMAA_LIBRARY_PATH in your shell. You can use unset DRMAA_LIBRARY_PATH to remove the environment variable from your current shell session. Limitations of the Slurm DRMAA Library \u00b6 We are using an installation of the natefoo Slurm DRMAA library fork . The library does not implement all srun / sbatch arguments and syntax. Most notably, you can only specify resourc requirements as numbers in megabytes (without units) and running time as hh:mm . Note that you cannot specify the --export command and the behaviour will default to --export=ALL . srun does not re-run ~/.bashrc on the login node \u00b6 This means that the value of $HOSTNAME will remain the one on the login node, for example. You can fix this by replacing $HOSTNAME by ${SLURMD_NODENAME-${HOSTNAME}} in your ~/.bashrc .","title":"Migrating to Slurm"},{"location":"slurm/migrating/#migrating-to-slurm","text":"This page describes list of steps that are commonly needed to migrate from SGE to Slurm. This section is deprecated and will be removed soon.","title":"Migrating to Slurm"},{"location":"slurm/migrating/#summary","text":"Use #SBATCH instead of #$ in your job scripts for resource definitions. Update your resource requirements parameters from SGE to Slurm using the Slurm Rosetta Stone . Unset environment variable DRMAA_LIBRARY_PATH and remove from your ~/.bashrc , job scripts, etc. Use ${SLURMD_NODENAME-${HOSTNAME}} instead of $HOSTNAME in your ~/.bashrc file. These steps are explained in detail below.","title":"Summary"},{"location":"slurm/migrating/#using-sbatch","text":"Slurm uses #SBATCH as the line marker at the head of your script file. So instead of #$ -N my_job You now use #SBATCH --job-name my_job","title":"Using #SBATCH"},{"location":"slurm/migrating/#use-slurm-parameters","text":"Of course, the Slurm scheduler parameters differ from the SGE ones. So you also have to use Slurm syntax instead of SGE syntax (e.g., swich from -N to -J/--job-name as show in the previous section). The page Slurm Rosetta Stone provides you with a translation table.","title":"Use Slurm Parameters"},{"location":"slurm/migrating/#getting-rid-of-drmaa_library_path","text":"For SGE, it was necessary to define the environment variable DRMAA_LIBRARY_PATH and the old document told you to put this into your ~/.bashrc file. You should remove the setting from your file. To check the results, you can use echo $DRMAA_LIBRARY_PATH in your shell. You can use unset DRMAA_LIBRARY_PATH to remove the environment variable from your current shell session.","title":"Getting Rid of DRMAA_LIBRARY_PATH"},{"location":"slurm/migrating/#limitations-of-the-slurm-drmaa-library","text":"We are using an installation of the natefoo Slurm DRMAA library fork . The library does not implement all srun / sbatch arguments and syntax. Most notably, you can only specify resourc requirements as numbers in megabytes (without units) and running time as hh:mm . Note that you cannot specify the --export command and the behaviour will default to --export=ALL .","title":"Limitations of the Slurm DRMAA Library"},{"location":"slurm/migrating/#srun-does-not-re-run-bashrc-on-the-login-node","text":"This means that the value of $HOSTNAME will remain the one on the login node, for example. You can fix this by replacing $HOSTNAME by ${SLURMD_NODENAME-${HOSTNAME}} in your ~/.bashrc .","title":"srun does not re-run ~/.bashrc on the login node"},{"location":"slurm/overview/","text":"Scheduling Overview \u00b6 The BIH HPC uses the Slurm scheduling system. This section of the manual attempts to give an overview of what scheduling is and how you can use the Slurm scheduler. For more detailed information, you will have to refer to the Slurm website and the Slurm man pages (e.g., by entering man sbatch or man srun on the HPC terminal's command line). For a quick introduction and hands-on examples, please see the manual sections Overview, starting with For the Impatient , and First Steps/Tutorial, starting with Episode 0 . Annotated Contents \u00b6 Background on Scheduling -- some background on scheduling and the terminology used Quickstart -- the most important Slurm commands, explained, with examples Cheat Sheet -- for quick reference Job Scripts -- how to setup job scripts with Slurm Memory Allocation -- memory allocation ( one of the most important concepts that is most often found confusing) Introduction to Slurm Commands srun -- running parallel jobs now sbatch -- submission of batch jobs scancel -- stop/kill jobs sinfo -- display information about the Slurm cluster squeue -- information about pending and running jbos scontrol -- detailed information (and control) sacct -- access Slurm accounting information (pending, running, and past jobs) Format Strings in Slurm -- format strings allow to display extended information about Slurm scheduler objects Slurm and Snakemake -- how to use Snakemake with Slurm X11 Forwarding -- X11 forwarding in Slurm (simple; short) Rosetta Stone -- lookup table for SGE <-> Slurm Migrating from SGE -- hints for migrating from SGE to Slurm ( deprecated, will be removed) A Word on \"Elsewhere\" \u00b6 Many facilities run Slurm clusters and have their documentation available on the internet and we will list some that we found useful below. However, beware that Slurm is a highly configurable and extensible system. Other sites may have different configurations and plugins enabled than we have (or might even have written custom plugins that are not available at BIH). In any case, it's always useful to look \"\u00fcber den Tellerrand\". Quick Start User Guide - the official guide from the Slurm creators. Slurm man Pages - web versions of man <slurm command> . TU Dresden Slurm Compendium - nice documentation from the installation in Dresden. Beware that their installation is highly customized, in particular partition selection is automatized for them (and it is not for us). Slurm at CECI - CECI is a HPC consortium from Belgium. Slurm at the Arctic University of Norway Slurm at Technical University of Denmark - if you want to get an insight in how this looks to administrator.","title":"Scheduling Overview"},{"location":"slurm/overview/#scheduling-overview","text":"The BIH HPC uses the Slurm scheduling system. This section of the manual attempts to give an overview of what scheduling is and how you can use the Slurm scheduler. For more detailed information, you will have to refer to the Slurm website and the Slurm man pages (e.g., by entering man sbatch or man srun on the HPC terminal's command line). For a quick introduction and hands-on examples, please see the manual sections Overview, starting with For the Impatient , and First Steps/Tutorial, starting with Episode 0 .","title":"Scheduling Overview"},{"location":"slurm/overview/#annotated-contents","text":"Background on Scheduling -- some background on scheduling and the terminology used Quickstart -- the most important Slurm commands, explained, with examples Cheat Sheet -- for quick reference Job Scripts -- how to setup job scripts with Slurm Memory Allocation -- memory allocation ( one of the most important concepts that is most often found confusing) Introduction to Slurm Commands srun -- running parallel jobs now sbatch -- submission of batch jobs scancel -- stop/kill jobs sinfo -- display information about the Slurm cluster squeue -- information about pending and running jbos scontrol -- detailed information (and control) sacct -- access Slurm accounting information (pending, running, and past jobs) Format Strings in Slurm -- format strings allow to display extended information about Slurm scheduler objects Slurm and Snakemake -- how to use Snakemake with Slurm X11 Forwarding -- X11 forwarding in Slurm (simple; short) Rosetta Stone -- lookup table for SGE <-> Slurm Migrating from SGE -- hints for migrating from SGE to Slurm ( deprecated, will be removed)","title":"Annotated Contents"},{"location":"slurm/overview/#a-word-on-elsewhere","text":"Many facilities run Slurm clusters and have their documentation available on the internet and we will list some that we found useful below. However, beware that Slurm is a highly configurable and extensible system. Other sites may have different configurations and plugins enabled than we have (or might even have written custom plugins that are not available at BIH). In any case, it's always useful to look \"\u00fcber den Tellerrand\". Quick Start User Guide - the official guide from the Slurm creators. Slurm man Pages - web versions of man <slurm command> . TU Dresden Slurm Compendium - nice documentation from the installation in Dresden. Beware that their installation is highly customized, in particular partition selection is automatized for them (and it is not for us). Slurm at CECI - CECI is a HPC consortium from Belgium. Slurm at the Arctic University of Norway Slurm at Technical University of Denmark - if you want to get an insight in how this looks to administrator.","title":"A Word on \"Elsewhere\""},{"location":"slurm/quickstart/","text":"Slurm Quickstart \u00b6 Create an interactive bash session ( srun will run bash in real-time, --pty makes stdout and stderr to your current session). med-login1:~$ srun --pty bash -i med0740:~$ echo \"Hello World\" Hello World med0740:~$ exit med-login1:~$ Note you probably want to specify the long partition and a longer running time for your interactive jobs . This way, your jobs can run for up to 28 days. med-login1:~$ srun --pty --partition long --time 28 -00 bash -i med0740:~$ Pro-Tip: Using Bash aliases for quick access. med-login1:~$ alias slogin=\"srun --pty bash -i\" med-login1:~$ slogin med0740:~$ exit med-login1:~$ cat >>~/.bashrc <<\"EOF\" # Useful aliases for logging in via Slurm alias slogin=\"srun --pty bash -i\" alias slogin-x11=\"srun --pty --x11 bash -i\" EOF Create an interactive R session on the cluster (assuming conda is active and the environment my-r is created, e.g., with conda create -n my-r r ). med-login1:~$ conda activate my-r med-login1:~$ srun --pty R R version 3 .6.2 ( 2019 -12-12 ) -- \"Dark and Stormy Night\" Copyright ( C ) 2019 The R Foundation for Statistical Computing [ ... ] Type 'demo()' for some demos, 'help()' for on-line help, or 'help.start()' for an HTML browser interface to help. Type 'q()' to quit R. > Sys.info ()[ \"nodename\" ] nodename \"med0740\" > q () Save workspace image? [ y/n/c ] : med-login1:~$ Create an interactive iPython session on the cluster (assuming conda is active and the environment my-python is created, e.g., with conda create -n my-python python=3 ipython ). med-login1:~$ conda activate my-python med-login1:~$ srun --pty ipython Python 3 .8.2 | packaged by conda-forge | ( default, Mar 5 2020 , 17 :11:00 ) Type 'copyright' , 'credits' or 'license' for more information IPython 7 .13.0 -- An enhanced Interactive Python. Type '?' for help. In [ 1 ] : import socket ; socket.gethostname () Out [ 1 ] : 'med0740' In [ 2 ] : exit med-login1:~$ Allocate 4 cores (default is 1 core), and a total of 4GB of RAM on one node (alternatively use --mem-per-cpu to set RAM per CPU); sbatch accepts the same argument. med-login1:~$ srun --cpus-per-task = 4 --nodes = 1 --mem = 4G --pty bash med0740:~$ export | grep SLURM_CPUS_ON_NODE 4 med0740:~$ your-parallel-script 4 Submit an R script to the cluster in batch mode ( sbatch schedules the job for later execution). med-login1:~$ cat >job-script.sh << \"EOF\" #!/bin/bash echo \"Hello, I'm running on $( hostname ) and it's $( date ) \" EOF med-login1:~$ sbatch job-script.sh Submitted batch job 7 med-login1:~$ cat slurm-7.out Hello, I 'm running on med0740 and it' s Fri Mar 6 07 :36:42 CET 2020 med-login1:~$","title":"Slurm Quickstart"},{"location":"slurm/quickstart/#slurm-quickstart","text":"Create an interactive bash session ( srun will run bash in real-time, --pty makes stdout and stderr to your current session). med-login1:~$ srun --pty bash -i med0740:~$ echo \"Hello World\" Hello World med0740:~$ exit med-login1:~$ Note you probably want to specify the long partition and a longer running time for your interactive jobs . This way, your jobs can run for up to 28 days. med-login1:~$ srun --pty --partition long --time 28 -00 bash -i med0740:~$ Pro-Tip: Using Bash aliases for quick access. med-login1:~$ alias slogin=\"srun --pty bash -i\" med-login1:~$ slogin med0740:~$ exit med-login1:~$ cat >>~/.bashrc <<\"EOF\" # Useful aliases for logging in via Slurm alias slogin=\"srun --pty bash -i\" alias slogin-x11=\"srun --pty --x11 bash -i\" EOF Create an interactive R session on the cluster (assuming conda is active and the environment my-r is created, e.g., with conda create -n my-r r ). med-login1:~$ conda activate my-r med-login1:~$ srun --pty R R version 3 .6.2 ( 2019 -12-12 ) -- \"Dark and Stormy Night\" Copyright ( C ) 2019 The R Foundation for Statistical Computing [ ... ] Type 'demo()' for some demos, 'help()' for on-line help, or 'help.start()' for an HTML browser interface to help. Type 'q()' to quit R. > Sys.info ()[ \"nodename\" ] nodename \"med0740\" > q () Save workspace image? [ y/n/c ] : med-login1:~$ Create an interactive iPython session on the cluster (assuming conda is active and the environment my-python is created, e.g., with conda create -n my-python python=3 ipython ). med-login1:~$ conda activate my-python med-login1:~$ srun --pty ipython Python 3 .8.2 | packaged by conda-forge | ( default, Mar 5 2020 , 17 :11:00 ) Type 'copyright' , 'credits' or 'license' for more information IPython 7 .13.0 -- An enhanced Interactive Python. Type '?' for help. In [ 1 ] : import socket ; socket.gethostname () Out [ 1 ] : 'med0740' In [ 2 ] : exit med-login1:~$ Allocate 4 cores (default is 1 core), and a total of 4GB of RAM on one node (alternatively use --mem-per-cpu to set RAM per CPU); sbatch accepts the same argument. med-login1:~$ srun --cpus-per-task = 4 --nodes = 1 --mem = 4G --pty bash med0740:~$ export | grep SLURM_CPUS_ON_NODE 4 med0740:~$ your-parallel-script 4 Submit an R script to the cluster in batch mode ( sbatch schedules the job for later execution). med-login1:~$ cat >job-script.sh << \"EOF\" #!/bin/bash echo \"Hello, I'm running on $( hostname ) and it's $( date ) \" EOF med-login1:~$ sbatch job-script.sh Submitted batch job 7 med-login1:~$ cat slurm-7.out Hello, I 'm running on med0740 and it' s Fri Mar 6 07 :36:42 CET 2020 med-login1:~$","title":"Slurm Quickstart"},{"location":"slurm/rosetta-stone/","text":"Slurm Rosetta Stone \u00b6 Rosetta Stone? The Rosetta Stone is a stone slab that carries the same text in Egyptian hieroglyphs and ancient Greek. This was key for decyphering Egyptian hieroglyphs in the 18 th century. Nowadays, the term is often used to label translation tables such as the one below. The table below shows some SGE commands and their Slurm equivalents. User Command SGE Slurm remote login qrsh/qlogin srun --pty bash run interactively N/A srun --pty program submit job qsub script.sh sbatch script.sh delete job qdel job-id scancel job-id job status by job id N/A squeue --job job-id detailed job status qstat -u '*' -j job-id sstat job-id job status of your jobs qstat squeue --me job status by user qstat -u user squeue -u user hold job qhold job-id scontrol hold job-id release job qrls job-id scontrol release job-id queue list qconf -sql scontrol show partitions node list qhost sinfo -N OR scontrol show nodes cluster status qhost -q sinfo show node resources N/A sinfo \"%n %G\" Job Specification SGE Slurm script directive marker #$ #SBATCH (run in queue) -q queue -p queue allocated nodes N/A -N min[-max] allocate cores -pe smp count -n count limit running time -l h_rt=time -t days-hh:mm:s redirectd stdout -o file -o file redirect stderr -e file -e file combine stdout/stderr -j yes -o without -e copy environment -V --export=ALL\\|NONE\\|variables email notification -m abe --mail-type=events send email to -M email --mail-user=email job name -N name --job-name=name restart job -r yes|no --requeue|--no-requeue working directory -wd path --workdir run exclusively -l exclusive --exclusive OR --shared allocate memory -l h_vmem=size --mem=mem OR --mem-per-cpu=mem wait for job -hold_jid jid --depend state:job select target host -l hostname=host1\\|host1 --nodelist=nodes AND/OR --exclude allocate GPU -l gpu=1 --gres=gpu:tesla:count","title":"Slurm Rosetta Stone"},{"location":"slurm/rosetta-stone/#slurm-rosetta-stone","text":"Rosetta Stone? The Rosetta Stone is a stone slab that carries the same text in Egyptian hieroglyphs and ancient Greek. This was key for decyphering Egyptian hieroglyphs in the 18 th century. Nowadays, the term is often used to label translation tables such as the one below. The table below shows some SGE commands and their Slurm equivalents. User Command SGE Slurm remote login qrsh/qlogin srun --pty bash run interactively N/A srun --pty program submit job qsub script.sh sbatch script.sh delete job qdel job-id scancel job-id job status by job id N/A squeue --job job-id detailed job status qstat -u '*' -j job-id sstat job-id job status of your jobs qstat squeue --me job status by user qstat -u user squeue -u user hold job qhold job-id scontrol hold job-id release job qrls job-id scontrol release job-id queue list qconf -sql scontrol show partitions node list qhost sinfo -N OR scontrol show nodes cluster status qhost -q sinfo show node resources N/A sinfo \"%n %G\" Job Specification SGE Slurm script directive marker #$ #SBATCH (run in queue) -q queue -p queue allocated nodes N/A -N min[-max] allocate cores -pe smp count -n count limit running time -l h_rt=time -t days-hh:mm:s redirectd stdout -o file -o file redirect stderr -e file -e file combine stdout/stderr -j yes -o without -e copy environment -V --export=ALL\\|NONE\\|variables email notification -m abe --mail-type=events send email to -M email --mail-user=email job name -N name --job-name=name restart job -r yes|no --requeue|--no-requeue working directory -wd path --workdir run exclusively -l exclusive --exclusive OR --shared allocate memory -l h_vmem=size --mem=mem OR --mem-per-cpu=mem wait for job -hold_jid jid --depend state:job select target host -l hostname=host1\\|host1 --nodelist=nodes AND/OR --exclude allocate GPU -l gpu=1 --gres=gpu:tesla:count","title":"Slurm Rosetta Stone"},{"location":"slurm/snakemake/","text":"Snakemake with Slurm \u00b6 This page describes how to use Snakemake with Slurm. Prerequisites \u00b6 This assumes that you have Miniconda properly setup with Bioconda. Also it assumes that you have already activated the Miniconda base environment with source miniconda/bin/activate . Environment Setup \u00b6 We first create a new environment snakemake-slurm and activate it. We need the snakemake package and the drmaa Python library for interfacing with the SLURM scheduler via the DRMAA interface. host:~$ conda create -y -n snakemake-slurm snakemake drmaa [ ... ] # # To activate this environment, use # # $ conda activate snakemake-slurm # # To deactivate an active environment, use # # $ conda deactivate host:~$ conda activate snakemake-slurm ( snakemake-slurm ) host:~$ Snakemake Workflow Setup \u00b6 We create a workflow and ensure that it works properly with multi-threaded Snakemake (no cluster submission here!) host:~$ mkdir -p snake-slurm host:~$ cd snake-slurm host:snake-slurm$ cat >Snakefile << \"EOF\" rule default: input: \"the-result.txt\" rule mkresult: output: \"the-result.txt\" shell: r \"sleep 1m; touch the-result.txt\" EOF host:snake-slurm$ snakemake --cores = 1 [ ... ] host:snake-slurm$ ls Snakefile the-result.txt host:snake-slurm$ rm the-result.txt Snakemake and Slurm \u00b6 It's really simple: unset the DRMAA_LIBRARY_PATH variable (that might be set and point to the SGE DRMAA library); you could set it to /usr/lib64/libdrmaa.so but that would not be necessary, consider all mentions of it from ~/.bashrc . Use snakemake --drmaa \" [params]\" and use SLURM syntax here. host:snake-slurm$ unset DRMAA_LIBRARY_PATH host:snake-slurm $snakemake --drmaa \" -t 05:00\" --jobs 2 Building DAG of jobs... Using shell: /usr/bin/bash Provided cluster nodes: 2 Job counts: count jobs 1 default 1 mkresult 2 [ Wed Mar 25 23 :06:01 2020 ] rule mkresult: output: the-result.txt jobid: 1 Submitted DRMAA job 1 with external jobid 325 . [ Wed Mar 25 23 :07:11 2020 ] Finished job 1 . 1 of 2 steps ( 50 % ) done [ Wed Mar 25 23 :07:11 2020 ] localrule default: input: the-result.txt jobid: 0 [ Wed Mar 25 23 :07:11 2020 ] Finished job 0 . 2 of 2 steps ( 100 % ) done Complete log: /fast/home/users/holtgrem_c/snake-slurm/.snakemake/log/2020-03-25T230601.353735.snakemake.log Note that we sneaked in a sleep 1m ? In a second terminal session, we can see that the job has been submitted to SLURM indeed. host:~$ squeue -u holtgrem_c JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 325 debug snakejob holtgrem R 0 :47 1 med0127 Limitations \u00b6 The DRMAA interface to Slurm has a few limitations: memory has to be given as an integer in the unit Megabytes (1024 * 1024 bytes) and as a plain number without unit, running time has to be given as hh:mm . --export is not supported by drmaa yet. The default of SLURM is --export=ALL (similar to -V for SGE). A full list of supported parameters can be found in the officical documentation . ... that's all, folks!","title":"Snakemake with Slurm"},{"location":"slurm/snakemake/#snakemake-with-slurm","text":"This page describes how to use Snakemake with Slurm.","title":"Snakemake with Slurm"},{"location":"slurm/snakemake/#prerequisites","text":"This assumes that you have Miniconda properly setup with Bioconda. Also it assumes that you have already activated the Miniconda base environment with source miniconda/bin/activate .","title":"Prerequisites"},{"location":"slurm/snakemake/#environment-setup","text":"We first create a new environment snakemake-slurm and activate it. We need the snakemake package and the drmaa Python library for interfacing with the SLURM scheduler via the DRMAA interface. host:~$ conda create -y -n snakemake-slurm snakemake drmaa [ ... ] # # To activate this environment, use # # $ conda activate snakemake-slurm # # To deactivate an active environment, use # # $ conda deactivate host:~$ conda activate snakemake-slurm ( snakemake-slurm ) host:~$","title":"Environment Setup"},{"location":"slurm/snakemake/#snakemake-workflow-setup","text":"We create a workflow and ensure that it works properly with multi-threaded Snakemake (no cluster submission here!) host:~$ mkdir -p snake-slurm host:~$ cd snake-slurm host:snake-slurm$ cat >Snakefile << \"EOF\" rule default: input: \"the-result.txt\" rule mkresult: output: \"the-result.txt\" shell: r \"sleep 1m; touch the-result.txt\" EOF host:snake-slurm$ snakemake --cores = 1 [ ... ] host:snake-slurm$ ls Snakefile the-result.txt host:snake-slurm$ rm the-result.txt","title":"Snakemake Workflow Setup"},{"location":"slurm/snakemake/#snakemake-and-slurm","text":"It's really simple: unset the DRMAA_LIBRARY_PATH variable (that might be set and point to the SGE DRMAA library); you could set it to /usr/lib64/libdrmaa.so but that would not be necessary, consider all mentions of it from ~/.bashrc . Use snakemake --drmaa \" [params]\" and use SLURM syntax here. host:snake-slurm$ unset DRMAA_LIBRARY_PATH host:snake-slurm $snakemake --drmaa \" -t 05:00\" --jobs 2 Building DAG of jobs... Using shell: /usr/bin/bash Provided cluster nodes: 2 Job counts: count jobs 1 default 1 mkresult 2 [ Wed Mar 25 23 :06:01 2020 ] rule mkresult: output: the-result.txt jobid: 1 Submitted DRMAA job 1 with external jobid 325 . [ Wed Mar 25 23 :07:11 2020 ] Finished job 1 . 1 of 2 steps ( 50 % ) done [ Wed Mar 25 23 :07:11 2020 ] localrule default: input: the-result.txt jobid: 0 [ Wed Mar 25 23 :07:11 2020 ] Finished job 0 . 2 of 2 steps ( 100 % ) done Complete log: /fast/home/users/holtgrem_c/snake-slurm/.snakemake/log/2020-03-25T230601.353735.snakemake.log Note that we sneaked in a sleep 1m ? In a second terminal session, we can see that the job has been submitted to SLURM indeed. host:~$ squeue -u holtgrem_c JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 325 debug snakejob holtgrem R 0 :47 1 med0127","title":"Snakemake and  Slurm"},{"location":"slurm/snakemake/#limitations","text":"The DRMAA interface to Slurm has a few limitations: memory has to be given as an integer in the unit Megabytes (1024 * 1024 bytes) and as a plain number without unit, running time has to be given as hh:mm . --export is not supported by drmaa yet. The default of SLURM is --export=ALL (similar to -V for SGE). A full list of supported parameters can be found in the officical documentation . ... that's all, folks!","title":"Limitations"},{"location":"slurm/x11/","text":"Slurm and X11 \u00b6 Make sure to connect to the login node with X11 forwarding. host:~$ ssh -X -l user_c med-login1.bihealth.org Once connected to the login node, pass the --x11 flag. med-login1:~$ srun --pty --x11 xterm","title":"X11 Forwarding"},{"location":"slurm/x11/#slurm-and-x11","text":"Make sure to connect to the login node with X11 forwarding. host:~$ ssh -X -l user_c med-login1.bihealth.org Once connected to the login node, pass the --x11 flag. med-login1:~$ srun --pty --x11 xterm","title":"Slurm and X11"},{"location":"storage/accessing-snapshots/","text":"Accessing Snapshots and Backups \u00b6 By now you have probably read that your home directory has strict quotas in place. You pay this price in usability by the fact that snapshots and backups exist. Snapshot \u00b6 Every night, a snapshot is created of all user, group, and project home directories. The snapshots are placed in the following locations. /fast/home/.snapshots/$SNAPSHOT/users/$USER /fast/home/.snapshots/$SNAPSHOT/groups/$GROUP /fast/home/.snapshots/$SNAPSHOT/projects/$PROJECT The snapshot name $SNAPSHOT simply is the 3-letter abbreviation of the week day (i.e., Mon, Tue, Thu, Fri, Sat, Sun). The snapshots contains the state of the home directories at the time that they were made. This also includes the permissions. That is you can simply retrieve the state of your home directory of last Tuesday (if the directory already existed back then) at: /fast/home/.snapshots/Tue/users/$USER e.g., $ ls /fast/home/.snapshots/Tue/users/ $USER ... Backups \u00b6 There are very few cases where backups will help you more than snapshots. As with snapshots, there is a 7-day rotation for backups. The backups fully reflect the snapshots and thus everything that is in the backups is also in the snapshots. The only time that you will need backups is when the GPFS and its snapshots are damaged. This protects the files in your home directory against technical errors and other catastrophes such as fire or water damage of the data center.","title":"Accessing Snapshots"},{"location":"storage/accessing-snapshots/#accessing-snapshots-and-backups","text":"By now you have probably read that your home directory has strict quotas in place. You pay this price in usability by the fact that snapshots and backups exist.","title":"Accessing Snapshots and Backups"},{"location":"storage/accessing-snapshots/#snapshot","text":"Every night, a snapshot is created of all user, group, and project home directories. The snapshots are placed in the following locations. /fast/home/.snapshots/$SNAPSHOT/users/$USER /fast/home/.snapshots/$SNAPSHOT/groups/$GROUP /fast/home/.snapshots/$SNAPSHOT/projects/$PROJECT The snapshot name $SNAPSHOT simply is the 3-letter abbreviation of the week day (i.e., Mon, Tue, Thu, Fri, Sat, Sun). The snapshots contains the state of the home directories at the time that they were made. This also includes the permissions. That is you can simply retrieve the state of your home directory of last Tuesday (if the directory already existed back then) at: /fast/home/.snapshots/Tue/users/$USER e.g., $ ls /fast/home/.snapshots/Tue/users/ $USER ...","title":"Snapshot"},{"location":"storage/accessing-snapshots/#backups","text":"There are very few cases where backups will help you more than snapshots. As with snapshots, there is a 7-day rotation for backups. The backups fully reflect the snapshots and thus everything that is in the backups is also in the snapshots. The only time that you will need backups is when the GPFS and its snapshots are damaged. This protects the files in your home directory against technical errors and other catastrophes such as fire or water damage of the data center.","title":"Backups"},{"location":"storage/querying-storage/","text":"Querying Storage Quotas \u00b6 As described elsewhere, all data in your user, group, and project volumes is subject to quotas. Again, note that in case you face a shortage of storage, do not hesitate to send a request for quota increase to hpc-gatekeeper@bihealth.de . This page quickly shows how to query for the current usage of data volume and file counts for your user, group, and projects. Query for User Data and File Usage \u00b6 The file /etc/bashrc.gpfs-quota contains some Bash functions that you can use for querying the quota usage. This file is automatically sourced in all of your Bash sessions. For querying your user's data and file usage, enter the following command: # bih-gpfs-quota-user holtgrem_c You will get a report as follows. As soon as usage reaches 90%, the data/file usage will be highlighted in yellow. If you pass 99%, the data/file usage will be highlighted in red. ================================= Quota Report for: user holtgrem_c ================================= DATA quota GR- FILES quota GR- ENTITY NAME FSET USED SOFT HARD ACE USED SOFT HARD ACE ------- ---------- ------- ----- ---- ----- ----- --- ----- ---- ----- ----- --- users holtgrem_c home 103M 10% 1.0G 1.5G - 2.5k 25% 10k 12k - users holtgrem_c work 639G 62% 1.0T 1.1T - 1.0M 52% 2.0M 2.2M - users holtgrem_c scratch 42G 0% 200T 220T - 207k 0.1% 200M 220M - [...] Query for Group Data and File Usage \u00b6 # bih-gpfs-report-quota group ag_someag ================================= Quota Report for: group ag_someag ================================= DATA quota GR- FILES quota GR- ENTITY NAME FSET USED SOFT HARD ACE USED SOFT HARD ACE ------- ---------- ------- ----- ---- ----- ----- --- ----- ---- ----- ----- --- groups ag_someag home 0 0% 1.0G 1.5G - 4 0% 10k 12k - groups ag_someag work 349G 34% 1.0T 1.5T - 302 0% 2.0M 2.2M - groups ag_someag scratch 0 0% 200T 220T - 1 0% 200M 220M - [...] Query for Project Data and File Usage \u00b6 # bih-gpfs-report-quota project someproj ================================== Quota Report for: project someproj ================================== DATA quota GR- FILES quota GR- ENTITY NAME FSET USED SOFT HARD ACE USED SOFT HARD ACE ------- ---------- ------- ----- ---- ----- ----- --- ----- ---- ----- ----- --- groups someproj home 0 0% 1.0G 1.5G - 4 0% 10k 12k - groups someproj work 349G 34% 1.0T 1.5T - 302 0% 2.0M 2.2M - groups someproj scratch 0 0% 200T 220T - 1 0% 200M 220M - [...]","title":"Querying Quotas"},{"location":"storage/querying-storage/#querying-storage-quotas","text":"As described elsewhere, all data in your user, group, and project volumes is subject to quotas. Again, note that in case you face a shortage of storage, do not hesitate to send a request for quota increase to hpc-gatekeeper@bihealth.de . This page quickly shows how to query for the current usage of data volume and file counts for your user, group, and projects.","title":"Querying Storage Quotas"},{"location":"storage/querying-storage/#query-for-user-data-and-file-usage","text":"The file /etc/bashrc.gpfs-quota contains some Bash functions that you can use for querying the quota usage. This file is automatically sourced in all of your Bash sessions. For querying your user's data and file usage, enter the following command: # bih-gpfs-quota-user holtgrem_c You will get a report as follows. As soon as usage reaches 90%, the data/file usage will be highlighted in yellow. If you pass 99%, the data/file usage will be highlighted in red. ================================= Quota Report for: user holtgrem_c ================================= DATA quota GR- FILES quota GR- ENTITY NAME FSET USED SOFT HARD ACE USED SOFT HARD ACE ------- ---------- ------- ----- ---- ----- ----- --- ----- ---- ----- ----- --- users holtgrem_c home 103M 10% 1.0G 1.5G - 2.5k 25% 10k 12k - users holtgrem_c work 639G 62% 1.0T 1.1T - 1.0M 52% 2.0M 2.2M - users holtgrem_c scratch 42G 0% 200T 220T - 207k 0.1% 200M 220M - [...]","title":"Query for User Data and File Usage"},{"location":"storage/querying-storage/#query-for-group-data-and-file-usage","text":"# bih-gpfs-report-quota group ag_someag ================================= Quota Report for: group ag_someag ================================= DATA quota GR- FILES quota GR- ENTITY NAME FSET USED SOFT HARD ACE USED SOFT HARD ACE ------- ---------- ------- ----- ---- ----- ----- --- ----- ---- ----- ----- --- groups ag_someag home 0 0% 1.0G 1.5G - 4 0% 10k 12k - groups ag_someag work 349G 34% 1.0T 1.5T - 302 0% 2.0M 2.2M - groups ag_someag scratch 0 0% 200T 220T - 1 0% 200M 220M - [...]","title":"Query for Group Data and File Usage"},{"location":"storage/querying-storage/#query-for-project-data-and-file-usage","text":"# bih-gpfs-report-quota project someproj ================================== Quota Report for: project someproj ================================== DATA quota GR- FILES quota GR- ENTITY NAME FSET USED SOFT HARD ACE USED SOFT HARD ACE ------- ---------- ------- ----- ---- ----- ----- --- ----- ---- ----- ----- --- groups someproj home 0 0% 1.0G 1.5G - 4 0% 10k 12k - groups someproj work 349G 34% 1.0T 1.5T - 302 0% 2.0M 2.2M - groups someproj scratch 0 0% 200T 220T - 1 0% 200M 220M - [...]","title":"Query for Project Data and File Usage"},{"location":"storage/storage-locations/","text":"Storage and Volumes: Locations \u00b6 On the BIH HPC cluster, there are three kinds of entities: users, groups ( Arbeitsgruppen ), and projects. Each user, group, and project has a central folder for their files to be stored. For the Impatient \u00b6 Storage Locations \u00b6 Each user, group, and project directory consists of three locations (using /fast/users/muster_c as an example here): /fast/users/muster_c/work : Here, you put your large data that you need to keep. Note that there is no backup or snapshots going on. /fast/users/muster_c/scratch : Here, you put your large temporary files that you will delete after a short time anyway. Data placed here will be automatically removed four weeks after creation. /fast/users/muster_c (and all other sub directories): Here you put your programs and scripts and very important small data. By default, you will have a soft quota of 1GB (hard quota of 1.5GB, 7 days grace period). However, we create snapshots of this data (every 24 hours) and this data goes to a backup. You can check your current usage using the command bih-gpfs-report-quota user $USER Do's and Don'ts \u00b6 First and foremost: DO NOT place any valuable data in scratch as it will be removed within 4 weeks. Further: DO set your TMPDIR environment variable to /fast/users/$USER/scratch/tmp . DO add mkdir -p /fast/users/$USER/scratch/tmp to your ~/.bashrc and job script files. DO try to prefer creating fewer large files over many small files. DO NOT create multiple copies of large data. For sequencing data, in most cases you should not need more than raw times the size of the raw data (raw data + alignments + derived results). Introduction \u00b6 This document describes the third iteration of the file system structure on the BIH HPC cluster. This iteration was made necessary by problems with second iteration which worked well for about two years but is now reaching its limits. Organizational Entities \u00b6 There are the following three entities on the cluster: (a) normal user accounts (\"natural people\") (b) groups (Arbeitsgruppen) with on leader and an optional delegate \u00a9 projects with one owner and an optional delegate. Their purpose is described in the document \"User and Group Management\". Storage/Data Tiers \u00b6 The files fall into one of three categories: (i) Home data are programs and scripts of which there is relatively few but which is long-lived and very important. Loss of home data requires to redo manual work (like programming). (ii) Work data is data of potential large size and has a medium life time and important. Examples are raw sequencing data and intermediate results that are to be kept (e.g., a final, sorted and indexed BAM file). Work data can time-consuming actions to be restored, such as downloading large amounts of data or time-consuming computation. (iii) Scratch data is data that is temporary by nature and has a short life-time only. Examples are temporary files (e.g., unsorted BAM files). Scratch data is created to be removed eventually. Snapshots, Backups, Archive \u00b6 A snapshot stores the state of a data volume at a given time. File systems like GPFS implement this in a copy-on-write manner, meaning that for a snapshot and the subsequent \"live\" state, only the differences in data need to be store.d Note that there is additional overhead in the meta data storage. A backup is a copy of a data set on another physical location, i.e., all data from a given date copied to another server. Backups are made regularly and only a small number of previous ones is usually kept. An archive is a single copy of a single state of a data set to be kept for a long time. Classically, archives are made by copying data to magnetic tape for long-term storage. Storage Locations \u00b6 This section describes the different storage locations and gives an overview of their properties. Home Directories \u00b6 Location /fast/{users,groups,projects}/<name> (except for work and scratch sub directories) the user, group, or project home directory meant for documents, scripts, and programs default quota for data: default soft quota of 1 GB, hard quota of 1.5 GB, grace period of 7 days quota can be increased on request with short reason statement default quota for metadata: 10k files soft, 12k files hard snapshots are regularly created, see Section \\ref{snapshot-details} nightly incremental backups are created, the last 5 are kept Long-term strategy: users are expected to manage data life time independently and use best practice for source code and document management best practice (e.g., use Git). When users/groups leave the organization or projects ends, they are expected to handle data storage and cleanup on their own. Responsibility to enforce this is with the leader of a user's group, the group leader, or the project owner, respectively. Work Directories \u00b6 Location /fast/{users,groups,projects}/<name>/work the user, group, or project work directory meant for larger data that is to be used for a longer time, e.g., raw data, final sorted BAM file default quota for data: default soft quota of 1 TB, hard quota of 1.1 TB, grace period of 7 days quota can be increased on request with short reason statement default quota for metadata: 2 Mfile soft, 2.2M files hard no snapshots, no backup Long-term strategy: When users/groups leave the organization or projects ends, they are expected to cleanup unneeded data on their own. HPC IT can provide archival services on request. Responsibility to enforce this is with the leader of a user's group, the group leader, or the project owner, respectively. Scratch Directories \u00b6 Location /fast/{users,groups,projects}/<name>/scratch the user, group, or project scratch directory files will be removed 2 weeks after their creation meant for temporary, potentially large data, e.g., intermediate unsorted or unmasked BAM files, data downloaded from the internet for trying out etc. default quota for data: default soft quota of 200TB, hard quota of 220TB, grace period of 7 days quota can be increased on request with short reason statement default quota for metadata: 2M files soft, 2.2M files hard no snapshots, no backup Long-term strategy: as data on this volume is not to be kept for longer than 2 weeks, the long term strategy is to delete all files. Snapshot Details \u00b6 Snapshots are made every 24 hours. Of these snapshots, the last 7 are kept, then one for each day. Backup Details \u00b6 Backups of the snapshots is made nightly. The backups of the last 7 days are kept. Archive Details \u00b6 BIH HPC IT has some space allocated on the MDC IT tape archive. User data can be put under archive after agreeing with head of HPC IT. The process is as describe in Section \\ref{sop-data-archival}. Technical Implementation \u00b6 As a quick (very) technical note: There exists a file system fast . This file system has three independent file sets home , work , scratch . On each of these file sets, there is a dependent file set for each user, group, and project below directories users , groups , and projects . home is also mounted as /fast_new/home and for each user, group, and project, the entry work links to the corresponding fileset in work , the same for scratch. Automatic file removal from scratch is implemented using GPFS ILM. Quotas are implemented on the file-set level.","title":"Storage Locations"},{"location":"storage/storage-locations/#storage-and-volumes-locations","text":"On the BIH HPC cluster, there are three kinds of entities: users, groups ( Arbeitsgruppen ), and projects. Each user, group, and project has a central folder for their files to be stored.","title":"Storage and Volumes: Locations"},{"location":"storage/storage-locations/#for-the-impatient","text":"","title":"For the Impatient"},{"location":"storage/storage-locations/#storage-locations","text":"Each user, group, and project directory consists of three locations (using /fast/users/muster_c as an example here): /fast/users/muster_c/work : Here, you put your large data that you need to keep. Note that there is no backup or snapshots going on. /fast/users/muster_c/scratch : Here, you put your large temporary files that you will delete after a short time anyway. Data placed here will be automatically removed four weeks after creation. /fast/users/muster_c (and all other sub directories): Here you put your programs and scripts and very important small data. By default, you will have a soft quota of 1GB (hard quota of 1.5GB, 7 days grace period). However, we create snapshots of this data (every 24 hours) and this data goes to a backup. You can check your current usage using the command bih-gpfs-report-quota user $USER","title":"Storage Locations"},{"location":"storage/storage-locations/#dos-and-donts","text":"First and foremost: DO NOT place any valuable data in scratch as it will be removed within 4 weeks. Further: DO set your TMPDIR environment variable to /fast/users/$USER/scratch/tmp . DO add mkdir -p /fast/users/$USER/scratch/tmp to your ~/.bashrc and job script files. DO try to prefer creating fewer large files over many small files. DO NOT create multiple copies of large data. For sequencing data, in most cases you should not need more than raw times the size of the raw data (raw data + alignments + derived results).","title":"Do's and Don'ts"},{"location":"storage/storage-locations/#introduction","text":"This document describes the third iteration of the file system structure on the BIH HPC cluster. This iteration was made necessary by problems with second iteration which worked well for about two years but is now reaching its limits.","title":"Introduction"},{"location":"storage/storage-locations/#organizational-entities","text":"There are the following three entities on the cluster: (a) normal user accounts (\"natural people\") (b) groups (Arbeitsgruppen) with on leader and an optional delegate \u00a9 projects with one owner and an optional delegate. Their purpose is described in the document \"User and Group Management\".","title":"Organizational Entities"},{"location":"storage/storage-locations/#storagedata-tiers","text":"The files fall into one of three categories: (i) Home data are programs and scripts of which there is relatively few but which is long-lived and very important. Loss of home data requires to redo manual work (like programming). (ii) Work data is data of potential large size and has a medium life time and important. Examples are raw sequencing data and intermediate results that are to be kept (e.g., a final, sorted and indexed BAM file). Work data can time-consuming actions to be restored, such as downloading large amounts of data or time-consuming computation. (iii) Scratch data is data that is temporary by nature and has a short life-time only. Examples are temporary files (e.g., unsorted BAM files). Scratch data is created to be removed eventually.","title":"Storage/Data Tiers"},{"location":"storage/storage-locations/#snapshots-backups-archive","text":"A snapshot stores the state of a data volume at a given time. File systems like GPFS implement this in a copy-on-write manner, meaning that for a snapshot and the subsequent \"live\" state, only the differences in data need to be store.d Note that there is additional overhead in the meta data storage. A backup is a copy of a data set on another physical location, i.e., all data from a given date copied to another server. Backups are made regularly and only a small number of previous ones is usually kept. An archive is a single copy of a single state of a data set to be kept for a long time. Classically, archives are made by copying data to magnetic tape for long-term storage.","title":"Snapshots, Backups, Archive"},{"location":"storage/storage-locations/#storage-locations_1","text":"This section describes the different storage locations and gives an overview of their properties.","title":"Storage Locations"},{"location":"storage/storage-locations/#home-directories","text":"Location /fast/{users,groups,projects}/<name> (except for work and scratch sub directories) the user, group, or project home directory meant for documents, scripts, and programs default quota for data: default soft quota of 1 GB, hard quota of 1.5 GB, grace period of 7 days quota can be increased on request with short reason statement default quota for metadata: 10k files soft, 12k files hard snapshots are regularly created, see Section \\ref{snapshot-details} nightly incremental backups are created, the last 5 are kept Long-term strategy: users are expected to manage data life time independently and use best practice for source code and document management best practice (e.g., use Git). When users/groups leave the organization or projects ends, they are expected to handle data storage and cleanup on their own. Responsibility to enforce this is with the leader of a user's group, the group leader, or the project owner, respectively.","title":"Home Directories"},{"location":"storage/storage-locations/#work-directories","text":"Location /fast/{users,groups,projects}/<name>/work the user, group, or project work directory meant for larger data that is to be used for a longer time, e.g., raw data, final sorted BAM file default quota for data: default soft quota of 1 TB, hard quota of 1.1 TB, grace period of 7 days quota can be increased on request with short reason statement default quota for metadata: 2 Mfile soft, 2.2M files hard no snapshots, no backup Long-term strategy: When users/groups leave the organization or projects ends, they are expected to cleanup unneeded data on their own. HPC IT can provide archival services on request. Responsibility to enforce this is with the leader of a user's group, the group leader, or the project owner, respectively.","title":"Work Directories"},{"location":"storage/storage-locations/#scratch-directories","text":"Location /fast/{users,groups,projects}/<name>/scratch the user, group, or project scratch directory files will be removed 2 weeks after their creation meant for temporary, potentially large data, e.g., intermediate unsorted or unmasked BAM files, data downloaded from the internet for trying out etc. default quota for data: default soft quota of 200TB, hard quota of 220TB, grace period of 7 days quota can be increased on request with short reason statement default quota for metadata: 2M files soft, 2.2M files hard no snapshots, no backup Long-term strategy: as data on this volume is not to be kept for longer than 2 weeks, the long term strategy is to delete all files.","title":"Scratch Directories"},{"location":"storage/storage-locations/#snapshot-details","text":"Snapshots are made every 24 hours. Of these snapshots, the last 7 are kept, then one for each day.","title":"Snapshot Details"},{"location":"storage/storage-locations/#backup-details","text":"Backups of the snapshots is made nightly. The backups of the last 7 days are kept.","title":"Backup Details"},{"location":"storage/storage-locations/#archive-details","text":"BIH HPC IT has some space allocated on the MDC IT tape archive. User data can be put under archive after agreeing with head of HPC IT. The process is as describe in Section \\ref{sop-data-archival}.","title":"Archive Details"},{"location":"storage/storage-locations/#technical-implementation","text":"As a quick (very) technical note: There exists a file system fast . This file system has three independent file sets home , work , scratch . On each of these file sets, there is a dependent file set for each user, group, and project below directories users , groups , and projects . home is also mounted as /fast_new/home and for each user, group, and project, the entry work links to the corresponding fileset in work , the same for scratch. Automatic file removal from scratch is implemented using GPFS ILM. Quotas are implemented on the file-set level.","title":"Technical Implementation"}]}